{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2822650,"sourceType":"datasetVersion","datasetId":1715304},{"sourceId":13015029,"sourceType":"datasetVersion","datasetId":8239947}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T23:09:41.309702Z","iopub.execute_input":"2025-09-10T23:09:41.310351Z","iopub.status.idle":"2025-09-10T23:09:51.511749Z","shell.execute_reply.started":"2025-09-10T23:09:41.310327Z","shell.execute_reply":"2025-09-10T23:09:51.511050Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/aptos2019/valid.csv\n/kaggle/input/aptos2019/test.csv\n/kaggle/input/aptos2019/train_1.csv\n/kaggle/input/aptos2019/val_images/val_images/17f6c7072f61.png\n/kaggle/input/aptos2019/val_images/val_images/0243404e8a00.png\n/kaggle/input/aptos2019/val_images/val_images/0083ee8054ee.png\n/kaggle/input/aptos2019/val_images/val_images/0ac436400db4.png\n/kaggle/input/aptos2019/val_images/val_images/0d0a21fd354f.png\n/kaggle/input/aptos2019/val_images/val_images/02da652c74b8.png\n/kaggle/input/aptos2019/val_images/val_images/15e96e848b46.png\n/kaggle/input/aptos2019/val_images/val_images/059bc89df7f4.png\n/kaggle/input/aptos2019/val_images/val_images/04d029cfb612.png\n/kaggle/input/aptos2019/val_images/val_images/08752092140d.png\n/kaggle/input/aptos2019/val_images/val_images/0ef4c61dc056.png\n/kaggle/input/aptos2019/val_images/val_images/1a7e3356b39c.png\n/kaggle/input/aptos2019/val_images/val_images/06be1092a062.png\n/kaggle/input/aptos2019/val_images/val_images/03a7f4a5786f.png\n/kaggle/input/aptos2019/val_images/val_images/0d0b8fc9ab5c.png\n/kaggle/input/aptos2019/val_images/val_images/0125fbd2e791.png\n/kaggle/input/aptos2019/val_images/val_images/0e43c8298fc0.png\n/kaggle/input/aptos2019/val_images/val_images/09662e462531.png\n/kaggle/input/aptos2019/val_images/val_images/12e6e66c80a7.png\n/kaggle/input/aptos2019/val_images/val_images/0afbeeef0ff7.png\n/kaggle/input/aptos2019/val_images/val_images/0eff8eacb2f7.png\n/kaggle/input/aptos2019/val_images/val_images/1ade1e949383.png\n/kaggle/input/aptos2019/val_images/val_images/0abf0c485f66.png\n/kaggle/input/aptos2019/val_images/val_images/0a61bddab956.png\n/kaggle/input/aptos2019/val_images/val_images/1608c82a263f.png\n/kaggle/input/aptos2019/val_images/val_images/19244004583f.png\n/kaggle/input/aptos2019/val_images/val_images/064af6592ba6.png\n/kaggle/input/aptos2019/val_images/val_images/12058bbb8299.png\n/kaggle/input/aptos2019/val_images/val_images/0a9ec1e99ce4.png\n/kaggle/input/aptos2019/val_images/val_images/15f8d769935c.png\n/kaggle/input/aptos2019/val_images/val_images/093cf723fede.png\n/kaggle/input/aptos2019/val_images/val_images/13073f075a56.png\n/kaggle/input/aptos2019/val_images/val_images/1509d097b69a.png\n/kaggle/input/aptos2019/val_images/val_images/101b9ebfc720.png\n/kaggle/input/aptos2019/val_images/val_images/0ae2dd2e09ea.png\n/kaggle/input/aptos2019/val_images/val_images/0d310aba6373.png\n/kaggle/input/aptos2019/val_images/val_images/00f6c1be5a33.png\n/kaggle/input/aptos2019/val_images/val_images/09c8323c612e.png\n/kaggle/input/aptos2019/val_images/val_images/07596907347b.png\n/kaggle/input/aptos2019/val_images/val_images/1a19f2ef4472.png\n/kaggle/input/aptos2019/val_images/val_images/022f820027b8.png\n/kaggle/input/aptos2019/val_images/val_images/10a5026eb8e6.png\n/kaggle/input/aptos2019/val_images/val_images/17eb5d4ad740.png\n/kaggle/input/aptos2019/val_images/val_images/1541226c5d72.png\n/kaggle/input/aptos2019/val_images/val_images/032d7b0b4bf6.png\n/kaggle/input/aptos2019/val_images/val_images/05cd0178ccfe.png\n/kaggle/input/aptos2019/val_images/val_images/01c7808d901d.png\n/kaggle/input/aptos2019/val_images/val_images/060e00d1e2ab.png\n/kaggle/input/aptos2019/val_images/val_images/002c21358ce6.png\n/kaggle/input/aptos2019/val_images/val_images/08b6e3240858.png\n/kaggle/input/aptos2019/val_images/val_images/0fc6829da85b.png\n/kaggle/input/aptos2019/val_images/val_images/0d744aed4d64.png\n/kaggle/input/aptos2019/val_images/val_images/194814669fee.png\n/kaggle/input/aptos2019/val_images/val_images/1943983492e5.png\n/kaggle/input/aptos2019/val_images/val_images/18af532e7e1e.png\n/kaggle/input/aptos2019/val_images/val_images/143db89c11c8.png\n/kaggle/input/aptos2019/val_images/val_images/054b1b305160.png\n/kaggle/input/aptos2019/val_images/val_images/00cc2b75cddd.png\n/kaggle/input/aptos2019/val_images/val_images/17e6116b89b3.png\n/kaggle/input/aptos2019/val_images/val_images/1124ffcd76c2.png\n/kaggle/input/aptos2019/val_images/val_images/0ada12c0e78f.png\n/kaggle/input/aptos2019/val_images/val_images/04aef84a2cc1.png\n/kaggle/input/aptos2019/val_images/val_images/10f6ef37fe43.png\n/kaggle/input/aptos2019/val_images/val_images/10ecc5292ab1.png\n/kaggle/input/aptos2019/val_images/val_images/150fc7127582.png\n/kaggle/input/aptos2019/val_images/val_images/17d7d6b092f4.png\n/kaggle/input/aptos2019/val_images/val_images/188a9323be03.png\n/kaggle/input/aptos2019/val_images/val_images/15cd5f52d300.png\n/kaggle/input/aptos2019/val_images/val_images/00e4ddff966a.png\n/kaggle/input/aptos2019/val_images/val_images/0afdfe5f422c.png\n/kaggle/input/aptos2019/val_images/val_images/1409ab48175a.png\n/kaggle/input/aptos2019/val_images/val_images/07d8db76b301.png\n/kaggle/input/aptos2019/val_images/val_images/0ca0aee4d57e.png\n/kaggle/input/aptos2019/val_images/val_images/18ce0cdc473d.png\n/kaggle/input/aptos2019/val_images/val_images/10eefba568dd.png\n/kaggle/input/aptos2019/val_images/val_images/18323d8f2470.png\n/kaggle/input/aptos2019/val_images/val_images/15b21c80cc31.png\n/kaggle/input/aptos2019/val_images/val_images/166068a24416.png\n/kaggle/input/aptos2019/val_images/val_images/10bf25731c08.png\n/kaggle/input/aptos2019/val_images/val_images/02685f13cefd.png\n/kaggle/input/aptos2019/val_images/val_images/00b74780d31d.png\n/kaggle/input/aptos2019/val_images/val_images/093a42649c29.png\n/kaggle/input/aptos2019/val_images/val_images/11d8e5eaee5b.png\n/kaggle/input/aptos2019/val_images/val_images/0f96c358a250.png\n/kaggle/input/aptos2019/val_images/val_images/144b01e7b993.png\n/kaggle/input/aptos2019/val_images/val_images/0ad7f631dedb.png\n/kaggle/input/aptos2019/val_images/val_images/0519b934f6b1.png\n/kaggle/input/aptos2019/val_images/val_images/12025b34deb8.png\n/kaggle/input/aptos2019/val_images/val_images/144a1a426137.png\n/kaggle/input/aptos2019/val_images/val_images/1438288bb2e1.png\n/kaggle/input/aptos2019/val_images/val_images/0104b032c141.png\n/kaggle/input/aptos2019/val_images/val_images/0dbe6c26cedc.png\n/kaggle/input/aptos2019/val_images/val_images/086d41d17da8.png\n/kaggle/input/aptos2019/val_images/val_images/042470a92154.png\n/kaggle/input/aptos2019/val_images/val_images/0cd31078cd08.png\n/kaggle/input/aptos2019/val_images/val_images/01b3aed3ed4c.png\n/kaggle/input/aptos2019/val_images/val_images/03b373718013.png\n/kaggle/input/aptos2019/val_images/val_images/1968183f0e61.png\n/kaggle/input/aptos2019/val_images/val_images/19722bff5a09.png\n/kaggle/input/aptos2019/val_images/val_images/03c85870824c.png\n/kaggle/input/aptos2019/val_images/val_images/070d4ce5fd90.png\n/kaggle/input/aptos2019/val_images/val_images/0b8bdec9d869.png\n/kaggle/input/aptos2019/val_images/val_images/1594ca6c30d3.png\n/kaggle/input/aptos2019/val_images/val_images/05e9126dfa5c.png\n/kaggle/input/aptos2019/val_images/val_images/150f92b45349.png\n/kaggle/input/aptos2019/val_images/val_images/005b95c28852.png\n/kaggle/input/aptos2019/val_images/val_images/1632c4311fc9.png\n/kaggle/input/aptos2019/val_images/val_images/115e42dd6a81.png\n/kaggle/input/aptos2019/val_images/val_images/191cf5668f33.png\n/kaggle/input/aptos2019/val_images/val_images/0e0003ddd8df.png\n/kaggle/input/aptos2019/val_images/val_images/16ce555748d8.png\n/kaggle/input/aptos2019/val_images/val_images/0fd16b64697e.png\n/kaggle/input/aptos2019/val_images/val_images/0fffa73e2402.png\n/kaggle/input/aptos2019/val_images/val_images/107aea0d9289.png\n/kaggle/input/aptos2019/val_images/val_images/0cecc2864b7f.png\n/kaggle/input/aptos2019/val_images/val_images/0dbaa09a458c.png\n/kaggle/input/aptos2019/val_images/val_images/06024377d573.png\n/kaggle/input/aptos2019/val_images/val_images/18f1f979d30d.png\n/kaggle/input/aptos2019/val_images/val_images/0fb1053285cf.png\n/kaggle/input/aptos2019/val_images/val_images/0f882877bf13.png\n/kaggle/input/aptos2019/val_images/val_images/197af0de76e2.png\n/kaggle/input/aptos2019/val_images/val_images/0c55d58bebaf.png\n/kaggle/input/aptos2019/val_images/val_images/0f364b7d4384.png\n/kaggle/input/aptos2019/val_images/val_images/17eff993386f.png\n/kaggle/input/aptos2019/val_images/val_images/16060f05d047.png\n/kaggle/input/aptos2019/val_images/val_images/07f5d7baf907.png\n/kaggle/input/aptos2019/val_images/val_images/0180bfa26c0b.png\n/kaggle/input/aptos2019/val_images/val_images/186c1835eec5.png\n/kaggle/input/aptos2019/val_images/val_images/0151781fe50b.png\n/kaggle/input/aptos2019/val_images/val_images/05339950962e.png\n/kaggle/input/aptos2019/val_images/val_images/1a03a7970337.png\n/kaggle/input/aptos2019/val_images/val_images/0a38b552372d.png\n/kaggle/input/aptos2019/val_images/val_images/19b0e3c734f5.png\n/kaggle/input/aptos2019/val_images/val_images/0babc12807b2.png\n/kaggle/input/aptos2019/val_images/val_images/157d17349cc6.png\n/kaggle/input/aptos2019/val_images/val_images/02358b47ea89.png\n/kaggle/input/aptos2019/val_images/val_images/0212dd31f623.png\n/kaggle/input/aptos2019/val_images/val_images/0cbcc7b23613.png\n/kaggle/input/aptos2019/val_images/val_images/14515b8f19b6.png\n/kaggle/input/aptos2019/val_images/val_images/04a6fc58dabc.png\n/kaggle/input/aptos2019/val_images/val_images/04efb1a284cc.png\n/kaggle/input/aptos2019/val_images/val_images/12a82fc7d73e.png\n/kaggle/input/aptos2019/val_images/val_images/174db0854291.png\n/kaggle/input/aptos2019/val_images/val_images/06586082a24d.png\n/kaggle/input/aptos2019/val_images/val_images/026dcd9af143.png\n/kaggle/input/aptos2019/val_images/val_images/0b64a0a06f9a.png\n/kaggle/input/aptos2019/val_images/val_images/12ef75375322.png\n/kaggle/input/aptos2019/val_images/val_images/0851d6a69589.png\n/kaggle/input/aptos2019/val_images/val_images/187f6ccda87a.png\n/kaggle/input/aptos2019/val_images/val_images/19ef4d292196.png\n/kaggle/input/aptos2019/val_images/val_images/0a74c92e287c.png\n/kaggle/input/aptos2019/val_images/val_images/1623e8e3adc4.png\n/kaggle/input/aptos2019/val_images/val_images/0b00f8a77510.png\n/kaggle/input/aptos2019/val_images/val_images/09eeafa9656a.png\n/kaggle/input/aptos2019/val_images/val_images/12b57dac703e.png\n/kaggle/input/aptos2019/val_images/val_images/000c1434d8d7.png\n/kaggle/input/aptos2019/val_images/val_images/1864d3411143.png\n/kaggle/input/aptos2019/val_images/val_images/1782142e17d9.png\n/kaggle/input/aptos2019/val_images/val_images/0161338f53cc.png\n/kaggle/input/aptos2019/val_images/val_images/152db3de8120.png\n/kaggle/input/aptos2019/val_images/val_images/19e350c7c83c.png\n/kaggle/input/aptos2019/val_images/val_images/190a309f2cc5.png\n/kaggle/input/aptos2019/val_images/val_images/0bf37ca3156a.png\n/kaggle/input/aptos2019/val_images/val_images/0a85a1e8f9e9.png\n/kaggle/input/aptos2019/val_images/val_images/0af296d2f04a.png\n/kaggle/input/aptos2019/val_images/val_images/0a3202889f4d.png\n/kaggle/input/aptos2019/val_images/val_images/07751b94a88a.png\n/kaggle/input/aptos2019/val_images/val_images/05195a3db5e2.png\n/kaggle/input/aptos2019/val_images/val_images/1006345f70b7.png\n/kaggle/input/aptos2019/val_images/val_images/0c38940e1f80.png\n/kaggle/input/aptos2019/val_images/val_images/13ab8db8c700.png\n/kaggle/input/aptos2019/val_images/val_images/1844a039b4ea.png\n/kaggle/input/aptos2019/val_images/val_images/0b2ea8f268cf.png\n/kaggle/input/aptos2019/val_images/val_images/111898ab463d.png\n/kaggle/input/aptos2019/val_images/val_images/13c191b59ed0.png\n/kaggle/input/aptos2019/val_images/val_images/13d71389563f.png\n/kaggle/input/aptos2019/val_images/val_images/0eced86c9db8.png\n/kaggle/input/aptos2019/val_images/val_images/165c548185f8.png\n/kaggle/input/aptos2019/val_images/val_images/1601c939412f.png\n/kaggle/input/aptos2019/val_images/val_images/001639a390f0.png\n/kaggle/input/aptos2019/val_images/val_images/12ab2f6397f0.png\n/kaggle/input/aptos2019/val_images/val_images/07a2b8cabf6b.png\n/kaggle/input/aptos2019/val_images/val_images/0318598cfd16.png\n/kaggle/input/aptos2019/val_images/val_images/08bef347f40d.png\n/kaggle/input/aptos2019/val_images/val_images/1414128bead0.png\n/kaggle/input/aptos2019/val_images/val_images/103f97a2ab15.png\n/kaggle/input/aptos2019/val_images/val_images/03ff7d159f10.png\n/kaggle/input/aptos2019/val_images/val_images/18b99159a14f.png\n/kaggle/input/aptos2019/val_images/val_images/18cde9649e90.png\n/kaggle/input/aptos2019/val_images/val_images/1120f6d08d95.png\n/kaggle/input/aptos2019/val_images/val_images/094858f005ab.png\n/kaggle/input/aptos2019/val_images/val_images/1ab3f1c71a5f.png\n/kaggle/input/aptos2019/val_images/val_images/17d997fe1090.png\n/kaggle/input/aptos2019/val_images/val_images/12ce6a1a1f31.png\n/kaggle/input/aptos2019/val_images/val_images/11b5c77fbf79.png\n/kaggle/input/aptos2019/val_images/val_images/1177d583c807.png\n/kaggle/input/aptos2019/val_images/val_images/0c2e2369dfff.png\n/kaggle/input/aptos2019/val_images/val_images/099021fac3c9.png\n/kaggle/input/aptos2019/val_images/val_images/03747397839f.png\n/kaggle/input/aptos2019/val_images/val_images/0cae727cf119.png\n/kaggle/input/aptos2019/val_images/val_images/1a0dbc6c0cda.png\n/kaggle/input/aptos2019/val_images/val_images/12e3f5f2cb17.png\n/kaggle/input/aptos2019/val_images/val_images/01d9477b1171.png\n/kaggle/input/aptos2019/val_images/val_images/05a5183c92d0.png\n/kaggle/input/aptos2019/val_images/val_images/0231642cf1c2.png\n/kaggle/input/aptos2019/val_images/val_images/0da632ca45e0.png\n/kaggle/input/aptos2019/val_images/val_images/191348830ddf.png\n/kaggle/input/aptos2019/val_images/val_images/1633f8291a80.png\n/kaggle/input/aptos2019/val_images/val_images/18b06f56ab27.png\n/kaggle/input/aptos2019/val_images/val_images/0c917c372572.png\n/kaggle/input/aptos2019/val_images/val_images/06b71823f9cd.png\n/kaggle/input/aptos2019/val_images/val_images/1ae3c58759fb.png\n/kaggle/input/aptos2019/val_images/val_images/11242a67122d.png\n/kaggle/input/aptos2019/val_images/val_images/071435a218ec.png\n/kaggle/input/aptos2019/val_images/val_images/14c3b41d289c.png\n/kaggle/input/aptos2019/val_images/val_images/0304bedad8fe.png\n/kaggle/input/aptos2019/val_images/val_images/0b3efe669365.png\n/kaggle/input/aptos2019/val_images/val_images/10de500cf0c5.png\n/kaggle/input/aptos2019/val_images/val_images/0423237770a7.png\n/kaggle/input/aptos2019/val_images/val_images/180afe1d5ef7.png\n/kaggle/input/aptos2019/val_images/val_images/164cd5a3a6cd.png\n/kaggle/input/aptos2019/val_images/val_images/0a1076183736.png\n/kaggle/input/aptos2019/val_images/val_images/04579e31e4be.png\n/kaggle/input/aptos2019/val_images/val_images/1269ab57c2e6.png\n/kaggle/input/aptos2019/val_images/val_images/0182152c50de.png\n/kaggle/input/aptos2019/val_images/val_images/15c24478ac72.png\n/kaggle/input/aptos2019/val_images/val_images/03676c71ed1b.png\n/kaggle/input/aptos2019/val_images/val_images/096436d68d06.png\n/kaggle/input/aptos2019/val_images/val_images/1638404f385c.png\n/kaggle/input/aptos2019/val_images/val_images/19113e5f45ec.png\n/kaggle/input/aptos2019/val_images/val_images/17188c13e635.png\n/kaggle/input/aptos2019/val_images/val_images/15e24b73d4a7.png\n/kaggle/input/aptos2019/val_images/val_images/13d411c85ffd.png\n/kaggle/input/aptos2019/val_images/val_images/0981195eb9fb.png\n/kaggle/input/aptos2019/val_images/val_images/172df1330a60.png\n/kaggle/input/aptos2019/val_images/val_images/0fb560f9adb2.png\n/kaggle/input/aptos2019/val_images/val_images/15bed5adde74.png\n/kaggle/input/aptos2019/val_images/val_images/02cd34a85b24.png\n/kaggle/input/aptos2019/val_images/val_images/0fbbd665431f.png\n/kaggle/input/aptos2019/val_images/val_images/0773a1c326ad.png\n/kaggle/input/aptos2019/val_images/val_images/0eb52045349f.png\n/kaggle/input/aptos2019/val_images/val_images/191a711852bd.png\n/kaggle/input/aptos2019/val_images/val_images/05b1bb2bdb81.png\n/kaggle/input/aptos2019/val_images/val_images/0e82bcacc475.png\n/kaggle/input/aptos2019/val_images/val_images/11b220a397b8.png\n/kaggle/input/aptos2019/val_images/val_images/188219f2d9c6.png\n/kaggle/input/aptos2019/val_images/val_images/0232dfea7547.png\n/kaggle/input/aptos2019/val_images/val_images/1ab8d3431ffc.png\n/kaggle/input/aptos2019/val_images/val_images/025a169a0bb0.png\n/kaggle/input/aptos2019/val_images/val_images/070f67572d03.png\n/kaggle/input/aptos2019/val_images/val_images/1a1b4b2450ca.png\n/kaggle/input/aptos2019/val_images/val_images/08037e4490e5.png\n/kaggle/input/aptos2019/val_images/val_images/041f09eec1e8.png\n/kaggle/input/aptos2019/val_images/val_images/0e3572b5884a.png\n/kaggle/input/aptos2019/val_images/val_images/0124dffecf29.png\n/kaggle/input/aptos2019/val_images/val_images/07a1c7073982.png\n/kaggle/input/aptos2019/val_images/val_images/0369f3efe69b.png\n/kaggle/input/aptos2019/val_images/val_images/03fd50da928d.png\n/kaggle/input/aptos2019/val_images/val_images/141735b57ec0.png\n/kaggle/input/aptos2019/val_images/val_images/0c76fd494af6.png\n/kaggle/input/aptos2019/val_images/val_images/0e75d51152fc.png\n/kaggle/input/aptos2019/val_images/val_images/0a4e1a29ffff.png\n/kaggle/input/aptos2019/val_images/val_images/0da09e3ce8f1.png\n/kaggle/input/aptos2019/val_images/val_images/04ac765f91a1.png\n/kaggle/input/aptos2019/val_images/val_images/0684311afdfc.png\n/kaggle/input/aptos2019/val_images/val_images/135575dc57c9.png\n/kaggle/input/aptos2019/val_images/val_images/10f10fd30718.png\n/kaggle/input/aptos2019/val_images/val_images/0924cec998fa.png\n/kaggle/input/aptos2019/val_images/val_images/08c17a2d95b7.png\n/kaggle/input/aptos2019/val_images/val_images/08f8838d69bb.png\n/kaggle/input/aptos2019/val_images/val_images/0e94cd271c00.png\n/kaggle/input/aptos2019/val_images/val_images/0097f532ac9f.png\n/kaggle/input/aptos2019/val_images/val_images/19545647508e.png\n/kaggle/input/aptos2019/val_images/val_images/09934421c79e.png\n/kaggle/input/aptos2019/val_images/val_images/0da321efbce6.png\n/kaggle/input/aptos2019/val_images/val_images/07a0e34c8d20.png\n/kaggle/input/aptos2019/val_images/val_images/178412895d5e.png\n/kaggle/input/aptos2019/val_images/val_images/14e3f84445f7.png\n/kaggle/input/aptos2019/val_images/val_images/0ce062f26edc.png\n/kaggle/input/aptos2019/val_images/val_images/0d8f60ed9280.png\n/kaggle/input/aptos2019/val_images/val_images/0c7e82daf5a0.png\n/kaggle/input/aptos2019/val_images/val_images/012a242ac6ff.png\n/kaggle/input/aptos2019/val_images/val_images/18621b9ca978.png\n/kaggle/input/aptos2019/val_images/val_images/08a3875063c3.png\n/kaggle/input/aptos2019/val_images/val_images/0551676cc2aa.png\n/kaggle/input/aptos2019/val_images/val_images/0efc93ec838b.png\n/kaggle/input/aptos2019/val_images/val_images/0024cdab0c1e.png\n/kaggle/input/aptos2019/val_images/val_images/052d9a3fe55a.png\n/kaggle/input/aptos2019/val_images/val_images/0db1d8dcf219.png\n/kaggle/input/aptos2019/val_images/val_images/1a369baf9ee6.png\n/kaggle/input/aptos2019/val_images/val_images/1002b5151b8e.png\n/kaggle/input/aptos2019/val_images/val_images/165cd2070ebd.png\n/kaggle/input/aptos2019/val_images/val_images/080ee76c958c.png\n/kaggle/input/aptos2019/val_images/val_images/0e0fc1d9810c.png\n/kaggle/input/aptos2019/val_images/val_images/07419eddd6be.png\n/kaggle/input/aptos2019/val_images/val_images/01eb826f6467.png\n/kaggle/input/aptos2019/val_images/val_images/12ae44be0d38.png\n/kaggle/input/aptos2019/val_images/val_images/175dd560810a.png\n/kaggle/input/aptos2019/val_images/val_images/09f6ab477654.png\n/kaggle/input/aptos2019/val_images/val_images/09935d72892b.png\n/kaggle/input/aptos2019/val_images/val_images/0cb14014117d.png\n/kaggle/input/aptos2019/val_images/val_images/0dc031c94225.png\n/kaggle/input/aptos2019/val_images/val_images/0daddc45d832.png\n/kaggle/input/aptos2019/val_images/val_images/15f440753916.png\n/kaggle/input/aptos2019/val_images/val_images/03e25101e8e8.png\n/kaggle/input/aptos2019/val_images/val_images/0dc8d25b3f69.png\n/kaggle/input/aptos2019/val_images/val_images/07083738b75e.png\n/kaggle/input/aptos2019/val_images/val_images/0709652336e2.png\n/kaggle/input/aptos2019/val_images/val_images/189cbbc9e5e3.png\n/kaggle/input/aptos2019/val_images/val_images/155e2df6bfcf.png\n/kaggle/input/aptos2019/val_images/val_images/0790515cf5af.png\n/kaggle/input/aptos2019/val_images/val_images/0f495d87656a.png\n/kaggle/input/aptos2019/val_images/val_images/0d9a9896f801.png\n/kaggle/input/aptos2019/val_images/val_images/13063d1bc4ea.png\n/kaggle/input/aptos2019/val_images/val_images/0415fc68b176.png\n/kaggle/input/aptos2019/val_images/val_images/069f43616fab.png\n/kaggle/input/aptos2019/val_images/val_images/0a09aa7356c0.png\n/kaggle/input/aptos2019/val_images/val_images/14ee87d6cc42.png\n/kaggle/input/aptos2019/val_images/val_images/02dda30d3acf.png\n/kaggle/input/aptos2019/val_images/val_images/15528e740543.png\n/kaggle/input/aptos2019/val_images/val_images/0c43c79e8cfb.png\n/kaggle/input/aptos2019/val_images/val_images/184a185e7447.png\n/kaggle/input/aptos2019/val_images/val_images/13d014ccd136.png\n/kaggle/input/aptos2019/val_images/val_images/033f2b43de6d.png\n/kaggle/input/aptos2019/val_images/val_images/0ceb222f6629.png\n/kaggle/input/aptos2019/val_images/val_images/0cb6b898389f.png\n/kaggle/input/aptos2019/val_images/val_images/0a902c80d5da.png\n/kaggle/input/aptos2019/val_images/val_images/080f66eedfb9.png\n/kaggle/input/aptos2019/val_images/val_images/1891698febce.png\n/kaggle/input/aptos2019/val_images/val_images/0f6e645466a2.png\n/kaggle/input/aptos2019/val_images/val_images/10f36b0239fb.png\n/kaggle/input/aptos2019/val_images/val_images/165634a6167e.png\n/kaggle/input/aptos2019/val_images/val_images/0dce95217626.png\n/kaggle/input/aptos2019/val_images/val_images/07a3be30563b.png\n/kaggle/input/aptos2019/val_images/val_images/196e6a186452.png\n/kaggle/input/aptos2019/val_images/val_images/1a90fad9ffa2.png\n/kaggle/input/aptos2019/val_images/val_images/014508ccb9cb.png\n/kaggle/input/aptos2019/val_images/val_images/0fcfc6301f3d.png\n/kaggle/input/aptos2019/val_images/val_images/09f1111a388a.png\n/kaggle/input/aptos2019/val_images/val_images/08ee569d4721.png\n/kaggle/input/aptos2019/val_images/val_images/10fca1abf338.png\n/kaggle/input/aptos2019/val_images/val_images/00a8624548a9.png\n/kaggle/input/aptos2019/val_images/val_images/084c02cf077f.png\n/kaggle/input/aptos2019/val_images/val_images/07929d32b5b3.png\n/kaggle/input/aptos2019/val_images/val_images/01f7bb8be950.png\n/kaggle/input/aptos2019/val_images/val_images/034cb07a550f.png\n/kaggle/input/aptos2019/val_images/val_images/18b7e34eab8f.png\n/kaggle/input/aptos2019/val_images/val_images/0fe31196e0e8.png\n/kaggle/input/aptos2019/val_images/val_images/07122e268a1d.png\n/kaggle/input/aptos2019/val_images/val_images/050bb1eafa76.png\n/kaggle/input/aptos2019/val_images/val_images/07a0be6b347f.png\n/kaggle/input/aptos2019/val_images/val_images/15cc2aef772a.png\n/kaggle/input/aptos2019/val_images/val_images/1a1e974a7dbf.png\n/kaggle/input/aptos2019/val_images/val_images/0edadb2aa127.png\n/kaggle/input/aptos2019/val_images/val_images/08c60c647673.png\n/kaggle/input/aptos2019/val_images/val_images/07e827469099.png\n/kaggle/input/aptos2019/val_images/val_images/18d8fdb140b7.png\n/kaggle/input/aptos2019/val_images/val_images/12bc439d373a.png\n/kaggle/input/aptos2019/val_images/val_images/1116271db4ea.png\n/kaggle/input/aptos2019/val_images/val_images/09237bf783a4.png\n/kaggle/input/aptos2019/val_images/val_images/00cb6555d108.png\n/kaggle/input/aptos2019/val_images/val_images/1002f3fe38f0.png\n/kaggle/input/aptos2019/val_images/val_images/05113073b268.png\n/kaggle/input/aptos2019/val_images/val_images/1411c8ab7161.png\n/kaggle/input/aptos2019/val_images/val_images/0953c0ac1735.png\n/kaggle/input/aptos2019/val_images/val_images/103abbd8b63e.png\n/kaggle/input/aptos2019/train_images/train_images/6dcde47060f9.png\n/kaggle/input/aptos2019/train_images/train_images/b49b2fac2514.png\n/kaggle/input/aptos2019/train_images/train_images/af6166d57f13.png\n/kaggle/input/aptos2019/train_images/train_images/8d13c46e7d75.png\n/kaggle/input/aptos2019/train_images/train_images/c3b15bf9b4bc.png\n/kaggle/input/aptos2019/train_images/train_images/be68322c7223.png\n/kaggle/input/aptos2019/train_images/train_images/88e4399d207c.png\n/kaggle/input/aptos2019/train_images/train_images/77ab222bf85c.png\n/kaggle/input/aptos2019/train_images/train_images/4a05f81b3aba.png\n/kaggle/input/aptos2019/train_images/train_images/61d9c88a3a4b.png\n/kaggle/input/aptos2019/train_images/train_images/8448af27ba07.png\n/kaggle/input/aptos2019/train_images/train_images/4554062fa836.png\n/kaggle/input/aptos2019/train_images/train_images/a688f20f8895.png\n/kaggle/input/aptos2019/train_images/train_images/3af9aaa880e9.png\n/kaggle/input/aptos2019/train_images/train_images/cb68fce07789.png\n/kaggle/input/aptos2019/train_images/train_images/5d9c841eb245.png\n/kaggle/input/aptos2019/train_images/train_images/d4be0403e6ab.png\n/kaggle/input/aptos2019/train_images/train_images/388f12e8df0b.png\n/kaggle/input/aptos2019/train_images/train_images/bb7e0a2544cd.png\n/kaggle/input/aptos2019/train_images/train_images/1d29cb2f4296.png\n/kaggle/input/aptos2019/train_images/train_images/d83c3efade75.png\n/kaggle/input/aptos2019/train_images/train_images/2927665214e1.png\n/kaggle/input/aptos2019/train_images/train_images/1e4650743fa2.png\n/kaggle/input/aptos2019/train_images/train_images/abdb365cacbc.png\n/kaggle/input/aptos2019/train_images/train_images/d15ca3469b87.png\n/kaggle/input/aptos2019/train_images/train_images/576e189d23d4.png\n/kaggle/input/aptos2019/train_images/train_images/33ffddea8c6e.png\n/kaggle/input/aptos2019/train_images/train_images/28f93cad89c5.png\n/kaggle/input/aptos2019/train_images/train_images/8af50c9d0a86.png\n/kaggle/input/aptos2019/train_images/train_images/932181b93b2f.png\n/kaggle/input/aptos2019/train_images/train_images/c68dfa021d62.png\n/kaggle/input/aptos2019/train_images/train_images/a4d4b69f7404.png\n/kaggle/input/aptos2019/train_images/train_images/278aa860dffd.png\n/kaggle/input/aptos2019/train_images/train_images/90bde2ff8953.png\n/kaggle/input/aptos2019/train_images/train_images/7d1b40fdbd86.png\n/kaggle/input/aptos2019/train_images/train_images/9f285b3e57ed.png\n/kaggle/input/aptos2019/train_images/train_images/c252da9b41d8.png\n/kaggle/input/aptos2019/train_images/train_images/dfea19863428.png\n/kaggle/input/aptos2019/train_images/train_images/8ee50c26fc13.png\n/kaggle/input/aptos2019/train_images/train_images/ba0107fb1bfd.png\n/kaggle/input/aptos2019/train_images/train_images/436fa3fd145a.png\n/kaggle/input/aptos2019/train_images/train_images/8fd7ad26e691.png\n/kaggle/input/aptos2019/train_images/train_images/82910bba4753.png\n/kaggle/input/aptos2019/train_images/train_images/a9e984b57556.png\n/kaggle/input/aptos2019/train_images/train_images/75ed83dbccce.png\n/kaggle/input/aptos2019/train_images/train_images/d66b6f333dc7.png\n/kaggle/input/aptos2019/train_images/train_images/45369821d6a3.png\n/kaggle/input/aptos2019/train_images/train_images/65c958379680.png\n/kaggle/input/aptos2019/train_images/train_images/b83a6eca125f.png\n/kaggle/input/aptos2019/train_images/train_images/79cbae28d8b2.png\n/kaggle/input/aptos2019/train_images/train_images/69b3a00927fc.png\n/kaggle/input/aptos2019/train_images/train_images/b05922e7abd3.png\n/kaggle/input/aptos2019/train_images/train_images/4ffa38550c95.png\n/kaggle/input/aptos2019/train_images/train_images/cb547e723a16.png\n/kaggle/input/aptos2019/train_images/train_images/a8c950a99107.png\n/kaggle/input/aptos2019/train_images/train_images/549381330191.png\n/kaggle/input/aptos2019/train_images/train_images/705f508d1e42.png\n/kaggle/input/aptos2019/train_images/train_images/934104859e68.png\n/kaggle/input/aptos2019/train_images/train_images/36e4b704b905.png\n/kaggle/input/aptos2019/train_images/train_images/6666c4f18396.png\n/kaggle/input/aptos2019/train_images/train_images/7525ebb3434d.png\n/kaggle/input/aptos2019/train_images/train_images/6363b360aefb.png\n/kaggle/input/aptos2019/train_images/train_images/a06b353e7bed.png\n/kaggle/input/aptos2019/train_images/train_images/8a9bef2fbd4e.png\n/kaggle/input/aptos2019/train_images/train_images/5078caaf1f57.png\n/kaggle/input/aptos2019/train_images/train_images/5ac7a414560e.png\n/kaggle/input/aptos2019/train_images/train_images/9bd008aab548.png\n/kaggle/input/aptos2019/train_images/train_images/77acc2cafee1.png\n/kaggle/input/aptos2019/train_images/train_images/812d5adafaf2.png\n/kaggle/input/aptos2019/train_images/train_images/ca891d37a43c.png\n/kaggle/input/aptos2019/train_images/train_images/437cbec4a3f8.png\n/kaggle/input/aptos2019/train_images/train_images/d865997a6280.png\n/kaggle/input/aptos2019/train_images/train_images/6294b378d09f.png\n/kaggle/input/aptos2019/train_images/train_images/cab3dfa7962d.png\n/kaggle/input/aptos2019/train_images/train_images/6b869f37cdf3.png\n/kaggle/input/aptos2019/train_images/train_images/ab7991df166b.png\n/kaggle/input/aptos2019/train_images/train_images/63363410389a.png\n/kaggle/input/aptos2019/train_images/train_images/e229aca862c7.png\n/kaggle/input/aptos2019/train_images/train_images/4da2961e62fe.png\n/kaggle/input/aptos2019/train_images/train_images/3a122851e526.png\n/kaggle/input/aptos2019/train_images/train_images/958c1fa044ba.png\n/kaggle/input/aptos2019/train_images/train_images/64bad93fde3f.png\n/kaggle/input/aptos2019/train_images/train_images/906d02fb822d.png\n/kaggle/input/aptos2019/train_images/train_images/a384e688e228.png\n/kaggle/input/aptos2019/train_images/train_images/22098b1fe461.png\n/kaggle/input/aptos2019/train_images/train_images/2e26762daed5.png\n/kaggle/input/aptos2019/train_images/train_images/9a28d4e8aef0.png\n/kaggle/input/aptos2019/train_images/train_images/530d78467615.png\n/kaggle/input/aptos2019/train_images/train_images/cf1b9d26d38d.png\n/kaggle/input/aptos2019/train_images/train_images/3ddb86eb530e.png\n/kaggle/input/aptos2019/train_images/train_images/6c2555a9cae4.png\n/kaggle/input/aptos2019/train_images/train_images/4a589edaea60.png\n/kaggle/input/aptos2019/train_images/train_images/ccd6dcb2f568.png\n/kaggle/input/aptos2019/train_images/train_images/2fefb720869a.png\n/kaggle/input/aptos2019/train_images/train_images/da44f80b422b.png\n/kaggle/input/aptos2019/train_images/train_images/31616ff6b53b.png\n/kaggle/input/aptos2019/train_images/train_images/bd500a73beae.png\n/kaggle/input/aptos2019/train_images/train_images/4a5a6efc0bef.png\n/kaggle/input/aptos2019/train_images/train_images/aef9016557ca.png\n/kaggle/input/aptos2019/train_images/train_images/4c129470cec4.png\n/kaggle/input/aptos2019/train_images/train_images/252305189b3a.png\n/kaggle/input/aptos2019/train_images/train_images/639915f58a2f.png\n/kaggle/input/aptos2019/train_images/train_images/8a759f94613a.png\n/kaggle/input/aptos2019/train_images/train_images/6b07971c3bf6.png\n/kaggle/input/aptos2019/train_images/train_images/c4e8b1ec8893.png\n/kaggle/input/aptos2019/train_images/train_images/2ba0b0d9bda2.png\n/kaggle/input/aptos2019/train_images/train_images/31b5d6fb0256.png\n/kaggle/input/aptos2019/train_images/train_images/9faad91b6578.png\n/kaggle/input/aptos2019/train_images/train_images/64b9206afb3f.png\n/kaggle/input/aptos2019/train_images/train_images/cbc23af521f3.png\n/kaggle/input/aptos2019/train_images/train_images/b1197f2cc9b3.png\n/kaggle/input/aptos2019/train_images/train_images/b91ef82e723a.png\n/kaggle/input/aptos2019/train_images/train_images/71e4130bf5c8.png\n/kaggle/input/aptos2019/train_images/train_images/90c982cc2d96.png\n/kaggle/input/aptos2019/train_images/train_images/3601dac9bed7.png\n/kaggle/input/aptos2019/train_images/train_images/d1afdb8cf70d.png\n/kaggle/input/aptos2019/train_images/train_images/a3706ce27869.png\n/kaggle/input/aptos2019/train_images/train_images/1f15ca672675.png\n/kaggle/input/aptos2019/train_images/train_images/a12ca80bb8c7.png\n/kaggle/input/aptos2019/train_images/train_images/58184d6fd087.png\n/kaggle/input/aptos2019/train_images/train_images/3ca8be3b40d6.png\n/kaggle/input/aptos2019/train_images/train_images/529906ff9dfa.png\n/kaggle/input/aptos2019/train_images/train_images/367c7049929c.png\n/kaggle/input/aptos2019/train_images/train_images/4409965eb2a4.png\n/kaggle/input/aptos2019/train_images/train_images/2a8a9e957a6c.png\n/kaggle/input/aptos2019/train_images/train_images/b0d35981708b.png\n/kaggle/input/aptos2019/train_images/train_images/847b04287c9c.png\n/kaggle/input/aptos2019/train_images/train_images/4d17559ac1e2.png\n/kaggle/input/aptos2019/train_images/train_images/87774aafe068.png\n/kaggle/input/aptos2019/train_images/train_images/a8263d248523.png\n/kaggle/input/aptos2019/train_images/train_images/9eaac43744f5.png\n/kaggle/input/aptos2019/train_images/train_images/51030843fde2.png\n/kaggle/input/aptos2019/train_images/train_images/b6bf847fbcb2.png\n/kaggle/input/aptos2019/train_images/train_images/62ecdc90dd42.png\n/kaggle/input/aptos2019/train_images/train_images/a7b7dc8788b9.png\n/kaggle/input/aptos2019/train_images/train_images/8a3eb86ae4bd.png\n/kaggle/input/aptos2019/train_images/train_images/8421107255ae.png\n/kaggle/input/aptos2019/train_images/train_images/76be29bb30b2.png\n/kaggle/input/aptos2019/train_images/train_images/876deb29f000.png\n/kaggle/input/aptos2019/train_images/train_images/42b08dca9b2f.png\n/kaggle/input/aptos2019/train_images/train_images/9d9bfefa809c.png\n/kaggle/input/aptos2019/train_images/train_images/2b48daf24be0.png\n/kaggle/input/aptos2019/train_images/train_images/c2f3281cf528.png\n/kaggle/input/aptos2019/train_images/train_images/cadde4030858.png\n/kaggle/input/aptos2019/train_images/train_images/63c0eafd6aa9.png\n/kaggle/input/aptos2019/train_images/train_images/302bcdb635ff.png\n/kaggle/input/aptos2019/train_images/train_images/290ecdba359f.png\n/kaggle/input/aptos2019/train_images/train_images/b9b99dad668d.png\n/kaggle/input/aptos2019/train_images/train_images/436e7a7af761.png\n/kaggle/input/aptos2019/train_images/train_images/4826d10030b3.png\n/kaggle/input/aptos2019/train_images/train_images/4489d421e5aa.png\n/kaggle/input/aptos2019/train_images/train_images/33778d136069.png\n/kaggle/input/aptos2019/train_images/train_images/ca2b54b95ade.png\n/kaggle/input/aptos2019/train_images/train_images/2682e6da9050.png\n/kaggle/input/aptos2019/train_images/train_images/8000a6b97a84.png\n/kaggle/input/aptos2019/train_images/train_images/873fe0404d6e.png\n/kaggle/input/aptos2019/train_images/train_images/1c578b72d7b3.png\n/kaggle/input/aptos2019/train_images/train_images/a0445785e2f7.png\n/kaggle/input/aptos2019/train_images/train_images/e1ab92228e60.png\n/kaggle/input/aptos2019/train_images/train_images/a3fcf42ff56d.png\n/kaggle/input/aptos2019/train_images/train_images/97da093947e8.png\n/kaggle/input/aptos2019/train_images/train_images/c5a9ebef1517.png\n/kaggle/input/aptos2019/train_images/train_images/9985375d709f.png\n/kaggle/input/aptos2019/train_images/train_images/b549af91bd30.png\n/kaggle/input/aptos2019/train_images/train_images/58f07741ee3b.png\n/kaggle/input/aptos2019/train_images/train_images/8344c783da65.png\n/kaggle/input/aptos2019/train_images/train_images/9870ce41cac4.png\n/kaggle/input/aptos2019/train_images/train_images/2c827005b8f8.png\n/kaggle/input/aptos2019/train_images/train_images/8d3d67661620.png\n/kaggle/input/aptos2019/train_images/train_images/8ed586c43023.png\n/kaggle/input/aptos2019/train_images/train_images/4a3da369b227.png\n/kaggle/input/aptos2019/train_images/train_images/849a91e9ab28.png\n/kaggle/input/aptos2019/train_images/train_images/9a78c6a7b1c2.png\n/kaggle/input/aptos2019/train_images/train_images/ba4d2c4b3039.png\n/kaggle/input/aptos2019/train_images/train_images/5c7ab966a3ee.png\n/kaggle/input/aptos2019/train_images/train_images/97a5ad7548b7.png\n/kaggle/input/aptos2019/train_images/train_images/4dc2211a1c31.png\n/kaggle/input/aptos2019/train_images/train_images/7d3835e4e63a.png\n/kaggle/input/aptos2019/train_images/train_images/61a62b1dcc36.png\n/kaggle/input/aptos2019/train_images/train_images/d1a60c3b9fe5.png\n/kaggle/input/aptos2019/train_images/train_images/b9d0b83d70c3.png\n/kaggle/input/aptos2019/train_images/train_images/521b5377a727.png\n/kaggle/input/aptos2019/train_images/train_images/44e0d56e9d42.png\n/kaggle/input/aptos2019/train_images/train_images/a8a6588c8eb7.png\n/kaggle/input/aptos2019/train_images/train_images/7a238a1d3cf3.png\n/kaggle/input/aptos2019/train_images/train_images/d3d578fe433f.png\n/kaggle/input/aptos2019/train_images/train_images/930fee99213a.png\n/kaggle/input/aptos2019/train_images/train_images/c67117c6ab3b.png\n/kaggle/input/aptos2019/train_images/train_images/1c47815f4a6b.png\n/kaggle/input/aptos2019/train_images/train_images/a419fcb2dfb5.png\n/kaggle/input/aptos2019/train_images/train_images/8f2996b8d855.png\n/kaggle/input/aptos2019/train_images/train_images/2b4c7b5f1f1e.png\n/kaggle/input/aptos2019/train_images/train_images/913b1890ed1e.png\n/kaggle/input/aptos2019/train_images/train_images/2f284b6a1940.png\n/kaggle/input/aptos2019/train_images/train_images/246e4506824a.png\n/kaggle/input/aptos2019/train_images/train_images/650fbed3fdca.png\n/kaggle/input/aptos2019/train_images/train_images/9b7b6e4db1d5.png\n/kaggle/input/aptos2019/train_images/train_images/7ddcfcea7369.png\n/kaggle/input/aptos2019/train_images/train_images/e499434242cc.png\n/kaggle/input/aptos2019/train_images/train_images/73ef3c3dcbe4.png\n/kaggle/input/aptos2019/train_images/train_images/bda8c973b09d.png\n/kaggle/input/aptos2019/train_images/train_images/df8365d6ac33.png\n/kaggle/input/aptos2019/train_images/train_images/207dd0487264.png\n/kaggle/input/aptos2019/train_images/train_images/c3acf47700ea.png\n/kaggle/input/aptos2019/train_images/train_images/d8da9de62743.png\n/kaggle/input/aptos2019/train_images/train_images/4b6cb0bcfd44.png\n/kaggle/input/aptos2019/train_images/train_images/a125377fb985.png\n/kaggle/input/aptos2019/train_images/train_images/ae94ce412de9.png\n/kaggle/input/aptos2019/train_images/train_images/66393d8c60ba.png\n/kaggle/input/aptos2019/train_images/train_images/db49cdf1ea64.png\n/kaggle/input/aptos2019/train_images/train_images/7eee3d1f1268.png\n/kaggle/input/aptos2019/train_images/train_images/cc3d2e961768.png\n/kaggle/input/aptos2019/train_images/train_images/6d454444f17c.png\n/kaggle/input/aptos2019/train_images/train_images/b5a3ca5c0a80.png\n/kaggle/input/aptos2019/train_images/train_images/c76664770c07.png\n/kaggle/input/aptos2019/train_images/train_images/28751f290ba3.png\n/kaggle/input/aptos2019/train_images/train_images/348598d01e18.png\n/kaggle/input/aptos2019/train_images/train_images/4ef7144e24ff.png\n/kaggle/input/aptos2019/train_images/train_images/5bea250d8bf5.png\n/kaggle/input/aptos2019/train_images/train_images/4f20f9a9a65b.png\n/kaggle/input/aptos2019/train_images/train_images/b9fe7da14a32.png\n/kaggle/input/aptos2019/train_images/train_images/524f240e0c90.png\n/kaggle/input/aptos2019/train_images/train_images/a53d6d2472a6.png\n/kaggle/input/aptos2019/train_images/train_images/799214e8b07c.png\n/kaggle/input/aptos2019/train_images/train_images/d4bc001f7224.png\n/kaggle/input/aptos2019/train_images/train_images/7269a1d84a57.png\n/kaggle/input/aptos2019/train_images/train_images/7b29e3783919.png\n/kaggle/input/aptos2019/train_images/train_images/b640e3bdff75.png\n/kaggle/input/aptos2019/train_images/train_images/7a3ea1779b13.png\n/kaggle/input/aptos2019/train_images/train_images/7ed4128b2a4e.png\n/kaggle/input/aptos2019/train_images/train_images/42a850acd2ac.png\n/kaggle/input/aptos2019/train_images/train_images/880edb2cdb69.png\n/kaggle/input/aptos2019/train_images/train_images/7c90ab025331.png\n/kaggle/input/aptos2019/train_images/train_images/6889bc64ab09.png\n/kaggle/input/aptos2019/train_images/train_images/80dbeb0fdc75.png\n/kaggle/input/aptos2019/train_images/train_images/5efa24b03d5e.png\n/kaggle/input/aptos2019/train_images/train_images/6b7cf869622a.png\n/kaggle/input/aptos2019/train_images/train_images/65e120143825.png\n/kaggle/input/aptos2019/train_images/train_images/3402124408ea.png\n/kaggle/input/aptos2019/train_images/train_images/a484bdf85b4c.png\n/kaggle/input/aptos2019/train_images/train_images/5889a0c75cac.png\n/kaggle/input/aptos2019/train_images/train_images/d6130f2ec903.png\n/kaggle/input/aptos2019/train_images/train_images/921433215353.png\n/kaggle/input/aptos2019/train_images/train_images/46923eea9a4e.png\n/kaggle/input/aptos2019/train_images/train_images/6b664ed2a938.png\n/kaggle/input/aptos2019/train_images/train_images/9ba469af2980.png\n/kaggle/input/aptos2019/train_images/train_images/7d48f8cdfb69.png\n/kaggle/input/aptos2019/train_images/train_images/90b8bf342032.png\n/kaggle/input/aptos2019/train_images/train_images/59ee65760535.png\n/kaggle/input/aptos2019/train_images/train_images/3ca12e02dd4e.png\n/kaggle/input/aptos2019/train_images/train_images/6f689fced922.png\n/kaggle/input/aptos2019/train_images/train_images/22449af52060.png\n/kaggle/input/aptos2019/train_images/train_images/e47770a2e5d1.png\n/kaggle/input/aptos2019/train_images/train_images/26d60db3bbfd.png\n/kaggle/input/aptos2019/train_images/train_images/513b0a4651fa.png\n/kaggle/input/aptos2019/train_images/train_images/bdc6c60e9133.png\n/kaggle/input/aptos2019/train_images/train_images/d91273efb92a.png\n/kaggle/input/aptos2019/train_images/train_images/417f408ee8e0.png\n/kaggle/input/aptos2019/train_images/train_images/976082127e2a.png\n/kaggle/input/aptos2019/train_images/train_images/4ee1ad981a6d.png\n/kaggle/input/aptos2019/train_images/train_images/df9cb3729eb1.png\n/kaggle/input/aptos2019/train_images/train_images/547b37da9223.png\n/kaggle/input/aptos2019/train_images/train_images/91b7a4179ecf.png\n/kaggle/input/aptos2019/train_images/train_images/d968a983d4d2.png\n/kaggle/input/aptos2019/train_images/train_images/c373b73a80c8.png\n/kaggle/input/aptos2019/train_images/train_images/3dbfbc11e105.png\n/kaggle/input/aptos2019/train_images/train_images/d51b3fe0fa1b.png\n/kaggle/input/aptos2019/train_images/train_images/c1ebe785503a.png\n/kaggle/input/aptos2019/train_images/train_images/1b329a127307.png\n/kaggle/input/aptos2019/train_images/train_images/4dd5d5ccddcf.png\n/kaggle/input/aptos2019/train_images/train_images/57760be09c03.png\n/kaggle/input/aptos2019/train_images/train_images/a07d9a5045cb.png\n/kaggle/input/aptos2019/train_images/train_images/6c250a30593b.png\n/kaggle/input/aptos2019/train_images/train_images/4f7755e74a9e.png\n/kaggle/input/aptos2019/train_images/train_images/9e5737f771c3.png\n/kaggle/input/aptos2019/train_images/train_images/73d40ce06a67.png\n/kaggle/input/aptos2019/train_images/train_images/1b8701231c8f.png\n/kaggle/input/aptos2019/train_images/train_images/308f7fce6f0d.png\n/kaggle/input/aptos2019/train_images/train_images/65a3b13ad9a0.png\n/kaggle/input/aptos2019/train_images/train_images/901a3552fe26.png\n/kaggle/input/aptos2019/train_images/train_images/b532dedd928c.png\n/kaggle/input/aptos2019/train_images/train_images/49eb73968c44.png\n/kaggle/input/aptos2019/train_images/train_images/d02b79fc3200.png\n/kaggle/input/aptos2019/train_images/train_images/ab50123abadb.png\n/kaggle/input/aptos2019/train_images/train_images/7d0a871c45db.png\n/kaggle/input/aptos2019/train_images/train_images/59fee5bc3479.png\n/kaggle/input/aptos2019/train_images/train_images/1dd9adcbfff4.png\n/kaggle/input/aptos2019/train_images/train_images/e0863b353093.png\n/kaggle/input/aptos2019/train_images/train_images/4d7d6928534a.png\n/kaggle/input/aptos2019/train_images/train_images/7bc00e58d419.png\n/kaggle/input/aptos2019/train_images/train_images/4aa3d771c5e4.png\n/kaggle/input/aptos2019/train_images/train_images/c23ff6dcf15e.png\n/kaggle/input/aptos2019/train_images/train_images/6f460f9968c7.png\n/kaggle/input/aptos2019/train_images/train_images/50916d67bb51.png\n/kaggle/input/aptos2019/train_images/train_images/1cb6961d141c.png\n/kaggle/input/aptos2019/train_images/train_images/542964865b1e.png\n/kaggle/input/aptos2019/train_images/train_images/24de56d433cd.png\n/kaggle/input/aptos2019/train_images/train_images/8e7981855125.png\n/kaggle/input/aptos2019/train_images/train_images/499c8df39222.png\n/kaggle/input/aptos2019/train_images/train_images/99ecdb41d5e7.png\n/kaggle/input/aptos2019/train_images/train_images/a95d9d61ddd4.png\n/kaggle/input/aptos2019/train_images/train_images/b67ae80f7eba.png\n/kaggle/input/aptos2019/train_images/train_images/ba08cee68c71.png\n/kaggle/input/aptos2019/train_images/train_images/c56e65f74187.png\n/kaggle/input/aptos2019/train_images/train_images/3132556f5352.png\n/kaggle/input/aptos2019/train_images/train_images/e1c02f6c3362.png\n/kaggle/input/aptos2019/train_images/train_images/a1b12fdce6c3.png\n/kaggle/input/aptos2019/train_images/train_images/6504b703c429.png\n/kaggle/input/aptos2019/train_images/train_images/1d674e2e32e0.png\n/kaggle/input/aptos2019/train_images/train_images/d69698f838db.png\n/kaggle/input/aptos2019/train_images/train_images/80c67efc8101.png\n/kaggle/input/aptos2019/train_images/train_images/cbe633765ea7.png\n/kaggle/input/aptos2019/train_images/train_images/a2b97d98f130.png\n/kaggle/input/aptos2019/train_images/train_images/3218a6d8eb2c.png\n/kaggle/input/aptos2019/train_images/train_images/545df1bbcd61.png\n/kaggle/input/aptos2019/train_images/train_images/4528fbbd43a3.png\n/kaggle/input/aptos2019/train_images/train_images/a49b0b4484ea.png\n/kaggle/input/aptos2019/train_images/train_images/52dbec057cc8.png\n/kaggle/input/aptos2019/train_images/train_images/48c72dec46e5.png\n/kaggle/input/aptos2019/train_images/train_images/b191ba0a2b12.png\n/kaggle/input/aptos2019/train_images/train_images/2bbcfdc477db.png\n/kaggle/input/aptos2019/train_images/train_images/7ccf9d25dc48.png\n/kaggle/input/aptos2019/train_images/train_images/bb11db08584a.png\n/kaggle/input/aptos2019/train_images/train_images/c25e02b39c01.png\n/kaggle/input/aptos2019/train_images/train_images/35aa7f5c2ec0.png\n/kaggle/input/aptos2019/train_images/train_images/454792eb6e05.png\n/kaggle/input/aptos2019/train_images/train_images/441affbe99aa.png\n/kaggle/input/aptos2019/train_images/train_images/7a77c3eb468c.png\n/kaggle/input/aptos2019/train_images/train_images/1c9521878baf.png\n/kaggle/input/aptos2019/train_images/train_images/43fb6eda9b97.png\n/kaggle/input/aptos2019/train_images/train_images/a56729de89e9.png\n/kaggle/input/aptos2019/train_images/train_images/3aa2b1ce6700.png\n/kaggle/input/aptos2019/train_images/train_images/cec299c2a2d5.png\n/kaggle/input/aptos2019/train_images/train_images/49e4b95ee2dc.png\n/kaggle/input/aptos2019/train_images/train_images/b73d0bcd3d33.png\n/kaggle/input/aptos2019/train_images/train_images/af8aa32beee4.png\n/kaggle/input/aptos2019/train_images/train_images/51af8a689511.png\n/kaggle/input/aptos2019/train_images/train_images/3044022c6969.png\n/kaggle/input/aptos2019/train_images/train_images/af3b0115aad1.png\n/kaggle/input/aptos2019/train_images/train_images/7f2123bc89a3.png\n/kaggle/input/aptos2019/train_images/train_images/ca360bec5851.png\n/kaggle/input/aptos2019/train_images/train_images/49d69c4c6290.png\n/kaggle/input/aptos2019/train_images/train_images/7435e9a3e36e.png\n/kaggle/input/aptos2019/train_images/train_images/bf811911acf9.png\n/kaggle/input/aptos2019/train_images/train_images/a75bab2463d4.png\n/kaggle/input/aptos2019/train_images/train_images/bf7047dc683c.png\n/kaggle/input/aptos2019/train_images/train_images/9fab29e69a6b.png\n/kaggle/input/aptos2019/train_images/train_images/52230bbef30e.png\n/kaggle/input/aptos2019/train_images/train_images/de16416220de.png\n/kaggle/input/aptos2019/train_images/train_images/2131aa3a1e6f.png\n/kaggle/input/aptos2019/train_images/train_images/835b9f6e12ba.png\n/kaggle/input/aptos2019/train_images/train_images/d16e39b9d6f0.png\n/kaggle/input/aptos2019/train_images/train_images/c0a117de7d0a.png\n/kaggle/input/aptos2019/train_images/train_images/ab32db41c409.png\n/kaggle/input/aptos2019/train_images/train_images/2eba4279e503.png\n/kaggle/input/aptos2019/train_images/train_images/3c28fd846b43.png\n/kaggle/input/aptos2019/train_images/train_images/e30a890600e1.png\n/kaggle/input/aptos2019/train_images/train_images/6fbaaf8eb67a.png\n/kaggle/input/aptos2019/train_images/train_images/b6d9974443ce.png\n/kaggle/input/aptos2019/train_images/train_images/b6a0e348a01e.png\n/kaggle/input/aptos2019/train_images/train_images/e4d3d437b0a8.png\n/kaggle/input/aptos2019/train_images/train_images/4a558a1cd243.png\n/kaggle/input/aptos2019/train_images/train_images/1d3e9b939732.png\n/kaggle/input/aptos2019/train_images/train_images/628b581aa905.png\n/kaggle/input/aptos2019/train_images/train_images/7e160c8b611e.png\n/kaggle/input/aptos2019/train_images/train_images/be7f791a7877.png\n/kaggle/input/aptos2019/train_images/train_images/a664d2055886.png\n/kaggle/input/aptos2019/train_images/train_images/d2ffe9287dc7.png\n/kaggle/input/aptos2019/train_images/train_images/55eac26bd383.png\n/kaggle/input/aptos2019/train_images/train_images/b9127e38d9b9.png\n/kaggle/input/aptos2019/train_images/train_images/460893cd86e3.png\n/kaggle/input/aptos2019/train_images/train_images/5a27b9b2a9c1.png\n/kaggle/input/aptos2019/train_images/train_images/9bbb6c455913.png\n/kaggle/input/aptos2019/train_images/train_images/d95959798b57.png\n/kaggle/input/aptos2019/train_images/train_images/2e79041ef722.png\n/kaggle/input/aptos2019/train_images/train_images/da9262d9f5d9.png\n/kaggle/input/aptos2019/train_images/train_images/a82a12ad3fb1.png\n/kaggle/input/aptos2019/train_images/train_images/d3e884109b45.png\n/kaggle/input/aptos2019/train_images/train_images/6d292ca4c9ad.png\n/kaggle/input/aptos2019/train_images/train_images/d0d59ed675b5.png\n/kaggle/input/aptos2019/train_images/train_images/3b0190bbe615.png\n/kaggle/input/aptos2019/train_images/train_images/2ef955d6d9ff.png\n/kaggle/input/aptos2019/train_images/train_images/d6f36ec5564a.png\n/kaggle/input/aptos2019/train_images/train_images/3f752fcccec0.png\n/kaggle/input/aptos2019/train_images/train_images/a62e995f167c.png\n/kaggle/input/aptos2019/train_images/train_images/4145dcb25053.png\n/kaggle/input/aptos2019/train_images/train_images/4b422b48d0d4.png\n/kaggle/input/aptos2019/train_images/train_images/1c3a6b4449e9.png\n/kaggle/input/aptos2019/train_images/train_images/de57c9e9fa93.png\n/kaggle/input/aptos2019/train_images/train_images/bc34ed91c9bc.png\n/kaggle/input/aptos2019/train_images/train_images/92e3d608fd3c.png\n/kaggle/input/aptos2019/train_images/train_images/66460ecab347.png\n/kaggle/input/aptos2019/train_images/train_images/c6a145742708.png\n/kaggle/input/aptos2019/train_images/train_images/51405d042000.png\n/kaggle/input/aptos2019/train_images/train_images/afc345cc9145.png\n/kaggle/input/aptos2019/train_images/train_images/1f9ccda4ddf2.png\n/kaggle/input/aptos2019/train_images/train_images/261c6bd63bff.png\n/kaggle/input/aptos2019/train_images/train_images/59f3f70abddd.png\n/kaggle/input/aptos2019/train_images/train_images/cf8f1bc7a215.png\n/kaggle/input/aptos2019/train_images/train_images/942f544c4e15.png\n/kaggle/input/aptos2019/train_images/train_images/aa60813e1a8d.png\n/kaggle/input/aptos2019/train_images/train_images/6bf26b777e3a.png\n/kaggle/input/aptos2019/train_images/train_images/db4ed1e07aa3.png\n/kaggle/input/aptos2019/train_images/train_images/7ec1ffe8220b.png\n/kaggle/input/aptos2019/train_images/train_images/6f4719c6bb4b.png\n/kaggle/input/aptos2019/train_images/train_images/b07bc463b718.png\n/kaggle/input/aptos2019/train_images/train_images/9dab2e6ba44b.png\n/kaggle/input/aptos2019/train_images/train_images/226c6ceb9185.png\n/kaggle/input/aptos2019/train_images/train_images/382752f6694a.png\n/kaggle/input/aptos2019/train_images/train_images/5265dc9acdf8.png\n/kaggle/input/aptos2019/train_images/train_images/5f4a8c074bd5.png\n/kaggle/input/aptos2019/train_images/train_images/29a13e666266.png\n/kaggle/input/aptos2019/train_images/train_images/d659d7fd5ccf.png\n/kaggle/input/aptos2019/train_images/train_images/bd34a0639575.png\n/kaggle/input/aptos2019/train_images/train_images/44976c3b11a6.png\n/kaggle/input/aptos2019/train_images/train_images/d0b132d2c7ec.png\n/kaggle/input/aptos2019/train_images/train_images/b8e20c076b03.png\n/kaggle/input/aptos2019/train_images/train_images/abf0f56c6f12.png\n/kaggle/input/aptos2019/train_images/train_images/ad029ba7fa8b.png\n/kaggle/input/aptos2019/train_images/train_images/ad312ca98202.png\n/kaggle/input/aptos2019/train_images/train_images/737ef6226677.png\n/kaggle/input/aptos2019/train_images/train_images/5dd2e26fc244.png\n/kaggle/input/aptos2019/train_images/train_images/5c8482926a08.png\n/kaggle/input/aptos2019/train_images/train_images/d94e10f42861.png\n/kaggle/input/aptos2019/train_images/train_images/3f3de2a6b0f5.png\n/kaggle/input/aptos2019/train_images/train_images/c704bd669f36.png\n/kaggle/input/aptos2019/train_images/train_images/76e6a9238570.png\n/kaggle/input/aptos2019/train_images/train_images/58eb3809f456.png\n/kaggle/input/aptos2019/train_images/train_images/84c663f39632.png\n/kaggle/input/aptos2019/train_images/train_images/43ddd0ab0cc4.png\n/kaggle/input/aptos2019/train_images/train_images/2b10f138e67d.png\n/kaggle/input/aptos2019/train_images/train_images/3b9c1f42c2f2.png\n/kaggle/input/aptos2019/train_images/train_images/62b826899151.png\n/kaggle/input/aptos2019/train_images/train_images/710b05a96e0f.png\n/kaggle/input/aptos2019/train_images/train_images/857002ed4e49.png\n/kaggle/input/aptos2019/train_images/train_images/98f48850ebce.png\n/kaggle/input/aptos2019/train_images/train_images/d99b0f7dd9b9.png\n/kaggle/input/aptos2019/train_images/train_images/a88f68b0b114.png\n/kaggle/input/aptos2019/train_images/train_images/262ad704319c.png\n/kaggle/input/aptos2019/train_images/train_images/236f56771ec6.png\n/kaggle/input/aptos2019/train_images/train_images/709784f7fcc2.png\n/kaggle/input/aptos2019/train_images/train_images/211518c46162.png\n/kaggle/input/aptos2019/train_images/train_images/7102f29e052e.png\n/kaggle/input/aptos2019/train_images/train_images/ceb601fe8dba.png\n/kaggle/input/aptos2019/train_images/train_images/7bc4dd99eee5.png\n/kaggle/input/aptos2019/train_images/train_images/aeccef0bdc26.png\n/kaggle/input/aptos2019/train_images/train_images/2f9b66784109.png\n/kaggle/input/aptos2019/train_images/train_images/33e8e26a75d4.png\n/kaggle/input/aptos2019/train_images/train_images/86b3a7929bec.png\n/kaggle/input/aptos2019/train_images/train_images/d16398c971e9.png\n/kaggle/input/aptos2019/train_images/train_images/e10190a9d52f.png\n/kaggle/input/aptos2019/train_images/train_images/996f9bba4ef0.png\n/kaggle/input/aptos2019/train_images/train_images/93a1b984de84.png\n/kaggle/input/aptos2019/train_images/train_images/6c6efb6b1358.png\n/kaggle/input/aptos2019/train_images/train_images/9ac41b9a809e.png\n/kaggle/input/aptos2019/train_images/train_images/cc1eebed9276.png\n/kaggle/input/aptos2019/train_images/train_images/60e269e3e188.png\n/kaggle/input/aptos2019/train_images/train_images/944a233fbf8e.png\n/kaggle/input/aptos2019/train_images/train_images/b8fb9f55cd6d.png\n/kaggle/input/aptos2019/train_images/train_images/49386d603494.png\n/kaggle/input/aptos2019/train_images/train_images/4dd14c380696.png\n/kaggle/input/aptos2019/train_images/train_images/53704c80f0d8.png\n/kaggle/input/aptos2019/train_images/train_images/cb39761f0712.png\n/kaggle/input/aptos2019/train_images/train_images/35362d43e753.png\n/kaggle/input/aptos2019/train_images/train_images/6e861bc3bd7b.png\n/kaggle/input/aptos2019/train_images/train_images/ad6b07d5c338.png\n/kaggle/input/aptos2019/train_images/train_images/4360a112db10.png\n/kaggle/input/aptos2019/train_images/train_images/6e092caa065f.png\n/kaggle/input/aptos2019/train_images/train_images/263d8851e33b.png\n/kaggle/input/aptos2019/train_images/train_images/bdb98063fe84.png\n/kaggle/input/aptos2019/train_images/train_images/c639d837f5e4.png\n/kaggle/input/aptos2019/train_images/train_images/3079490a4b9c.png\n/kaggle/input/aptos2019/train_images/train_images/58059e73d2d4.png\n/kaggle/input/aptos2019/train_images/train_images/6e0f78e188ff.png\n/kaggle/input/aptos2019/train_images/train_images/49c5e7f6b8d2.png\n/kaggle/input/aptos2019/train_images/train_images/64ac539f58cb.png\n/kaggle/input/aptos2019/train_images/train_images/b0619ca93a5f.png\n/kaggle/input/aptos2019/train_images/train_images/23d7ca170bdb.png\n/kaggle/input/aptos2019/train_images/train_images/1d46f1326394.png\n/kaggle/input/aptos2019/train_images/train_images/7d94a000c2d0.png\n/kaggle/input/aptos2019/train_images/train_images/40140a925c43.png\n/kaggle/input/aptos2019/train_images/train_images/91a88d3b0358.png\n/kaggle/input/aptos2019/train_images/train_images/453a1e2754b2.png\n/kaggle/input/aptos2019/train_images/train_images/c8fc0df22999.png\n/kaggle/input/aptos2019/train_images/train_images/77b7b71ebcc3.png\n/kaggle/input/aptos2019/train_images/train_images/da8900ac7f29.png\n/kaggle/input/aptos2019/train_images/train_images/61c2fbd16e38.png\n/kaggle/input/aptos2019/train_images/train_images/48afe8c47454.png\n/kaggle/input/aptos2019/train_images/train_images/30db694bee42.png\n/kaggle/input/aptos2019/train_images/train_images/80e7cc0a0649.png\n/kaggle/input/aptos2019/train_images/train_images/bb85097857fa.png\n/kaggle/input/aptos2019/train_images/train_images/885fa5fc5da8.png\n/kaggle/input/aptos2019/train_images/train_images/32ed318235b8.png\n/kaggle/input/aptos2019/train_images/train_images/aa6242f9e08c.png\n/kaggle/input/aptos2019/train_images/train_images/a7b0d0c51731.png\n/kaggle/input/aptos2019/train_images/train_images/788ddb0b70b7.png\n/kaggle/input/aptos2019/train_images/train_images/41ab357d103f.png\n/kaggle/input/aptos2019/train_images/train_images/91cf56d3d1af.png\n/kaggle/input/aptos2019/train_images/train_images/6f3b62e5b7f5.png\n/kaggle/input/aptos2019/train_images/train_images/1bea04b2bb2d.png\n/kaggle/input/aptos2019/train_images/train_images/e47452069ea1.png\n/kaggle/input/aptos2019/train_images/train_images/a9c7b83caf81.png\n/kaggle/input/aptos2019/train_images/train_images/53273d664cd8.png\n/kaggle/input/aptos2019/train_images/train_images/5e505e25cd3e.png\n/kaggle/input/aptos2019/train_images/train_images/62cc7ddb53b6.png\n/kaggle/input/aptos2019/train_images/train_images/6cd606dc52e9.png\n/kaggle/input/aptos2019/train_images/train_images/b200c23b299b.png\n/kaggle/input/aptos2019/train_images/train_images/1ca35d483772.png\n/kaggle/input/aptos2019/train_images/train_images/a1faeb4d5f10.png\n/kaggle/input/aptos2019/train_images/train_images/58c12863f33d.png\n/kaggle/input/aptos2019/train_images/train_images/65dda202653d.png\n/kaggle/input/aptos2019/train_images/train_images/1f5496352859.png\n/kaggle/input/aptos2019/train_images/train_images/bfa30ebf63a8.png\n/kaggle/input/aptos2019/train_images/train_images/9d74428188bb.png\n/kaggle/input/aptos2019/train_images/train_images/6dc0281f11e3.png\n/kaggle/input/aptos2019/train_images/train_images/63a03880939c.png\n/kaggle/input/aptos2019/train_images/train_images/237aa50edc34.png\n/kaggle/input/aptos2019/train_images/train_images/4a1afe4044f4.png\n/kaggle/input/aptos2019/train_images/train_images/a8c9fcdbc0be.png\n/kaggle/input/aptos2019/train_images/train_images/8433a032b96c.png\n/kaggle/input/aptos2019/train_images/train_images/c4aef0d88d1b.png\n/kaggle/input/aptos2019/train_images/train_images/bf8092e4001d.png\n/kaggle/input/aptos2019/train_images/train_images/6c3745a222da.png\n/kaggle/input/aptos2019/train_images/train_images/3908b3cfd620.png\n/kaggle/input/aptos2019/train_images/train_images/ce6f33a81ad5.png\n/kaggle/input/aptos2019/train_images/train_images/b9b6ee2b9453.png\n/kaggle/input/aptos2019/train_images/train_images/4b6895d0cf8d.png\n/kaggle/input/aptos2019/train_images/train_images/1f63d44d9e3c.png\n/kaggle/input/aptos2019/train_images/train_images/8446826853d0.png\n/kaggle/input/aptos2019/train_images/train_images/523d0c2cb4d6.png\n/kaggle/input/aptos2019/train_images/train_images/9eaf735cf01f.png\n/kaggle/input/aptos2019/train_images/train_images/a73d012c4c38.png\n/kaggle/input/aptos2019/train_images/train_images/c9e697117f3f.png\n/kaggle/input/aptos2019/train_images/train_images/d801c0a66738.png\n/kaggle/input/aptos2019/train_images/train_images/45e4b7eada54.png\n/kaggle/input/aptos2019/train_images/train_images/8b76c3c5cb3e.png\n/kaggle/input/aptos2019/train_images/train_images/207a580de0ea.png\n/kaggle/input/aptos2019/train_images/train_images/7526cf435753.png\n/kaggle/input/aptos2019/train_images/train_images/b5b913358b32.png\n/kaggle/input/aptos2019/train_images/train_images/9a159d4674cd.png\n/kaggle/input/aptos2019/train_images/train_images/dcc6c0ad5cad.png\n/kaggle/input/aptos2019/train_images/train_images/374535e0adb8.png\n/kaggle/input/aptos2019/train_images/train_images/77a1f1398fdb.png\n/kaggle/input/aptos2019/train_images/train_images/a2696f444ecb.png\n/kaggle/input/aptos2019/train_images/train_images/2ef10194e80d.png\n/kaggle/input/aptos2019/train_images/train_images/5077cdb88aed.png\n/kaggle/input/aptos2019/train_images/train_images/4cfd22ae43d4.png\n/kaggle/input/aptos2019/train_images/train_images/c755a0c4edcc.png\n/kaggle/input/aptos2019/train_images/train_images/1df0a4c23c95.png\n/kaggle/input/aptos2019/train_images/train_images/342edf9b889d.png\n/kaggle/input/aptos2019/train_images/train_images/1d55e689cf84.png\n/kaggle/input/aptos2019/train_images/train_images/d8cdb7d7283a.png\n/kaggle/input/aptos2019/train_images/train_images/ba2624883599.png\n/kaggle/input/aptos2019/train_images/train_images/e322acd46152.png\n/kaggle/input/aptos2019/train_images/train_images/9a94e0316ee3.png\n/kaggle/input/aptos2019/train_images/train_images/98e44127872f.png\n/kaggle/input/aptos2019/train_images/train_images/349f3c0ac83e.png\n/kaggle/input/aptos2019/train_images/train_images/8bf2d925dc0c.png\n/kaggle/input/aptos2019/train_images/train_images/657859f893d9.png\n/kaggle/input/aptos2019/train_images/train_images/8dfff47b06b7.png\n/kaggle/input/aptos2019/train_images/train_images/6ea07d19b4ce.png\n/kaggle/input/aptos2019/train_images/train_images/9f8112c710be.png\n/kaggle/input/aptos2019/train_images/train_images/85cc6d636898.png\n/kaggle/input/aptos2019/train_images/train_images/224c14366e11.png\n/kaggle/input/aptos2019/train_images/train_images/bf1b7e21e774.png\n/kaggle/input/aptos2019/train_images/train_images/8cb6b5b2f19c.png\n/kaggle/input/aptos2019/train_images/train_images/e246cd89e1cc.png\n/kaggle/input/aptos2019/train_images/train_images/b4b04d81acbb.png\n/kaggle/input/aptos2019/train_images/train_images/a5bb85afc6e8.png\n/kaggle/input/aptos2019/train_images/train_images/4e54ccfd49b2.png\n/kaggle/input/aptos2019/train_images/train_images/4c3c1ed09771.png\n/kaggle/input/aptos2019/train_images/train_images/3de8ad4151e1.png\n/kaggle/input/aptos2019/train_images/train_images/1fb455685328.png\n/kaggle/input/aptos2019/train_images/train_images/b927a9238434.png\n/kaggle/input/aptos2019/train_images/train_images/370f575adb23.png\n/kaggle/input/aptos2019/train_images/train_images/7ce671f952be.png\n/kaggle/input/aptos2019/train_images/train_images/e160a3b19911.png\n/kaggle/input/aptos2019/train_images/train_images/d85588ff2ebd.png\n/kaggle/input/aptos2019/train_images/train_images/4bcee3cbe232.png\n/kaggle/input/aptos2019/train_images/train_images/4c17e85686f0.png\n/kaggle/input/aptos2019/train_images/train_images/25d069089c5e.png\n/kaggle/input/aptos2019/train_images/train_images/445a8a6da55c.png\n/kaggle/input/aptos2019/train_images/train_images/c8a3eb9a5b52.png\n/kaggle/input/aptos2019/train_images/train_images/da9fe02dead3.png\n/kaggle/input/aptos2019/train_images/train_images/a0d04a19cf40.png\n/kaggle/input/aptos2019/train_images/train_images/883ddb650967.png\n/kaggle/input/aptos2019/train_images/train_images/63c7b0265775.png\n/kaggle/input/aptos2019/train_images/train_images/b86fb2d5be1a.png\n/kaggle/input/aptos2019/train_images/train_images/47e51065b819.png\n/kaggle/input/aptos2019/train_images/train_images/30941b65348b.png\n/kaggle/input/aptos2019/train_images/train_images/b376def52ccc.png\n/kaggle/input/aptos2019/train_images/train_images/aa8a1e814811.png\n/kaggle/input/aptos2019/train_images/train_images/c98f623d08d1.png\n/kaggle/input/aptos2019/train_images/train_images/624fb7317106.png\n/kaggle/input/aptos2019/train_images/train_images/66cd9c28e636.png\n/kaggle/input/aptos2019/train_images/train_images/d6df4fe492ec.png\n/kaggle/input/aptos2019/train_images/train_images/573ea80a53be.png\n/kaggle/input/aptos2019/train_images/train_images/e2a233493b90.png\n/kaggle/input/aptos2019/train_images/train_images/4f6abc40c72d.png\n/kaggle/input/aptos2019/train_images/train_images/6cbc3dad809c.png\n/kaggle/input/aptos2019/train_images/train_images/4ecd1fdd1435.png\n/kaggle/input/aptos2019/train_images/train_images/4e82c3c8d31f.png\n/kaggle/input/aptos2019/train_images/train_images/62b4be2799ca.png\n/kaggle/input/aptos2019/train_images/train_images/757e39293591.png\n/kaggle/input/aptos2019/train_images/train_images/5288f7441f64.png\n/kaggle/input/aptos2019/train_images/train_images/d7a01fca9838.png\n/kaggle/input/aptos2019/train_images/train_images/441117562359.png\n/kaggle/input/aptos2019/train_images/train_images/74898f372d2b.png\n/kaggle/input/aptos2019/train_images/train_images/1ca91751be4d.png\n/kaggle/input/aptos2019/train_images/train_images/6a244e855d0e.png\n/kaggle/input/aptos2019/train_images/train_images/b553e7909535.png\n/kaggle/input/aptos2019/train_images/train_images/504a69096fcb.png\n/kaggle/input/aptos2019/train_images/train_images/aca88f566228.png\n/kaggle/input/aptos2019/train_images/train_images/3d2ecffe0386.png\n/kaggle/input/aptos2019/train_images/train_images/6630f8675a97.png\n/kaggle/input/aptos2019/train_images/train_images/5d4e5fd34d91.png\n/kaggle/input/aptos2019/train_images/train_images/22895c89792f.png\n/kaggle/input/aptos2019/train_images/train_images/3730c322d35b.png\n/kaggle/input/aptos2019/train_images/train_images/58af2c054ced.png\n/kaggle/input/aptos2019/train_images/train_images/655cafb4c932.png\n/kaggle/input/aptos2019/train_images/train_images/86baef833ae0.png\n/kaggle/input/aptos2019/train_images/train_images/d6283ded6aea.png\n/kaggle/input/aptos2019/train_images/train_images/76bc31e0d3be.png\n/kaggle/input/aptos2019/train_images/train_images/9da74370835a.png\n/kaggle/input/aptos2019/train_images/train_images/351e842842a2.png\n/kaggle/input/aptos2019/train_images/train_images/883c6a184f16.png\n/kaggle/input/aptos2019/train_images/train_images/415f2d2bd2a1.png\n/kaggle/input/aptos2019/train_images/train_images/a1b28bcbce00.png\n/kaggle/input/aptos2019/train_images/train_images/92f313287a29.png\n/kaggle/input/aptos2019/train_images/train_images/dee1031a76ae.png\n/kaggle/input/aptos2019/train_images/train_images/57f933d3d7c7.png\n/kaggle/input/aptos2019/train_images/train_images/9f436886e056.png\n/kaggle/input/aptos2019/train_images/train_images/4e0656629d02.png\n/kaggle/input/aptos2019/train_images/train_images/4e85aa647534.png\n/kaggle/input/aptos2019/train_images/train_images/2bd4d4fbed5c.png\n/kaggle/input/aptos2019/train_images/train_images/8d9516ea3587.png\n/kaggle/input/aptos2019/train_images/train_images/bc92a61a1f9c.png\n/kaggle/input/aptos2019/train_images/train_images/d7bc62d60e8c.png\n/kaggle/input/aptos2019/train_images/train_images/568455854a11.png\n/kaggle/input/aptos2019/train_images/train_images/5fcff7280019.png\n/kaggle/input/aptos2019/train_images/train_images/6cb96a6fb029.png\n/kaggle/input/aptos2019/train_images/train_images/a87f53bc984a.png\n/kaggle/input/aptos2019/train_images/train_images/338326891d84.png\n/kaggle/input/aptos2019/train_images/train_images/66bae1ba227f.png\n/kaggle/input/aptos2019/train_images/train_images/df6d13d04da1.png\n/kaggle/input/aptos2019/train_images/train_images/c3c8fdda50c0.png\n/kaggle/input/aptos2019/train_images/train_images/ad570b850a4f.png\n/kaggle/input/aptos2019/train_images/train_images/33b893e18eb3.png\n/kaggle/input/aptos2019/train_images/train_images/3ccf96c1dd6d.png\n/kaggle/input/aptos2019/train_images/train_images/354b8911d6ed.png\n/kaggle/input/aptos2019/train_images/train_images/c7e827fc7f41.png\n/kaggle/input/aptos2019/train_images/train_images/e2a47a74e6e1.png\n/kaggle/input/aptos2019/train_images/train_images/e2ec22b3d07e.png\n/kaggle/input/aptos2019/train_images/train_images/1cb814ed6332.png\n/kaggle/input/aptos2019/train_images/train_images/2bbd1f99ecc3.png\n/kaggle/input/aptos2019/train_images/train_images/874f8c1929f6.png\n/kaggle/input/aptos2019/train_images/train_images/40dd4e6e4444.png\n/kaggle/input/aptos2019/train_images/train_images/87d46b1cc4e9.png\n/kaggle/input/aptos2019/train_images/train_images/692e946b1f85.png\n/kaggle/input/aptos2019/train_images/train_images/760b6f4c6d82.png\n/kaggle/input/aptos2019/train_images/train_images/1c4d87baaffc.png\n/kaggle/input/aptos2019/train_images/train_images/84b472c49cfa.png\n/kaggle/input/aptos2019/train_images/train_images/2cef97083e6f.png\n/kaggle/input/aptos2019/train_images/train_images/71c22da3d6c6.png\n/kaggle/input/aptos2019/train_images/train_images/25002fe43f92.png\n/kaggle/input/aptos2019/train_images/train_images/401fdfd0db07.png\n/kaggle/input/aptos2019/train_images/train_images/7bda86d95c5b.png\n/kaggle/input/aptos2019/train_images/train_images/7455e2b5fc57.png\n/kaggle/input/aptos2019/train_images/train_images/1e8c31e29dd3.png\n/kaggle/input/aptos2019/train_images/train_images/b1c6f0997e27.png\n/kaggle/input/aptos2019/train_images/train_images/200d947f75db.png\n/kaggle/input/aptos2019/train_images/train_images/75c180e04f65.png\n/kaggle/input/aptos2019/train_images/train_images/55034b1dbff2.png\n/kaggle/input/aptos2019/train_images/train_images/959bb2d01091.png\n/kaggle/input/aptos2019/train_images/train_images/3ac92ac3d65a.png\n/kaggle/input/aptos2019/train_images/train_images/d29096bd94aa.png\n/kaggle/input/aptos2019/train_images/train_images/7ec0e61a7e29.png\n/kaggle/input/aptos2019/train_images/train_images/85f99e7e4052.png\n/kaggle/input/aptos2019/train_images/train_images/1db0393cdbc1.png\n/kaggle/input/aptos2019/train_images/train_images/565f3404f9b2.png\n/kaggle/input/aptos2019/train_images/train_images/2b07790a2422.png\n/kaggle/input/aptos2019/train_images/train_images/bcb0498ed2c1.png\n/kaggle/input/aptos2019/train_images/train_images/a3ad6c2db6f1.png\n/kaggle/input/aptos2019/train_images/train_images/328d141ed3aa.png\n/kaggle/input/aptos2019/train_images/train_images/b70cb31b9abb.png\n/kaggle/input/aptos2019/train_images/train_images/1f0e223b8055.png\n/kaggle/input/aptos2019/train_images/train_images/5e7db41b3bee.png\n/kaggle/input/aptos2019/train_images/train_images/2c1d5be654dd.png\n/kaggle/input/aptos2019/train_images/train_images/83517eaeccb9.png\n/kaggle/input/aptos2019/train_images/train_images/d1f1ea894da1.png\n/kaggle/input/aptos2019/train_images/train_images/203275daf46d.png\n/kaggle/input/aptos2019/train_images/train_images/356304d15a5c.png\n/kaggle/input/aptos2019/train_images/train_images/946545473380.png\n/kaggle/input/aptos2019/train_images/train_images/3bf3085ac167.png\n/kaggle/input/aptos2019/train_images/train_images/95a4cc805c7b.png\n/kaggle/input/aptos2019/train_images/train_images/5ea9e447bb62.png\n/kaggle/input/aptos2019/train_images/train_images/9f4132bd6ed6.png\n/kaggle/input/aptos2019/train_images/train_images/e4c799738a19.png\n/kaggle/input/aptos2019/train_images/train_images/239f2c348ea4.png\n/kaggle/input/aptos2019/train_images/train_images/6181aa9f75f4.png\n/kaggle/input/aptos2019/train_images/train_images/55eb405ec71e.png\n/kaggle/input/aptos2019/train_images/train_images/d436c06f0490.png\n/kaggle/input/aptos2019/train_images/train_images/6e018411ba4a.png\n/kaggle/input/aptos2019/train_images/train_images/87a9f4d20f07.png\n/kaggle/input/aptos2019/train_images/train_images/d2901144070c.png\n/kaggle/input/aptos2019/train_images/train_images/b64e1eef3d63.png\n/kaggle/input/aptos2019/train_images/train_images/89ee1fa16f90.png\n/kaggle/input/aptos2019/train_images/train_images/ae58ccb5905e.png\n/kaggle/input/aptos2019/train_images/train_images/8bbd7835e9aa.png\n/kaggle/input/aptos2019/train_images/train_images/7adfb8fc0621.png\n/kaggle/input/aptos2019/train_images/train_images/a30a143a53a3.png\n/kaggle/input/aptos2019/train_images/train_images/75a7bc945b7d.png\n/kaggle/input/aptos2019/train_images/train_images/b468ebf5cb11.png\n/kaggle/input/aptos2019/train_images/train_images/5b644a403e1f.png\n/kaggle/input/aptos2019/train_images/train_images/dc0eea0b68a7.png\n/kaggle/input/aptos2019/train_images/train_images/dbb2c63f6f08.png\n/kaggle/input/aptos2019/train_images/train_images/55092c0071eb.png\n/kaggle/input/aptos2019/train_images/train_images/299086c6d1b5.png\n/kaggle/input/aptos2019/train_images/train_images/97bf61736b86.png\n/kaggle/input/aptos2019/train_images/train_images/5327f88a1919.png\n/kaggle/input/aptos2019/train_images/train_images/b5bf7b84fc66.png\n/kaggle/input/aptos2019/train_images/train_images/3dbc90c7ee7d.png\n/kaggle/input/aptos2019/train_images/train_images/2dd28ac497d2.png\n/kaggle/input/aptos2019/train_images/train_images/ca7570c5925c.png\n/kaggle/input/aptos2019/train_images/train_images/9cedf5c7016b.png\n/kaggle/input/aptos2019/train_images/train_images/94b9ccc73bb9.png\n/kaggle/input/aptos2019/train_images/train_images/5f13e8a07344.png\n/kaggle/input/aptos2019/train_images/train_images/6966abf40b8c.png\n/kaggle/input/aptos2019/train_images/train_images/6af071b0ac6e.png\n/kaggle/input/aptos2019/train_images/train_images/80f6b30ece8c.png\n/kaggle/input/aptos2019/train_images/train_images/cc839823755b.png\n/kaggle/input/aptos2019/train_images/train_images/be3a7d9e981e.png\n/kaggle/input/aptos2019/train_images/train_images/63e041a757eb.png\n/kaggle/input/aptos2019/train_images/train_images/9b418ce42c13.png\n/kaggle/input/aptos2019/train_images/train_images/44271f3cb18f.png\n/kaggle/input/aptos2019/train_images/train_images/a627eb8c08c5.png\n/kaggle/input/aptos2019/train_images/train_images/2d7666b8884f.png\n/kaggle/input/aptos2019/train_images/train_images/ae61e19fb766.png\n/kaggle/input/aptos2019/train_images/train_images/23fca0693e2a.png\n/kaggle/input/aptos2019/train_images/train_images/b498b84d383f.png\n/kaggle/input/aptos2019/train_images/train_images/6110ecb3bb1c.png\n/kaggle/input/aptos2019/train_images/train_images/2f7789c1e046.png\n/kaggle/input/aptos2019/train_images/train_images/aaaadb174012.png\n/kaggle/input/aptos2019/train_images/train_images/4e8585a96739.png\n/kaggle/input/aptos2019/train_images/train_images/2994f17f58a5.png\n/kaggle/input/aptos2019/train_images/train_images/a4b8de38eac1.png\n/kaggle/input/aptos2019/train_images/train_images/a45d77edf8d9.png\n/kaggle/input/aptos2019/train_images/train_images/b402daa0864c.png\n/kaggle/input/aptos2019/train_images/train_images/c62585bd68fb.png\n/kaggle/input/aptos2019/train_images/train_images/7f1f3269f546.png\n/kaggle/input/aptos2019/train_images/train_images/d18b1d8ac4de.png\n/kaggle/input/aptos2019/train_images/train_images/bb783d8e496f.png\n/kaggle/input/aptos2019/train_images/train_images/c6f5b5b5be41.png\n/kaggle/input/aptos2019/train_images/train_images/2967e578939f.png\n/kaggle/input/aptos2019/train_images/train_images/e1e490773462.png\n/kaggle/input/aptos2019/train_images/train_images/42c65af5ab16.png\n/kaggle/input/aptos2019/train_images/train_images/907aaff827e5.png\n/kaggle/input/aptos2019/train_images/train_images/6324d77cf926.png\n/kaggle/input/aptos2019/train_images/train_images/586f5c56081e.png\n/kaggle/input/aptos2019/train_images/train_images/4ccfa0b4e96c.png\n/kaggle/input/aptos2019/train_images/train_images/89ed6a0dd53f.png\n/kaggle/input/aptos2019/train_images/train_images/9df31421cdd2.png\n/kaggle/input/aptos2019/train_images/train_images/5e5275ddee29.png\n/kaggle/input/aptos2019/train_images/train_images/4318b6adeb97.png\n/kaggle/input/aptos2019/train_images/train_images/d990a3f0cbdb.png\n/kaggle/input/aptos2019/train_images/train_images/5b301a6d1ac7.png\n/kaggle/input/aptos2019/train_images/train_images/26d9576e8043.png\n/kaggle/input/aptos2019/train_images/train_images/ae57c8630249.png\n/kaggle/input/aptos2019/train_images/train_images/a76132e79688.png\n/kaggle/input/aptos2019/train_images/train_images/383e72af1955.png\n/kaggle/input/aptos2019/train_images/train_images/762d6e5d5068.png\n/kaggle/input/aptos2019/train_images/train_images/6d259b5b4c76.png\n/kaggle/input/aptos2019/train_images/train_images/d3be5346684b.png\n/kaggle/input/aptos2019/train_images/train_images/2974c6ad1d58.png\n/kaggle/input/aptos2019/train_images/train_images/8c84e96d9b01.png\n/kaggle/input/aptos2019/train_images/train_images/89fc080f7e83.png\n/kaggle/input/aptos2019/train_images/train_images/b94c58d063bf.png\n/kaggle/input/aptos2019/train_images/train_images/4d1e7def7624.png\n/kaggle/input/aptos2019/train_images/train_images/902dc5a91a3f.png\n/kaggle/input/aptos2019/train_images/train_images/91f3c4c1e72b.png\n/kaggle/input/aptos2019/train_images/train_images/d144144a2f3f.png\n/kaggle/input/aptos2019/train_images/train_images/4982378d72f9.png\n/kaggle/input/aptos2019/train_images/train_images/aafb0c944f14.png\n/kaggle/input/aptos2019/train_images/train_images/3f44d749cd0b.png\n/kaggle/input/aptos2019/train_images/train_images/38373431d996.png\n/kaggle/input/aptos2019/train_images/train_images/8010c931321f.png\n/kaggle/input/aptos2019/train_images/train_images/77e15f213b04.png\n/kaggle/input/aptos2019/train_images/train_images/5ad3dabeb2cd.png\n/kaggle/input/aptos2019/train_images/train_images/582115961a3d.png\n/kaggle/input/aptos2019/train_images/train_images/975252e325e3.png\n/kaggle/input/aptos2019/train_images/train_images/e4730ddde408.png\n/kaggle/input/aptos2019/train_images/train_images/54f57cf26126.png\n/kaggle/input/aptos2019/train_images/train_images/3e61703b5ab2.png\n/kaggle/input/aptos2019/train_images/train_images/2af1bf226f51.png\n/kaggle/input/aptos2019/train_images/train_images/83b61051737f.png\n/kaggle/input/aptos2019/train_images/train_images/6810410187a0.png\n/kaggle/input/aptos2019/train_images/train_images/a804cef3e51f.png\n/kaggle/input/aptos2019/train_images/train_images/9d1feed37610.png\n/kaggle/input/aptos2019/train_images/train_images/916ec976ff30.png\n/kaggle/input/aptos2019/train_images/train_images/34acae864963.png\n/kaggle/input/aptos2019/train_images/train_images/63b71347e95d.png\n/kaggle/input/aptos2019/train_images/train_images/a8c54e2a4b79.png\n/kaggle/input/aptos2019/train_images/train_images/c58971bcebb2.png\n/kaggle/input/aptos2019/train_images/train_images/79ce83c07588.png\n/kaggle/input/aptos2019/train_images/train_images/5a36cea278ae.png\n/kaggle/input/aptos2019/train_images/train_images/1c6d119c3d70.png\n/kaggle/input/aptos2019/train_images/train_images/4189d4e631ec.png\n/kaggle/input/aptos2019/train_images/train_images/4c60f6fcea75.png\n/kaggle/input/aptos2019/train_images/train_images/c6229222bf22.png\n/kaggle/input/aptos2019/train_images/train_images/e26d8718ca58.png\n/kaggle/input/aptos2019/train_images/train_images/cd48cfab4e44.png\n/kaggle/input/aptos2019/train_images/train_images/bc7bf19b84e3.png\n/kaggle/input/aptos2019/train_images/train_images/cd563556cb57.png\n/kaggle/input/aptos2019/train_images/train_images/767d777ee889.png\n/kaggle/input/aptos2019/train_images/train_images/60aa4e649abf.png\n/kaggle/input/aptos2019/train_images/train_images/247ac63e5510.png\n/kaggle/input/aptos2019/train_images/train_images/24f3e70f0419.png\n/kaggle/input/aptos2019/train_images/train_images/1e8a1fdee5b9.png\n/kaggle/input/aptos2019/train_images/train_images/6852f4531591.png\n/kaggle/input/aptos2019/train_images/train_images/8a01daa423f7.png\n/kaggle/input/aptos2019/train_images/train_images/435d900fa7b2.png\n/kaggle/input/aptos2019/train_images/train_images/9c72ed6befa0.png\n/kaggle/input/aptos2019/train_images/train_images/855f0a5442b6.png\n/kaggle/input/aptos2019/train_images/train_images/358d2224de73.png\n/kaggle/input/aptos2019/train_images/train_images/a0cd7bffdaa0.png\n/kaggle/input/aptos2019/train_images/train_images/2f7fbdcc9a4b.png\n/kaggle/input/aptos2019/train_images/train_images/7a9f45fdf29b.png\n/kaggle/input/aptos2019/train_images/train_images/aeab0a63bcaf.png\n/kaggle/input/aptos2019/train_images/train_images/74211a2b6dcf.png\n/kaggle/input/aptos2019/train_images/train_images/7a6e384a0846.png\n/kaggle/input/aptos2019/train_images/train_images/5347b4c8e9b3.png\n/kaggle/input/aptos2019/train_images/train_images/878a3a097436.png\n/kaggle/input/aptos2019/train_images/train_images/6bf2a81a5d91.png\n/kaggle/input/aptos2019/train_images/train_images/1dbdc32c17db.png\n/kaggle/input/aptos2019/train_images/train_images/9904939ab83d.png\n/kaggle/input/aptos2019/train_images/train_images/a73c3d516c59.png\n/kaggle/input/aptos2019/train_images/train_images/bfda2fd0533a.png\n/kaggle/input/aptos2019/train_images/train_images/b2748ac28fc1.png\n/kaggle/input/aptos2019/train_images/train_images/d567a1a22d33.png\n/kaggle/input/aptos2019/train_images/train_images/2c9dfc270f1b.png\n/kaggle/input/aptos2019/train_images/train_images/772af553b8b7.png\n/kaggle/input/aptos2019/train_images/train_images/cae51154e1ce.png\n/kaggle/input/aptos2019/train_images/train_images/4d167ca69ea8.png\n/kaggle/input/aptos2019/train_images/train_images/9ad92f1c1542.png\n/kaggle/input/aptos2019/train_images/train_images/c80f79579fed.png\n/kaggle/input/aptos2019/train_images/train_images/aabd867043cf.png\n/kaggle/input/aptos2019/train_images/train_images/bba38f2294a3.png\n/kaggle/input/aptos2019/train_images/train_images/51a1d162e223.png\n/kaggle/input/aptos2019/train_images/train_images/ace2281f00c4.png\n/kaggle/input/aptos2019/train_images/train_images/248139c423c4.png\n/kaggle/input/aptos2019/train_images/train_images/c568e5245ea5.png\n/kaggle/input/aptos2019/train_images/train_images/96ce10a1dbd7.png\n/kaggle/input/aptos2019/train_images/train_images/76cab26493f1.png\n/kaggle/input/aptos2019/train_images/train_images/b0cc9f8d06e4.png\n/kaggle/input/aptos2019/train_images/train_images/595446774178.png\n/kaggle/input/aptos2019/train_images/train_images/caec68f11c86.png\n/kaggle/input/aptos2019/train_images/train_images/cfd1bd0fcbb4.png\n/kaggle/input/aptos2019/train_images/train_images/9b70f84400af.png\n/kaggle/input/aptos2019/train_images/train_images/345b1f0abbba.png\n/kaggle/input/aptos2019/train_images/train_images/2a3a1ed1c285.png\n/kaggle/input/aptos2019/train_images/train_images/aba3063c5413.png\n/kaggle/input/aptos2019/train_images/train_images/5db2e3a4594a.png\n/kaggle/input/aptos2019/train_images/train_images/a04fb36db784.png\n/kaggle/input/aptos2019/train_images/train_images/ce8d2efd9d4f.png\n/kaggle/input/aptos2019/train_images/train_images/a2dff8dbc9f8.png\n/kaggle/input/aptos2019/train_images/train_images/61c667663f2f.png\n/kaggle/input/aptos2019/train_images/train_images/cae33655ca00.png\n/kaggle/input/aptos2019/train_images/train_images/adb56cecafaf.png\n/kaggle/input/aptos2019/train_images/train_images/83d81ba5959c.png\n/kaggle/input/aptos2019/train_images/train_images/a8652b2de23f.png\n/kaggle/input/aptos2019/train_images/train_images/e12df54e0d1e.png\n/kaggle/input/aptos2019/train_images/train_images/d807c53c1399.png\n/kaggle/input/aptos2019/train_images/train_images/d10d315f123f.png\n/kaggle/input/aptos2019/train_images/train_images/d29b37d110f3.png\n/kaggle/input/aptos2019/train_images/train_images/4350a1b2f3cb.png\n/kaggle/input/aptos2019/train_images/train_images/c8823cdaf7fa.png\n/kaggle/input/aptos2019/train_images/train_images/3d663a6a50a3.png\n/kaggle/input/aptos2019/train_images/train_images/8ffa608170d3.png\n/kaggle/input/aptos2019/train_images/train_images/7e70344b0c25.png\n/kaggle/input/aptos2019/train_images/train_images/1db6bb46c102.png\n/kaggle/input/aptos2019/train_images/train_images/1e9224ccca95.png\n/kaggle/input/aptos2019/train_images/train_images/6e73acb2cf60.png\n/kaggle/input/aptos2019/train_images/train_images/2a2a6435f7f3.png\n/kaggle/input/aptos2019/train_images/train_images/7d1da90d3ca9.png\n/kaggle/input/aptos2019/train_images/train_images/80feb1f7ca5e.png\n/kaggle/input/aptos2019/train_images/train_images/a64273801bde.png\n/kaggle/input/aptos2019/train_images/train_images/2735be026d44.png\n/kaggle/input/aptos2019/train_images/train_images/3f49f8d100e9.png\n/kaggle/input/aptos2019/train_images/train_images/b574d229ec4c.png\n/kaggle/input/aptos2019/train_images/train_images/5ed6dc419e4d.png\n/kaggle/input/aptos2019/train_images/train_images/7a42443ed106.png\n/kaggle/input/aptos2019/train_images/train_images/aeb6f4fd2eed.png\n/kaggle/input/aptos2019/train_images/train_images/9c5dd3612f0c.png\n/kaggle/input/aptos2019/train_images/train_images/ababe19ed448.png\n/kaggle/input/aptos2019/train_images/train_images/d57d1be1bbd1.png\n/kaggle/input/aptos2019/train_images/train_images/4f60129e9a5b.png\n/kaggle/input/aptos2019/train_images/train_images/c90c6b94cf40.png\n/kaggle/input/aptos2019/train_images/train_images/d2fb715b0c41.png\n/kaggle/input/aptos2019/train_images/train_images/3ca637fddd56.png\n/kaggle/input/aptos2019/train_images/train_images/df0886f1e76b.png\n/kaggle/input/aptos2019/train_images/train_images/d1a24527a15d.png\n/kaggle/input/aptos2019/train_images/train_images/d9d2631f043c.png\n/kaggle/input/aptos2019/train_images/train_images/1c5e6cdc7ee1.png\n/kaggle/input/aptos2019/train_images/train_images/54038e56131d.png\n/kaggle/input/aptos2019/train_images/train_images/b0f8613305a3.png\n/kaggle/input/aptos2019/train_images/train_images/5905a9b06a73.png\n/kaggle/input/aptos2019/train_images/train_images/3732de8b416f.png\n/kaggle/input/aptos2019/train_images/train_images/c9c563864ab1.png\n/kaggle/input/aptos2019/train_images/train_images/8e67f2d7e0ee.png\n/kaggle/input/aptos2019/train_images/train_images/a9b3177f01c0.png\n/kaggle/input/aptos2019/train_images/train_images/e07045d7c5f7.png\n/kaggle/input/aptos2019/train_images/train_images/6531070bf03c.png\n/kaggle/input/aptos2019/train_images/train_images/283c3aeba594.png\n/kaggle/input/aptos2019/train_images/train_images/9c52b87d01f1.png\n/kaggle/input/aptos2019/train_images/train_images/c31651ea04c6.png\n/kaggle/input/aptos2019/train_images/train_images/38b9bb961847.png\n/kaggle/input/aptos2019/train_images/train_images/a654b25124c3.png\n/kaggle/input/aptos2019/train_images/train_images/1f4bf8e28b41.png\n/kaggle/input/aptos2019/train_images/train_images/ae20112e7a1e.png\n/kaggle/input/aptos2019/train_images/train_images/78937523f7a8.png\n/kaggle/input/aptos2019/train_images/train_images/5e18af29d812.png\n/kaggle/input/aptos2019/train_images/train_images/be21d8b60e2a.png\n/kaggle/input/aptos2019/train_images/train_images/ace287a5c991.png\n/kaggle/input/aptos2019/train_images/train_images/c57c164bca05.png\n/kaggle/input/aptos2019/train_images/train_images/2cdcc910778d.png\n/kaggle/input/aptos2019/train_images/train_images/d6228d951958.png\n/kaggle/input/aptos2019/train_images/train_images/41960d5f58c2.png\n/kaggle/input/aptos2019/train_images/train_images/b598bc9753c2.png\n/kaggle/input/aptos2019/train_images/train_images/a7c10ca6c117.png\n/kaggle/input/aptos2019/train_images/train_images/51af8c112682.png\n/kaggle/input/aptos2019/train_images/train_images/3b185ac445d0.png\n/kaggle/input/aptos2019/train_images/train_images/8a234d68b27e.png\n/kaggle/input/aptos2019/train_images/train_images/cf6551521a35.png\n/kaggle/input/aptos2019/train_images/train_images/cfdbaef73a8b.png\n/kaggle/input/aptos2019/train_images/train_images/5e52c9fe676f.png\n/kaggle/input/aptos2019/train_images/train_images/331b87d1b9d1.png\n/kaggle/input/aptos2019/train_images/train_images/4ccee4db09b6.png\n/kaggle/input/aptos2019/train_images/train_images/ac720570dd0f.png\n/kaggle/input/aptos2019/train_images/train_images/28503940d10b.png\n/kaggle/input/aptos2019/train_images/train_images/254052cf3e48.png\n/kaggle/input/aptos2019/train_images/train_images/698d6e422a80.png\n/kaggle/input/aptos2019/train_images/train_images/be7bc89f5fec.png\n/kaggle/input/aptos2019/train_images/train_images/8eeac97f02f0.png\n/kaggle/input/aptos2019/train_images/train_images/6cffc6c6851a.png\n/kaggle/input/aptos2019/train_images/train_images/6f0e5848d9ce.png\n/kaggle/input/aptos2019/train_images/train_images/540e4973829e.png\n/kaggle/input/aptos2019/train_images/train_images/8a482c024fc2.png\n/kaggle/input/aptos2019/train_images/train_images/d9a475dfe59a.png\n/kaggle/input/aptos2019/train_images/train_images/596f4fdb0004.png\n/kaggle/input/aptos2019/train_images/train_images/4393c5bc576a.png\n/kaggle/input/aptos2019/train_images/train_images/d9bbdc33db83.png\n/kaggle/input/aptos2019/train_images/train_images/1b398c0494d1.png\n/kaggle/input/aptos2019/train_images/train_images/6dc07f968794.png\n/kaggle/input/aptos2019/train_images/train_images/5b1c4cefeb24.png\n/kaggle/input/aptos2019/train_images/train_images/3cd801ffdbf0.png\n/kaggle/input/aptos2019/train_images/train_images/6cb98da77e3e.png\n/kaggle/input/aptos2019/train_images/train_images/2a4520f1f9a3.png\n/kaggle/input/aptos2019/train_images/train_images/7b8c78b41c0d.png\n/kaggle/input/aptos2019/train_images/train_images/b576c5269ad1.png\n/kaggle/input/aptos2019/train_images/train_images/a0b7ad98df57.png\n/kaggle/input/aptos2019/train_images/train_images/27f82ada84ac.png\n/kaggle/input/aptos2019/train_images/train_images/2d3f4094c08a.png\n/kaggle/input/aptos2019/train_images/train_images/e019b3e0f33d.png\n/kaggle/input/aptos2019/train_images/train_images/896ad584a841.png\n/kaggle/input/aptos2019/train_images/train_images/a28bfb772f50.png\n/kaggle/input/aptos2019/train_images/train_images/c3789c1dab96.png\n/kaggle/input/aptos2019/train_images/train_images/a3bd2e034614.png\n/kaggle/input/aptos2019/train_images/train_images/d81b6ed83bc2.png\n/kaggle/input/aptos2019/train_images/train_images/799cb4c816ae.png\n/kaggle/input/aptos2019/train_images/train_images/9de9421f17e3.png\n/kaggle/input/aptos2019/train_images/train_images/4abca30b676b.png\n/kaggle/input/aptos2019/train_images/train_images/721214151233.png\n/kaggle/input/aptos2019/train_images/train_images/6fb656d506b2.png\n/kaggle/input/aptos2019/train_images/train_images/be161517d3ac.png\n/kaggle/input/aptos2019/train_images/train_images/bfb578c0e8d8.png\n/kaggle/input/aptos2019/train_images/train_images/4d21ce39c905.png\n/kaggle/input/aptos2019/train_images/train_images/388279491b5d.png\n/kaggle/input/aptos2019/train_images/train_images/1c7a013eeba7.png\n/kaggle/input/aptos2019/train_images/train_images/c8d2d32f7f29.png\n/kaggle/input/aptos2019/train_images/train_images/d85ea1220a03.png\n/kaggle/input/aptos2019/train_images/train_images/ac2c814949f9.png\n/kaggle/input/aptos2019/train_images/train_images/61bbe8db6f3a.png\n/kaggle/input/aptos2019/train_images/train_images/b8ac328009e0.png\n/kaggle/input/aptos2019/train_images/train_images/ae8424cdb029.png\n/kaggle/input/aptos2019/train_images/train_images/37c523296d42.png\n/kaggle/input/aptos2019/train_images/train_images/8e6df9eedcd8.png\n/kaggle/input/aptos2019/train_images/train_images/4818672273af.png\n/kaggle/input/aptos2019/train_images/train_images/4bd941611343.png\n/kaggle/input/aptos2019/train_images/train_images/af87d48ffe01.png\n/kaggle/input/aptos2019/train_images/train_images/22a6da005395.png\n/kaggle/input/aptos2019/train_images/train_images/6f4e0538d1e4.png\n/kaggle/input/aptos2019/train_images/train_images/96ea316ed0ab.png\n/kaggle/input/aptos2019/train_images/train_images/da0a83f074f3.png\n/kaggle/input/aptos2019/train_images/train_images/e2161692a0b4.png\n/kaggle/input/aptos2019/train_images/train_images/999ad827ed35.png\n/kaggle/input/aptos2019/train_images/train_images/4ef16a53d899.png\n/kaggle/input/aptos2019/train_images/train_images/1d74c4713e21.png\n/kaggle/input/aptos2019/train_images/train_images/c24bcf7a1bc4.png\n/kaggle/input/aptos2019/train_images/train_images/46cdc8b685bd.png\n/kaggle/input/aptos2019/train_images/train_images/82f2784ead76.png\n/kaggle/input/aptos2019/train_images/train_images/c94f37085d0f.png\n/kaggle/input/aptos2019/train_images/train_images/9519a590985d.png\n/kaggle/input/aptos2019/train_images/train_images/521d3e264d71.png\n/kaggle/input/aptos2019/train_images/train_images/dd02d60bef14.png\n/kaggle/input/aptos2019/train_images/train_images/bcc762618e7d.png\n/kaggle/input/aptos2019/train_images/train_images/4c5ab774a381.png\n/kaggle/input/aptos2019/train_images/train_images/4ed31cc07366.png\n/kaggle/input/aptos2019/train_images/train_images/a21b37719f9b.png\n/kaggle/input/aptos2019/train_images/train_images/75369248dba0.png\n/kaggle/input/aptos2019/train_images/train_images/3abac0961bfd.png\n/kaggle/input/aptos2019/train_images/train_images/1fd5d860d4d7.png\n/kaggle/input/aptos2019/train_images/train_images/b960142a8de7.png\n/kaggle/input/aptos2019/train_images/train_images/2fdffb6160a6.png\n/kaggle/input/aptos2019/train_images/train_images/b82d5f1f1145.png\n/kaggle/input/aptos2019/train_images/train_images/7427dedafccf.png\n/kaggle/input/aptos2019/train_images/train_images/9b32e8ef0ca0.png\n/kaggle/input/aptos2019/train_images/train_images/69c4cbb630de.png\n/kaggle/input/aptos2019/train_images/train_images/3ce2f8a77a32.png\n/kaggle/input/aptos2019/train_images/train_images/d2c6b99ef62c.png\n/kaggle/input/aptos2019/train_images/train_images/d7bc00091cfc.png\n/kaggle/input/aptos2019/train_images/train_images/e12d41e7b221.png\n/kaggle/input/aptos2019/train_images/train_images/e3cd96cb094c.png\n/kaggle/input/aptos2019/train_images/train_images/389552047476.png\n/kaggle/input/aptos2019/train_images/train_images/5a091e8cd95c.png\n/kaggle/input/aptos2019/train_images/train_images/27e4c800a449.png\n/kaggle/input/aptos2019/train_images/train_images/cd45bfa07d41.png\n/kaggle/input/aptos2019/train_images/train_images/878e356c8fc9.png\n/kaggle/input/aptos2019/train_images/train_images/5b47043942f4.png\n/kaggle/input/aptos2019/train_images/train_images/895fe2bfc5b6.png\n/kaggle/input/aptos2019/train_images/train_images/663a923d5398.png\n/kaggle/input/aptos2019/train_images/train_images/875a2fc5fe23.png\n/kaggle/input/aptos2019/train_images/train_images/3fd45879afe6.png\n/kaggle/input/aptos2019/train_images/train_images/3810040096cb.png\n/kaggle/input/aptos2019/train_images/train_images/d035c2bd9104.png\n/kaggle/input/aptos2019/train_images/train_images/b0eeae01b8ab.png\n/kaggle/input/aptos2019/train_images/train_images/d7ab5c040294.png\n/kaggle/input/aptos2019/train_images/train_images/d868acdccb5b.png\n/kaggle/input/aptos2019/train_images/train_images/969f92a390db.png\n/kaggle/input/aptos2019/train_images/train_images/8f318a978844.png\n/kaggle/input/aptos2019/train_images/train_images/6377e23928f6.png\n/kaggle/input/aptos2019/train_images/train_images/8acffaf1f4b9.png\n/kaggle/input/aptos2019/train_images/train_images/d88806d9ece9.png\n/kaggle/input/aptos2019/train_images/train_images/362c4a96cebb.png\n/kaggle/input/aptos2019/train_images/train_images/aa9b8f05f4bf.png\n/kaggle/input/aptos2019/train_images/train_images/8ead8f37423c.png\n/kaggle/input/aptos2019/train_images/train_images/98104c8c67eb.png\n/kaggle/input/aptos2019/train_images/train_images/467b7d9d811c.png\n/kaggle/input/aptos2019/train_images/train_images/c446985355f1.png\n/kaggle/input/aptos2019/train_images/train_images/7af4d8704032.png\n/kaggle/input/aptos2019/train_images/train_images/d27ac9e54901.png\n/kaggle/input/aptos2019/train_images/train_images/a3957df90a78.png\n/kaggle/input/aptos2019/train_images/train_images/62ab144d5cee.png\n/kaggle/input/aptos2019/train_images/train_images/a8854768549f.png\n/kaggle/input/aptos2019/train_images/train_images/1b3647865779.png\n/kaggle/input/aptos2019/train_images/train_images/269b44e628eb.png\n/kaggle/input/aptos2019/train_images/train_images/7ae69d22075a.png\n/kaggle/input/aptos2019/train_images/train_images/bac1744955c2.png\n/kaggle/input/aptos2019/train_images/train_images/a32b5ce3d48a.png\n/kaggle/input/aptos2019/train_images/train_images/a2163f0c2af5.png\n/kaggle/input/aptos2019/train_images/train_images/e067b06fd655.png\n/kaggle/input/aptos2019/train_images/train_images/41345cec5957.png\n/kaggle/input/aptos2019/train_images/train_images/8f14bca04b47.png\n/kaggle/input/aptos2019/train_images/train_images/3b58b02c89ed.png\n/kaggle/input/aptos2019/train_images/train_images/26e231747848.png\n/kaggle/input/aptos2019/train_images/train_images/b21abe5d9722.png\n/kaggle/input/aptos2019/train_images/train_images/71a39c660432.png\n/kaggle/input/aptos2019/train_images/train_images/8dafa62f9322.png\n/kaggle/input/aptos2019/train_images/train_images/8596a24a14bd.png\n/kaggle/input/aptos2019/train_images/train_images/9e5ec293267c.png\n/kaggle/input/aptos2019/train_images/train_images/54334705a34d.png\n/kaggle/input/aptos2019/train_images/train_images/991a0b7a8c87.png\n/kaggle/input/aptos2019/train_images/train_images/8ff2733f6aef.png\n/kaggle/input/aptos2019/train_images/train_images/8185ce1cdcef.png\n/kaggle/input/aptos2019/train_images/train_images/b4e15102cd7a.png\n/kaggle/input/aptos2019/train_images/train_images/7b691d9ced34.png\n/kaggle/input/aptos2019/train_images/train_images/3c726de3ee90.png\n/kaggle/input/aptos2019/train_images/train_images/3323fd59782e.png\n/kaggle/input/aptos2019/train_images/train_images/da6389d129aa.png\n/kaggle/input/aptos2019/train_images/train_images/badb5ff8d3c7.png\n/kaggle/input/aptos2019/train_images/train_images/cd54d022e37d.png\n/kaggle/input/aptos2019/train_images/train_images/a5c9a8c726b2.png\n/kaggle/input/aptos2019/train_images/train_images/582f739b8f62.png\n/kaggle/input/aptos2019/train_images/train_images/2d9d97a6e713.png\n/kaggle/input/aptos2019/train_images/train_images/a15590a7d774.png\n/kaggle/input/aptos2019/train_images/train_images/d1b279cc02ae.png\n/kaggle/input/aptos2019/train_images/train_images/33b91def2035.png\n/kaggle/input/aptos2019/train_images/train_images/61da799bf0aa.png\n/kaggle/input/aptos2019/train_images/train_images/731b19a460ad.png\n/kaggle/input/aptos2019/train_images/train_images/e1dc02a3dc2a.png\n/kaggle/input/aptos2019/train_images/train_images/a8582e346df0.png\n/kaggle/input/aptos2019/train_images/train_images/37c4dfe03aba.png\n/kaggle/input/aptos2019/train_images/train_images/9cf7c1349673.png\n/kaggle/input/aptos2019/train_images/train_images/29b52f64d2db.png\n/kaggle/input/aptos2019/train_images/train_images/789f0ec1eab8.png\n/kaggle/input/aptos2019/train_images/train_images/aed4e743c230.png\n/kaggle/input/aptos2019/train_images/train_images/31452ad8808c.png\n/kaggle/input/aptos2019/train_images/train_images/b842b43cb7fb.png\n/kaggle/input/aptos2019/train_images/train_images/28dc010a0780.png\n/kaggle/input/aptos2019/train_images/train_images/c58e5c0c5b33.png\n/kaggle/input/aptos2019/train_images/train_images/a4d41c495666.png\n/kaggle/input/aptos2019/train_images/train_images/22d843b2bbd1.png\n/kaggle/input/aptos2019/train_images/train_images/c7d0deb71576.png\n/kaggle/input/aptos2019/train_images/train_images/44ecf3f4efa5.png\n/kaggle/input/aptos2019/train_images/train_images/28824d12d31d.png\n/kaggle/input/aptos2019/train_images/train_images/4731e708ede3.png\n/kaggle/input/aptos2019/train_images/train_images/20688cb25704.png\n/kaggle/input/aptos2019/train_images/train_images/e16af45285e5.png\n/kaggle/input/aptos2019/train_images/train_images/7f43becd3e83.png\n/kaggle/input/aptos2019/train_images/train_images/d364423ec6f9.png\n/kaggle/input/aptos2019/train_images/train_images/bf87aedf2489.png\n/kaggle/input/aptos2019/train_images/train_images/64fedbf97473.png\n/kaggle/input/aptos2019/train_images/train_images/29192375ab1b.png\n/kaggle/input/aptos2019/train_images/train_images/e4b0df29b96f.png\n/kaggle/input/aptos2019/train_images/train_images/8acdf12f412a.png\n/kaggle/input/aptos2019/train_images/train_images/9cc6b1f9bcbd.png\n/kaggle/input/aptos2019/train_images/train_images/65cf00be6fb4.png\n/kaggle/input/aptos2019/train_images/train_images/51da6aebba8f.png\n/kaggle/input/aptos2019/train_images/train_images/ad20080452de.png\n/kaggle/input/aptos2019/train_images/train_images/ae2c3f6312ef.png\n/kaggle/input/aptos2019/train_images/train_images/2209daf71aab.png\n/kaggle/input/aptos2019/train_images/train_images/55f7f018c61c.png\n/kaggle/input/aptos2019/train_images/train_images/20404ec7b518.png\n/kaggle/input/aptos2019/train_images/train_images/2376e5415458.png\n/kaggle/input/aptos2019/train_images/train_images/9910c827e2fe.png\n/kaggle/input/aptos2019/train_images/train_images/30263a7d5609.png\n/kaggle/input/aptos2019/train_images/train_images/8201cab8322d.png\n/kaggle/input/aptos2019/train_images/train_images/242fc19be06f.png\n/kaggle/input/aptos2019/train_images/train_images/8c0d05233238.png\n/kaggle/input/aptos2019/train_images/train_images/3748349334f6.png\n/kaggle/input/aptos2019/train_images/train_images/e1fb532f55df.png\n/kaggle/input/aptos2019/train_images/train_images/2532613a584a.png\n/kaggle/input/aptos2019/train_images/train_images/756b0d6488bb.png\n/kaggle/input/aptos2019/train_images/train_images/d9ba044671e1.png\n/kaggle/input/aptos2019/train_images/train_images/6ccfdb031184.png\n/kaggle/input/aptos2019/train_images/train_images/1e4b3b823b95.png\n/kaggle/input/aptos2019/train_images/train_images/8906c9ed54a2.png\n/kaggle/input/aptos2019/train_images/train_images/52886bed8a07.png\n/kaggle/input/aptos2019/train_images/train_images/df841a0440d8.png\n/kaggle/input/aptos2019/train_images/train_images/1d37f1c8b6d8.png\n/kaggle/input/aptos2019/train_images/train_images/5c85d22bd0de.png\n/kaggle/input/aptos2019/train_images/train_images/79ade634c633.png\n/kaggle/input/aptos2019/train_images/train_images/894a37fc3738.png\n/kaggle/input/aptos2019/train_images/train_images/50d8249f7bc9.png\n/kaggle/input/aptos2019/train_images/train_images/1efa5d443707.png\n/kaggle/input/aptos2019/train_images/train_images/c9f0dc2c8b43.png\n/kaggle/input/aptos2019/train_images/train_images/78bcdffb8785.png\n/kaggle/input/aptos2019/train_images/train_images/a1eb88562239.png\n/kaggle/input/aptos2019/train_images/train_images/e1418d28d668.png\n/kaggle/input/aptos2019/train_images/train_images/8846b09384a4.png\n/kaggle/input/aptos2019/train_images/train_images/2ef4a04aed1b.png\n/kaggle/input/aptos2019/train_images/train_images/9be0683649ff.png\n/kaggle/input/aptos2019/train_images/train_images/72606afaf3da.png\n/kaggle/input/aptos2019/train_images/train_images/4134b290f5f3.png\n/kaggle/input/aptos2019/train_images/train_images/c73c5f6ef664.png\n/kaggle/input/aptos2019/train_images/train_images/8e4a354e3da2.png\n/kaggle/input/aptos2019/train_images/train_images/750e0168399d.png\n/kaggle/input/aptos2019/train_images/train_images/4d47300e3ddb.png\n/kaggle/input/aptos2019/train_images/train_images/44878f34e31f.png\n/kaggle/input/aptos2019/train_images/train_images/b37aae3c8fe1.png\n/kaggle/input/aptos2019/train_images/train_images/a81b06f50612.png\n/kaggle/input/aptos2019/train_images/train_images/36a1e3c780a0.png\n/kaggle/input/aptos2019/train_images/train_images/27933cdbe0cc.png\n/kaggle/input/aptos2019/train_images/train_images/bffca6eeb2bf.png\n/kaggle/input/aptos2019/train_images/train_images/b09101adb478.png\n/kaggle/input/aptos2019/train_images/train_images/9122b31414d3.png\n/kaggle/input/aptos2019/train_images/train_images/dd285d9e97fe.png\n/kaggle/input/aptos2019/train_images/train_images/92fcf50b3562.png\n/kaggle/input/aptos2019/train_images/train_images/9f935fb38440.png\n/kaggle/input/aptos2019/train_images/train_images/97d02a9b94f7.png\n/kaggle/input/aptos2019/train_images/train_images/e135d7ba9a0e.png\n/kaggle/input/aptos2019/train_images/train_images/c3e02d4a1798.png\n/kaggle/input/aptos2019/train_images/train_images/735836b1ffa6.png\n/kaggle/input/aptos2019/train_images/train_images/52ae917fcea4.png\n/kaggle/input/aptos2019/train_images/train_images/86722fcd802c.png\n/kaggle/input/aptos2019/train_images/train_images/3796af4d987a.png\n/kaggle/input/aptos2019/train_images/train_images/1f543a86c4d4.png\n/kaggle/input/aptos2019/train_images/train_images/db3cd58aa315.png\n/kaggle/input/aptos2019/train_images/train_images/20f86e068276.png\n/kaggle/input/aptos2019/train_images/train_images/282bc792d23a.png\n/kaggle/input/aptos2019/train_images/train_images/dbfd238b3468.png\n/kaggle/input/aptos2019/train_images/train_images/3cdda8b3df19.png\n/kaggle/input/aptos2019/train_images/train_images/52ddde91a349.png\n/kaggle/input/aptos2019/train_images/train_images/de778495a1cd.png\n/kaggle/input/aptos2019/train_images/train_images/25b0e72705a8.png\n/kaggle/input/aptos2019/train_images/train_images/a673193cd5a9.png\n/kaggle/input/aptos2019/train_images/train_images/6b00cb764237.png\n/kaggle/input/aptos2019/train_images/train_images/369229040a34.png\n/kaggle/input/aptos2019/train_images/train_images/77a9538b8362.png\n/kaggle/input/aptos2019/train_images/train_images/2fe06bedb2c4.png\n/kaggle/input/aptos2019/train_images/train_images/2b5bb6d33959.png\n/kaggle/input/aptos2019/train_images/train_images/2b88cb6e31cd.png\n/kaggle/input/aptos2019/train_images/train_images/76516f828d88.png\n/kaggle/input/aptos2019/train_images/train_images/27e2be850a99.png\n/kaggle/input/aptos2019/train_images/train_images/272d9c043c81.png\n/kaggle/input/aptos2019/train_images/train_images/82deb07a6618.png\n/kaggle/input/aptos2019/train_images/train_images/1c0e5dd1b14c.png\n/kaggle/input/aptos2019/train_images/train_images/a08a0133754a.png\n/kaggle/input/aptos2019/train_images/train_images/5879285f9d8d.png\n/kaggle/input/aptos2019/train_images/train_images/bec0acd539b2.png\n/kaggle/input/aptos2019/train_images/train_images/59e5212f7139.png\n/kaggle/input/aptos2019/train_images/train_images/6c3589d7ed8d.png\n/kaggle/input/aptos2019/train_images/train_images/830e297965a1.png\n/kaggle/input/aptos2019/train_images/train_images/b56340f472d2.png\n/kaggle/input/aptos2019/train_images/train_images/7d37a2939f12.png\n/kaggle/input/aptos2019/train_images/train_images/876e1dd12d38.png\n/kaggle/input/aptos2019/train_images/train_images/ccd34029493d.png\n/kaggle/input/aptos2019/train_images/train_images/48c49f662f7d.png\n/kaggle/input/aptos2019/train_images/train_images/2df07eb5779f.png\n/kaggle/input/aptos2019/train_images/train_images/98d41bce73a8.png\n/kaggle/input/aptos2019/train_images/train_images/cd1c98ec48b1.png\n/kaggle/input/aptos2019/train_images/train_images/81371b0c01ad.png\n/kaggle/input/aptos2019/train_images/train_images/7828dd083cdc.png\n/kaggle/input/aptos2019/train_images/train_images/983c98354e9c.png\n/kaggle/input/aptos2019/train_images/train_images/c0e15e8e2b46.png\n/kaggle/input/aptos2019/train_images/train_images/c6916bc42016.png\n/kaggle/input/aptos2019/train_images/train_images/3f82631e9080.png\n/kaggle/input/aptos2019/train_images/train_images/cf8ae5501bd6.png\n/kaggle/input/aptos2019/train_images/train_images/77baa08a1345.png\n/kaggle/input/aptos2019/train_images/train_images/6987804eb464.png\n/kaggle/input/aptos2019/train_images/train_images/9bafbbd152d2.png\n/kaggle/input/aptos2019/train_images/train_images/cc671a73e1cb.png\n/kaggle/input/aptos2019/train_images/train_images/a85cda5f725d.png\n/kaggle/input/aptos2019/train_images/train_images/726dff37edc0.png\n/kaggle/input/aptos2019/train_images/train_images/dad71ba27a9b.png\n/kaggle/input/aptos2019/train_images/train_images/4d3de40ced3a.png\n/kaggle/input/aptos2019/train_images/train_images/c0202976c670.png\n/kaggle/input/aptos2019/train_images/train_images/beb2ad14fd2d.png\n/kaggle/input/aptos2019/train_images/train_images/882a71de424e.png\n/kaggle/input/aptos2019/train_images/train_images/cbc2e57447c2.png\n/kaggle/input/aptos2019/train_images/train_images/93d6d20a5ee3.png\n/kaggle/input/aptos2019/train_images/train_images/408ea9d5e082.png\n/kaggle/input/aptos2019/train_images/train_images/531b39880c32.png\n/kaggle/input/aptos2019/train_images/train_images/b7bd4a6627b6.png\n/kaggle/input/aptos2019/train_images/train_images/e33766353db2.png\n/kaggle/input/aptos2019/train_images/train_images/abf09c44d5f4.png\n/kaggle/input/aptos2019/train_images/train_images/5bf6f2958e53.png\n/kaggle/input/aptos2019/train_images/train_images/2923971566fe.png\n/kaggle/input/aptos2019/train_images/train_images/4e7694eebb91.png\n/kaggle/input/aptos2019/train_images/train_images/d7ac4a0c9760.png\n/kaggle/input/aptos2019/train_images/train_images/ca63fe4f4b52.png\n/kaggle/input/aptos2019/train_images/train_images/9274e75dc4d5.png\n/kaggle/input/aptos2019/train_images/train_images/1c0cf251b426.png\n/kaggle/input/aptos2019/train_images/train_images/b72f59b85f7c.png\n/kaggle/input/aptos2019/train_images/train_images/dcf109df1a2b.png\n/kaggle/input/aptos2019/train_images/train_images/ca6842bfcbc9.png\n/kaggle/input/aptos2019/train_images/train_images/6253f23229b1.png\n/kaggle/input/aptos2019/train_images/train_images/96c699221180.png\n/kaggle/input/aptos2019/train_images/train_images/ca7f5caddf96.png\n/kaggle/input/aptos2019/train_images/train_images/c6d4e4a3bd4c.png\n/kaggle/input/aptos2019/train_images/train_images/26463a5fb949.png\n/kaggle/input/aptos2019/train_images/train_images/5b2648ad455e.png\n/kaggle/input/aptos2019/train_images/train_images/5814cbd2e9bf.png\n/kaggle/input/aptos2019/train_images/train_images/89d9c071a56f.png\n/kaggle/input/aptos2019/train_images/train_images/c0a0828e01b4.png\n/kaggle/input/aptos2019/train_images/train_images/36041171f441.png\n/kaggle/input/aptos2019/train_images/train_images/6ba5ed791444.png\n/kaggle/input/aptos2019/train_images/train_images/8e63fc4ab532.png\n/kaggle/input/aptos2019/train_images/train_images/c2a58b2cfd0b.png\n/kaggle/input/aptos2019/train_images/train_images/691eeb59b4cb.png\n/kaggle/input/aptos2019/train_images/train_images/8660e1864665.png\n/kaggle/input/aptos2019/train_images/train_images/973b0facfa9b.png\n/kaggle/input/aptos2019/train_images/train_images/b5204c0decc7.png\n/kaggle/input/aptos2019/train_images/train_images/7b49041cbf17.png\n/kaggle/input/aptos2019/train_images/train_images/63c3c571b8ee.png\n/kaggle/input/aptos2019/train_images/train_images/e32a359be36d.png\n/kaggle/input/aptos2019/train_images/train_images/aeed1f251ceb.png\n/kaggle/input/aptos2019/train_images/train_images/65f5d2a6eb7e.png\n/kaggle/input/aptos2019/train_images/train_images/2017cd92c63d.png\n/kaggle/input/aptos2019/train_images/train_images/a879c3569552.png\n/kaggle/input/aptos2019/train_images/train_images/5cab3ef4b31c.png\n/kaggle/input/aptos2019/train_images/train_images/a2b995b81692.png\n/kaggle/input/aptos2019/train_images/train_images/7e6e90a93aa5.png\n/kaggle/input/aptos2019/train_images/train_images/8241e43408a8.png\n/kaggle/input/aptos2019/train_images/train_images/a2811f512c1c.png\n/kaggle/input/aptos2019/train_images/train_images/598b8f5b3822.png\n/kaggle/input/aptos2019/train_images/train_images/3c78bfca247b.png\n/kaggle/input/aptos2019/train_images/train_images/44e951e45dca.png\n/kaggle/input/aptos2019/train_images/train_images/3fa4f4d77177.png\n/kaggle/input/aptos2019/train_images/train_images/3254e48c8aa0.png\n/kaggle/input/aptos2019/train_images/train_images/82bb8a01935f.png\n/kaggle/input/aptos2019/train_images/train_images/c12e9ca420a5.png\n/kaggle/input/aptos2019/train_images/train_images/b10fca20c885.png\n/kaggle/input/aptos2019/train_images/train_images/d4f32b9c07df.png\n/kaggle/input/aptos2019/train_images/train_images/905cc86bf100.png\n/kaggle/input/aptos2019/train_images/train_images/7e0598cc88a0.png\n/kaggle/input/aptos2019/train_images/train_images/58ccba7eec9c.png\n/kaggle/input/aptos2019/train_images/train_images/cb28adab4e8a.png\n/kaggle/input/aptos2019/train_images/train_images/75238d945315.png\n/kaggle/input/aptos2019/train_images/train_images/c096131ad065.png\n/kaggle/input/aptos2019/train_images/train_images/4dd71fc7f22b.png\n/kaggle/input/aptos2019/train_images/train_images/56e56aa08362.png\n/kaggle/input/aptos2019/train_images/train_images/519c6e8f78dc.png\n/kaggle/input/aptos2019/train_images/train_images/ab653b8554c0.png\n/kaggle/input/aptos2019/train_images/train_images/68ddb15a74de.png\n/kaggle/input/aptos2019/train_images/train_images/5671eb95512b.png\n/kaggle/input/aptos2019/train_images/train_images/bfe467b7e997.png\n/kaggle/input/aptos2019/train_images/train_images/7eeb191ad06b.png\n/kaggle/input/aptos2019/train_images/train_images/9a1029536d78.png\n/kaggle/input/aptos2019/train_images/train_images/8f06ca4642bd.png\n/kaggle/input/aptos2019/train_images/train_images/64a13949e879.png\n/kaggle/input/aptos2019/train_images/train_images/a11bf2edd470.png\n/kaggle/input/aptos2019/train_images/train_images/ae2e888905ba.png\n/kaggle/input/aptos2019/train_images/train_images/a6356a3c5d11.png\n/kaggle/input/aptos2019/train_images/train_images/365f8c01d994.png\n/kaggle/input/aptos2019/train_images/train_images/c1819db0aece.png\n/kaggle/input/aptos2019/train_images/train_images/2db0cd3e30da.png\n/kaggle/input/aptos2019/train_images/train_images/5bda2ed09e62.png\n/kaggle/input/aptos2019/train_images/train_images/201f6e10c108.png\n/kaggle/input/aptos2019/train_images/train_images/e01b7bac822b.png\n/kaggle/input/aptos2019/train_images/train_images/80b5a9519aec.png\n/kaggle/input/aptos2019/train_images/train_images/72a867980067.png\n/kaggle/input/aptos2019/train_images/train_images/79540be95177.png\n/kaggle/input/aptos2019/train_images/train_images/68332fdcaa70.png\n/kaggle/input/aptos2019/train_images/train_images/44f7f3ef9d50.png\n/kaggle/input/aptos2019/train_images/train_images/1ec95179cdfe.png\n/kaggle/input/aptos2019/train_images/train_images/7f60f2a083d3.png\n/kaggle/input/aptos2019/train_images/train_images/9e7a63b2fc6a.png\n/kaggle/input/aptos2019/train_images/train_images/b746a6681ba9.png\n/kaggle/input/aptos2019/train_images/train_images/81bc03e2ff2b.png\n/kaggle/input/aptos2019/train_images/train_images/39aa3cd93c50.png\n/kaggle/input/aptos2019/train_images/train_images/dd19428c3d29.png\n/kaggle/input/aptos2019/train_images/train_images/c96f743915b5.png\n/kaggle/input/aptos2019/train_images/train_images/c9485c38fdd5.png\n/kaggle/input/aptos2019/train_images/train_images/310c27067ac0.png\n/kaggle/input/aptos2019/train_images/train_images/cb02bb47fdc5.png\n/kaggle/input/aptos2019/train_images/train_images/5548a7961a3e.png\n/kaggle/input/aptos2019/train_images/train_images/495106ae3b68.png\n/kaggle/input/aptos2019/train_images/train_images/ba2ea9182090.png\n/kaggle/input/aptos2019/train_images/train_images/291f581d365e.png\n/kaggle/input/aptos2019/train_images/train_images/a3475dc3ac80.png\n/kaggle/input/aptos2019/train_images/train_images/6fe67482bfae.png\n/kaggle/input/aptos2019/train_images/train_images/286e9981dd9b.png\n/kaggle/input/aptos2019/train_images/train_images/7a39c91416e2.png\n/kaggle/input/aptos2019/train_images/train_images/6c30dd481717.png\n/kaggle/input/aptos2019/train_images/train_images/a56230242a95.png\n/kaggle/input/aptos2019/train_images/train_images/d85d052900b4.png\n/kaggle/input/aptos2019/train_images/train_images/5a444c32cd9a.png\n/kaggle/input/aptos2019/train_images/train_images/47536db39f00.png\n/kaggle/input/aptos2019/train_images/train_images/51e656e5a541.png\n/kaggle/input/aptos2019/train_images/train_images/609be3ca5ddf.png\n/kaggle/input/aptos2019/train_images/train_images/b835b6e31a59.png\n/kaggle/input/aptos2019/train_images/train_images/aa31bc6b8f4d.png\n/kaggle/input/aptos2019/train_images/train_images/1db18bdd43aa.png\n/kaggle/input/aptos2019/train_images/train_images/c5431b81cbc9.png\n/kaggle/input/aptos2019/train_images/train_images/d5a39339ff3d.png\n/kaggle/input/aptos2019/train_images/train_images/ddb222ff7c1d.png\n/kaggle/input/aptos2019/train_images/train_images/be8697eb2078.png\n/kaggle/input/aptos2019/train_images/train_images/ba25f947f4ec.png\n/kaggle/input/aptos2019/train_images/train_images/7347f5133a6a.png\n/kaggle/input/aptos2019/train_images/train_images/dc3c0d8ee20b.png\n/kaggle/input/aptos2019/train_images/train_images/b351ae99413a.png\n/kaggle/input/aptos2019/train_images/train_images/33596a635b53.png\n/kaggle/input/aptos2019/train_images/train_images/3b232b394e4f.png\n/kaggle/input/aptos2019/train_images/train_images/992599744a23.png\n/kaggle/input/aptos2019/train_images/train_images/423abbaa5fad.png\n/kaggle/input/aptos2019/train_images/train_images/9d62478042b6.png\n/kaggle/input/aptos2019/train_images/train_images/3325b1fe55d2.png\n/kaggle/input/aptos2019/train_images/train_images/4dd9d29eae5d.png\n/kaggle/input/aptos2019/train_images/train_images/7247a2c97f71.png\n/kaggle/input/aptos2019/train_images/train_images/d41b33fcb94f.png\n/kaggle/input/aptos2019/train_images/train_images/cd01672507c9.png\n/kaggle/input/aptos2019/train_images/train_images/8dfa629ca74e.png\n/kaggle/input/aptos2019/train_images/train_images/d9311f7497cb.png\n/kaggle/input/aptos2019/train_images/train_images/8bed09514c3b.png\n/kaggle/input/aptos2019/train_images/train_images/7efc91af4ae6.png\n/kaggle/input/aptos2019/train_images/train_images/9287e57326d0.png\n/kaggle/input/aptos2019/train_images/train_images/d1fa0f744620.png\n/kaggle/input/aptos2019/train_images/train_images/ca1036496659.png\n/kaggle/input/aptos2019/train_images/train_images/971bb98ab935.png\n/kaggle/input/aptos2019/train_images/train_images/b7a1bb106051.png\n/kaggle/input/aptos2019/train_images/train_images/d1ca85af57c9.png\n/kaggle/input/aptos2019/train_images/train_images/3f5b4c2948e8.png\n/kaggle/input/aptos2019/train_images/train_images/821789e9053f.png\n/kaggle/input/aptos2019/train_images/train_images/4403538fb50f.png\n/kaggle/input/aptos2019/train_images/train_images/da949aa67a4f.png\n/kaggle/input/aptos2019/train_images/train_images/25a0a1e41afd.png\n/kaggle/input/aptos2019/train_images/train_images/b8ebedd382de.png\n/kaggle/input/aptos2019/train_images/train_images/8bdb891661a8.png\n/kaggle/input/aptos2019/train_images/train_images/587146a55885.png\n/kaggle/input/aptos2019/train_images/train_images/1c9c583c10bf.png\n/kaggle/input/aptos2019/train_images/train_images/86d6808f0609.png\n/kaggle/input/aptos2019/train_images/train_images/64eb5a79dfdd.png\n/kaggle/input/aptos2019/train_images/train_images/493d99f030e2.png\n/kaggle/input/aptos2019/train_images/train_images/4cf4d528c08e.png\n/kaggle/input/aptos2019/train_images/train_images/4c570172778b.png\n/kaggle/input/aptos2019/train_images/train_images/c5ba9e455d5e.png\n/kaggle/input/aptos2019/train_images/train_images/9a3109657ac1.png\n/kaggle/input/aptos2019/train_images/train_images/33b978734eab.png\n/kaggle/input/aptos2019/train_images/train_images/a77eb914b383.png\n/kaggle/input/aptos2019/train_images/train_images/c739ff9580d3.png\n/kaggle/input/aptos2019/train_images/train_images/3435fd8675a2.png\n/kaggle/input/aptos2019/train_images/train_images/ab686895533e.png\n/kaggle/input/aptos2019/train_images/train_images/ae8472f8d310.png\n/kaggle/input/aptos2019/train_images/train_images/224bb938e2dd.png\n/kaggle/input/aptos2019/train_images/train_images/475300735b7f.png\n/kaggle/input/aptos2019/train_images/train_images/c0f15fe3b4b7.png\n/kaggle/input/aptos2019/train_images/train_images/a66c3165876f.png\n/kaggle/input/aptos2019/train_images/train_images/d93b61dc8f64.png\n/kaggle/input/aptos2019/train_images/train_images/d0079cc188e9.png\n/kaggle/input/aptos2019/train_images/train_images/73881f55a3ec.png\n/kaggle/input/aptos2019/train_images/train_images/e34fa07bd64d.png\n/kaggle/input/aptos2019/train_images/train_images/e4ae1ee6aada.png\n/kaggle/input/aptos2019/train_images/train_images/233d948e2544.png\n/kaggle/input/aptos2019/train_images/train_images/68987fb159ab.png\n/kaggle/input/aptos2019/train_images/train_images/3a643599f852.png\n/kaggle/input/aptos2019/train_images/train_images/d516f77d4516.png\n/kaggle/input/aptos2019/train_images/train_images/3286073a976e.png\n/kaggle/input/aptos2019/train_images/train_images/b96b518596b3.png\n/kaggle/input/aptos2019/train_images/train_images/4158c340fa49.png\n/kaggle/input/aptos2019/train_images/train_images/c52bb7343387.png\n/kaggle/input/aptos2019/train_images/train_images/cc12453ea915.png\n/kaggle/input/aptos2019/train_images/train_images/5b068765e846.png\n/kaggle/input/aptos2019/train_images/train_images/3fc219927a97.png\n/kaggle/input/aptos2019/train_images/train_images/7743f4e04a6d.png\n/kaggle/input/aptos2019/train_images/train_images/7d626a7ffe76.png\n/kaggle/input/aptos2019/train_images/train_images/419406328dcd.png\n/kaggle/input/aptos2019/train_images/train_images/80b5697f2a5e.png\n/kaggle/input/aptos2019/train_images/train_images/38055d8b9f08.png\n/kaggle/input/aptos2019/train_images/train_images/1d11794057ff.png\n/kaggle/input/aptos2019/train_images/train_images/2fde69f20585.png\n/kaggle/input/aptos2019/train_images/train_images/810d3779abd9.png\n/kaggle/input/aptos2019/train_images/train_images/bfefa7344e7d.png\n/kaggle/input/aptos2019/train_images/train_images/51cd8d2057fa.png\n/kaggle/input/aptos2019/train_images/train_images/b7278b4f2448.png\n/kaggle/input/aptos2019/train_images/train_images/d911dd40c63b.png\n/kaggle/input/aptos2019/train_images/train_images/aebe87a423c8.png\n/kaggle/input/aptos2019/train_images/train_images/2700754f71e9.png\n/kaggle/input/aptos2019/train_images/train_images/437900a99871.png\n/kaggle/input/aptos2019/train_images/train_images/6bb30ec3231a.png\n/kaggle/input/aptos2019/train_images/train_images/ab724603ee93.png\n/kaggle/input/aptos2019/train_images/train_images/50915e2329a1.png\n/kaggle/input/aptos2019/train_images/train_images/e16fc934069f.png\n/kaggle/input/aptos2019/train_images/train_images/d38cf0f4a9af.png\n/kaggle/input/aptos2019/train_images/train_images/a32886cb31ab.png\n/kaggle/input/aptos2019/train_images/train_images/4958bfcc9f38.png\n/kaggle/input/aptos2019/train_images/train_images/a015ce4f51ad.png\n/kaggle/input/aptos2019/train_images/train_images/d871895742b1.png\n/kaggle/input/aptos2019/train_images/train_images/8785b71238d8.png\n/kaggle/input/aptos2019/train_images/train_images/916915f01e17.png\n/kaggle/input/aptos2019/train_images/train_images/c365c598ad4e.png\n/kaggle/input/aptos2019/train_images/train_images/75a071608ea6.png\n/kaggle/input/aptos2019/train_images/train_images/5090917a2676.png\n/kaggle/input/aptos2019/train_images/train_images/b87f9c59748b.png\n/kaggle/input/aptos2019/train_images/train_images/a61723fc38c2.png\n/kaggle/input/aptos2019/train_images/train_images/837acf120946.png\n/kaggle/input/aptos2019/train_images/train_images/6bcce181be65.png\n/kaggle/input/aptos2019/train_images/train_images/7aabd768abff.png\n/kaggle/input/aptos2019/train_images/train_images/83038ca49b6d.png\n/kaggle/input/aptos2019/train_images/train_images/74eee788edee.png\n/kaggle/input/aptos2019/train_images/train_images/64678182d8a8.png\n/kaggle/input/aptos2019/train_images/train_images/82ac8463fadd.png\n/kaggle/input/aptos2019/train_images/train_images/7c2e852171c0.png\n/kaggle/input/aptos2019/train_images/train_images/27b68863349f.png\n/kaggle/input/aptos2019/train_images/train_images/992b9a07b25f.png\n/kaggle/input/aptos2019/train_images/train_images/2f8d14a7d390.png\n/kaggle/input/aptos2019/train_images/train_images/681c3c115684.png\n/kaggle/input/aptos2019/train_images/train_images/a9dc80cba9a4.png\n/kaggle/input/aptos2019/train_images/train_images/92d8a7c8e718.png\n/kaggle/input/aptos2019/train_images/train_images/613bacb35c05.png\n/kaggle/input/aptos2019/train_images/train_images/7cc4b7aabe04.png\n/kaggle/input/aptos2019/train_images/train_images/5b76117c4bcb.png\n/kaggle/input/aptos2019/train_images/train_images/af7a36454670.png\n/kaggle/input/aptos2019/train_images/train_images/57f5ad4b5b29.png\n/kaggle/input/aptos2019/train_images/train_images/26999ebc21de.png\n/kaggle/input/aptos2019/train_images/train_images/e265c870f9b3.png\n/kaggle/input/aptos2019/train_images/train_images/50d8a8fb7737.png\n/kaggle/input/aptos2019/train_images/train_images/2a3378bcfbcc.png\n/kaggle/input/aptos2019/train_images/train_images/c64c0966b4cf.png\n/kaggle/input/aptos2019/train_images/train_images/869bbd3170cc.png\n/kaggle/input/aptos2019/train_images/train_images/1fddd7c98fd2.png\n/kaggle/input/aptos2019/train_images/train_images/97c6cb55866d.png\n/kaggle/input/aptos2019/train_images/train_images/ceaa5803d780.png\n/kaggle/input/aptos2019/train_images/train_images/3f6c627e2ff2.png\n/kaggle/input/aptos2019/train_images/train_images/ba4e62c11cc0.png\n/kaggle/input/aptos2019/train_images/train_images/c7c0470bcf87.png\n/kaggle/input/aptos2019/train_images/train_images/9d98a0b585f2.png\n/kaggle/input/aptos2019/train_images/train_images/8714d17bb6da.png\n/kaggle/input/aptos2019/train_images/train_images/753b14c27c83.png\n/kaggle/input/aptos2019/train_images/train_images/ceb32a193eff.png\n/kaggle/input/aptos2019/train_images/train_images/4289af3afbd2.png\n/kaggle/input/aptos2019/train_images/train_images/bacfa2b8e706.png\n/kaggle/input/aptos2019/train_images/train_images/8a87dd2a784e.png\n/kaggle/input/aptos2019/train_images/train_images/1d14dd912671.png\n/kaggle/input/aptos2019/train_images/train_images/912fbe06407e.png\n/kaggle/input/aptos2019/train_images/train_images/3fe282197c1c.png\n/kaggle/input/aptos2019/train_images/train_images/bd9904495ccd.png\n/kaggle/input/aptos2019/train_images/train_images/4e231670b48c.png\n/kaggle/input/aptos2019/train_images/train_images/977e1ca77653.png\n/kaggle/input/aptos2019/train_images/train_images/a5a2a7003d60.png\n/kaggle/input/aptos2019/train_images/train_images/4c635a01593d.png\n/kaggle/input/aptos2019/train_images/train_images/bd269a1f0e4d.png\n/kaggle/input/aptos2019/train_images/train_images/c3cd0200df79.png\n/kaggle/input/aptos2019/train_images/train_images/5173d54fc214.png\n/kaggle/input/aptos2019/train_images/train_images/3710ff45299c.png\n/kaggle/input/aptos2019/train_images/train_images/98e8adcf085c.png\n/kaggle/input/aptos2019/train_images/train_images/42a67337fa8e.png\n/kaggle/input/aptos2019/train_images/train_images/6fe67fd7f5d1.png\n/kaggle/input/aptos2019/train_images/train_images/a7b03e58a6e1.png\n/kaggle/input/aptos2019/train_images/train_images/23148a40ecb0.png\n/kaggle/input/aptos2019/train_images/train_images/9a56cfb980ec.png\n/kaggle/input/aptos2019/train_images/train_images/54b322c66d01.png\n/kaggle/input/aptos2019/train_images/train_images/a06e41bd2634.png\n/kaggle/input/aptos2019/train_images/train_images/bacfb1029f6b.png\n/kaggle/input/aptos2019/train_images/train_images/a44345b27804.png\n/kaggle/input/aptos2019/train_images/train_images/85fce24084da.png\n/kaggle/input/aptos2019/train_images/train_images/4a7dc013e802.png\n/kaggle/input/aptos2019/train_images/train_images/5b72ff04333d.png\n/kaggle/input/aptos2019/train_images/train_images/c027e5482e8c.png\n/kaggle/input/aptos2019/train_images/train_images/69591ebb198d.png\n/kaggle/input/aptos2019/train_images/train_images/e26bcae6c67b.png\n/kaggle/input/aptos2019/train_images/train_images/6e68e742f5bc.png\n/kaggle/input/aptos2019/train_images/train_images/66bfec8d6bcd.png\n/kaggle/input/aptos2019/train_images/train_images/7ea756985353.png\n/kaggle/input/aptos2019/train_images/train_images/1ae8c165fd53.png\n/kaggle/input/aptos2019/train_images/train_images/dd3176bacfe2.png\n/kaggle/input/aptos2019/train_images/train_images/bb9a3d835a94.png\n/kaggle/input/aptos2019/train_images/train_images/bd06028eb7dd.png\n/kaggle/input/aptos2019/train_images/train_images/91b6ebaa3678.png\n/kaggle/input/aptos2019/train_images/train_images/e4a44f9158dc.png\n/kaggle/input/aptos2019/train_images/train_images/266fbefa58fb.png\n/kaggle/input/aptos2019/train_images/train_images/b033ab4fb723.png\n/kaggle/input/aptos2019/train_images/train_images/4cde86044ad1.png\n/kaggle/input/aptos2019/train_images/train_images/6d7d26025122.png\n/kaggle/input/aptos2019/train_images/train_images/2da82d14e1b7.png\n/kaggle/input/aptos2019/train_images/train_images/a721efb1e049.png\n/kaggle/input/aptos2019/train_images/train_images/9b4fc15df3c8.png\n/kaggle/input/aptos2019/train_images/train_images/1e7ccd4a1c87.png\n/kaggle/input/aptos2019/train_images/train_images/9c14ce27cbfc.png\n/kaggle/input/aptos2019/train_images/train_images/3cab32dd6ef9.png\n/kaggle/input/aptos2019/train_images/train_images/bcd503c726ba.png\n/kaggle/input/aptos2019/train_images/train_images/d0ffa0425ef1.png\n/kaggle/input/aptos2019/train_images/train_images/af5a0bc4e1fa.png\n/kaggle/input/aptos2019/train_images/train_images/a4359815f152.png\n/kaggle/input/aptos2019/train_images/train_images/42cc993f23a9.png\n/kaggle/input/aptos2019/train_images/train_images/d91635f380b4.png\n/kaggle/input/aptos2019/train_images/train_images/cc9270f06b65.png\n/kaggle/input/aptos2019/train_images/train_images/3e1f8fecb06f.png\n/kaggle/input/aptos2019/train_images/train_images/b90bc89ce8d8.png\n/kaggle/input/aptos2019/train_images/train_images/525d0dd8dc45.png\n/kaggle/input/aptos2019/train_images/train_images/8b26d3cd61e8.png\n/kaggle/input/aptos2019/train_images/train_images/a1822dd8d05d.png\n/kaggle/input/aptos2019/train_images/train_images/3f8d5c940ba4.png\n/kaggle/input/aptos2019/train_images/train_images/9a9b21215c55.png\n/kaggle/input/aptos2019/train_images/train_images/d06ccd0cf4b8.png\n/kaggle/input/aptos2019/train_images/train_images/1f07dae3cadb.png\n/kaggle/input/aptos2019/train_images/train_images/b3819a805dca.png\n/kaggle/input/aptos2019/train_images/train_images/6b3860e8f64f.png\n/kaggle/input/aptos2019/train_images/train_images/71e43b4f8ba6.png\n/kaggle/input/aptos2019/train_images/train_images/1e143fa3de57.png\n/kaggle/input/aptos2019/train_images/train_images/8ff863f8874f.png\n/kaggle/input/aptos2019/train_images/train_images/50ddd7d976df.png\n/kaggle/input/aptos2019/train_images/train_images/44a4d04162cc.png\n/kaggle/input/aptos2019/train_images/train_images/8a67f1efa315.png\n/kaggle/input/aptos2019/train_images/train_images/97a235367f9d.png\n/kaggle/input/aptos2019/train_images/train_images/6061f5b7378d.png\n/kaggle/input/aptos2019/train_images/train_images/7da558d92100.png\n/kaggle/input/aptos2019/train_images/train_images/78d53c82a23e.png\n/kaggle/input/aptos2019/train_images/train_images/5ba156a35ff2.png\n/kaggle/input/aptos2019/train_images/train_images/80a02014b418.png\n/kaggle/input/aptos2019/train_images/train_images/2d04cead4d3a.png\n/kaggle/input/aptos2019/train_images/train_images/73e83a07a16d.png\n/kaggle/input/aptos2019/train_images/train_images/2608e1dac5b1.png\n/kaggle/input/aptos2019/train_images/train_images/35d6c4c50072.png\n/kaggle/input/aptos2019/train_images/train_images/5777ef74c9ec.png\n/kaggle/input/aptos2019/train_images/train_images/dbee04ae6426.png\n/kaggle/input/aptos2019/train_images/train_images/d332d7b8a26e.png\n/kaggle/input/aptos2019/train_images/train_images/e29e54ff921e.png\n/kaggle/input/aptos2019/train_images/train_images/4dd4a4bf2421.png\n/kaggle/input/aptos2019/train_images/train_images/76e589911303.png\n/kaggle/input/aptos2019/train_images/train_images/8b8fe3fc8950.png\n/kaggle/input/aptos2019/train_images/train_images/9fefe2b44795.png\n/kaggle/input/aptos2019/train_images/train_images/82088c6734e6.png\n/kaggle/input/aptos2019/train_images/train_images/697538183db5.png\n/kaggle/input/aptos2019/train_images/train_images/807135cbc438.png\n/kaggle/input/aptos2019/train_images/train_images/43bc7c066dfb.png\n/kaggle/input/aptos2019/train_images/train_images/bca2bdc15fc5.png\n/kaggle/input/aptos2019/train_images/train_images/c5a0e84e955d.png\n/kaggle/input/aptos2019/train_images/train_images/72c31aa48e2c.png\n/kaggle/input/aptos2019/train_images/train_images/4e1e252317b5.png\n/kaggle/input/aptos2019/train_images/train_images/ad3fc5076852.png\n/kaggle/input/aptos2019/train_images/train_images/c9ea9d5eab65.png\n/kaggle/input/aptos2019/train_images/train_images/3c9529918097.png\n/kaggle/input/aptos2019/train_images/train_images/d7078e8b0349.png\n/kaggle/input/aptos2019/train_images/train_images/b98f77098b9d.png\n/kaggle/input/aptos2019/train_images/train_images/44c869174e3a.png\n/kaggle/input/aptos2019/train_images/train_images/d51c2153d151.png\n/kaggle/input/aptos2019/train_images/train_images/84b79243e430.png\n/kaggle/input/aptos2019/train_images/train_images/c5a6f432a1ec.png\n/kaggle/input/aptos2019/train_images/train_images/cd66754e1b3b.png\n/kaggle/input/aptos2019/train_images/train_images/5511f114e7ee.png\n/kaggle/input/aptos2019/train_images/train_images/c05b7b4c22fe.png\n/kaggle/input/aptos2019/train_images/train_images/c2d2b4f536da.png\n/kaggle/input/aptos2019/train_images/train_images/a19507501b40.png\n/kaggle/input/aptos2019/train_images/train_images/40e9b5630438.png\n/kaggle/input/aptos2019/train_images/train_images/bd375ba756b1.png\n/kaggle/input/aptos2019/train_images/train_images/d18aea8238a0.png\n/kaggle/input/aptos2019/train_images/train_images/df84e7113003.png\n/kaggle/input/aptos2019/train_images/train_images/21d18b022429.png\n/kaggle/input/aptos2019/train_images/train_images/7347bd23ba80.png\n/kaggle/input/aptos2019/train_images/train_images/cf0575534cec.png\n/kaggle/input/aptos2019/train_images/train_images/2bb3c492d6d3.png\n/kaggle/input/aptos2019/train_images/train_images/dea7538bb91a.png\n/kaggle/input/aptos2019/train_images/train_images/b3a994760537.png\n/kaggle/input/aptos2019/train_images/train_images/c102db7634d8.png\n/kaggle/input/aptos2019/train_images/train_images/541db13517e2.png\n/kaggle/input/aptos2019/train_images/train_images/7f39c36469b5.png\n/kaggle/input/aptos2019/train_images/train_images/3ee17aa12e46.png\n/kaggle/input/aptos2019/train_images/train_images/3f73c91b7e32.png\n/kaggle/input/aptos2019/train_images/train_images/a505981d1cab.png\n/kaggle/input/aptos2019/train_images/train_images/5d6239c0fd39.png\n/kaggle/input/aptos2019/train_images/train_images/73a07e2ea23e.png\n/kaggle/input/aptos2019/train_images/train_images/9785805af1b8.png\n/kaggle/input/aptos2019/train_images/train_images/bb5083fae98f.png\n/kaggle/input/aptos2019/train_images/train_images/8e3b79e1f1f7.png\n/kaggle/input/aptos2019/train_images/train_images/76cb010f7aa0.png\n/kaggle/input/aptos2019/train_images/train_images/8a25a080f28f.png\n/kaggle/input/aptos2019/train_images/train_images/7ba6b23c4b46.png\n/kaggle/input/aptos2019/train_images/train_images/76f3473df8a6.png\n/kaggle/input/aptos2019/train_images/train_images/222d0ac042b4.png\n/kaggle/input/aptos2019/train_images/train_images/7dee6bf8b9c1.png\n/kaggle/input/aptos2019/train_images/train_images/5ca73d28f17f.png\n/kaggle/input/aptos2019/train_images/train_images/3580a545016d.png\n/kaggle/input/aptos2019/train_images/train_images/e25ccfe38e44.png\n/kaggle/input/aptos2019/train_images/train_images/6155cf375354.png\n/kaggle/input/aptos2019/train_images/train_images/df4aec4a0eaf.png\n/kaggle/input/aptos2019/train_images/train_images/7569ac24762e.png\n/kaggle/input/aptos2019/train_images/train_images/b2aaa81cc8f0.png\n/kaggle/input/aptos2019/train_images/train_images/4ef0b485a7da.png\n/kaggle/input/aptos2019/train_images/train_images/a06a63d866b2.png\n/kaggle/input/aptos2019/train_images/train_images/cfed7c1172ec.png\n/kaggle/input/aptos2019/train_images/train_images/9ed6c2b25767.png\n/kaggle/input/aptos2019/train_images/train_images/b69c224edd6e.png\n/kaggle/input/aptos2019/train_images/train_images/e3ec668f6fad.png\n/kaggle/input/aptos2019/train_images/train_images/b5c80d0ed0ff.png\n/kaggle/input/aptos2019/train_images/train_images/daff5427c9b2.png\n/kaggle/input/aptos2019/train_images/train_images/c261b1aaa828.png\n/kaggle/input/aptos2019/train_images/train_images/a3d2a0c4cd17.png\n/kaggle/input/aptos2019/train_images/train_images/ab1c20a94f3f.png\n/kaggle/input/aptos2019/train_images/train_images/684dd88a0d49.png\n/kaggle/input/aptos2019/train_images/train_images/de50dfa745f8.png\n/kaggle/input/aptos2019/train_images/train_images/7e9458de5707.png\n/kaggle/input/aptos2019/train_images/train_images/82e5bc01f8a4.png\n/kaggle/input/aptos2019/train_images/train_images/35cd9832fc0a.png\n/kaggle/input/aptos2019/train_images/train_images/578109578b46.png\n/kaggle/input/aptos2019/train_images/train_images/1c4f3aa4df06.png\n/kaggle/input/aptos2019/train_images/train_images/c5e238aa18be.png\n/kaggle/input/aptos2019/train_images/train_images/20d5fdd450ae.png\n/kaggle/input/aptos2019/train_images/train_images/34723fae6475.png\n/kaggle/input/aptos2019/train_images/train_images/1b862fb6f65d.png\n/kaggle/input/aptos2019/train_images/train_images/9c893e16c055.png\n/kaggle/input/aptos2019/train_images/train_images/633fe9dbaf39.png\n/kaggle/input/aptos2019/train_images/train_images/6dfd80748e72.png\n/kaggle/input/aptos2019/train_images/train_images/9fa02dfb5553.png\n/kaggle/input/aptos2019/train_images/train_images/bb2f89488ecd.png\n/kaggle/input/aptos2019/train_images/train_images/8c2f0f04e1ed.png\n/kaggle/input/aptos2019/train_images/train_images/c334f8688b77.png\n/kaggle/input/aptos2019/train_images/train_images/baaca2f7e1f0.png\n/kaggle/input/aptos2019/train_images/train_images/4462fba1d2a1.png\n/kaggle/input/aptos2019/train_images/train_images/bda7ff3b1562.png\n/kaggle/input/aptos2019/train_images/train_images/7116128c65ab.png\n/kaggle/input/aptos2019/train_images/train_images/d2afca74cbc3.png\n/kaggle/input/aptos2019/train_images/train_images/2f4e81787d9b.png\n/kaggle/input/aptos2019/train_images/train_images/d78b7401096f.png\n/kaggle/input/aptos2019/train_images/train_images/ccea49708830.png\n/kaggle/input/aptos2019/train_images/train_images/3599029efeb3.png\n/kaggle/input/aptos2019/train_images/train_images/c597ef460944.png\n/kaggle/input/aptos2019/train_images/train_images/5e97cb2b0888.png\n/kaggle/input/aptos2019/train_images/train_images/5d024177e214.png\n/kaggle/input/aptos2019/train_images/train_images/ca05f7e7801b.png\n/kaggle/input/aptos2019/train_images/train_images/1dfbede13143.png\n/kaggle/input/aptos2019/train_images/train_images/315c1a0d87fd.png\n/kaggle/input/aptos2019/train_images/train_images/4e4a6224a04e.png\n/kaggle/input/aptos2019/train_images/train_images/80ca40196225.png\n/kaggle/input/aptos2019/train_images/train_images/3461dc601cc2.png\n/kaggle/input/aptos2019/train_images/train_images/a987aa7aac37.png\n/kaggle/input/aptos2019/train_images/train_images/2a2274bcb00a.png\n/kaggle/input/aptos2019/train_images/train_images/d774692d9919.png\n/kaggle/input/aptos2019/train_images/train_images/949710bead24.png\n/kaggle/input/aptos2019/train_images/train_images/b9bc81fcb075.png\n/kaggle/input/aptos2019/train_images/train_images/4d9fc85a8259.png\n/kaggle/input/aptos2019/train_images/train_images/c40976189f22.png\n/kaggle/input/aptos2019/train_images/train_images/a86128b601a7.png\n/kaggle/input/aptos2019/train_images/train_images/ce207b69ff37.png\n/kaggle/input/aptos2019/train_images/train_images/913490237ad4.png\n/kaggle/input/aptos2019/train_images/train_images/c947bb6cf9f6.png\n/kaggle/input/aptos2019/train_images/train_images/b6304c545f95.png\n/kaggle/input/aptos2019/train_images/train_images/525acfea47e8.png\n/kaggle/input/aptos2019/train_images/train_images/6d0c0531083f.png\n/kaggle/input/aptos2019/train_images/train_images/537e50fdf22e.png\n/kaggle/input/aptos2019/train_images/train_images/61f403fdb434.png\n/kaggle/input/aptos2019/train_images/train_images/9f1b14dfa14c.png\n/kaggle/input/aptos2019/train_images/train_images/a26f50218b84.png\n/kaggle/input/aptos2019/train_images/train_images/a0a0cd8af5a6.png\n/kaggle/input/aptos2019/train_images/train_images/51aa3361294c.png\n/kaggle/input/aptos2019/train_images/train_images/ca25745942b0.png\n/kaggle/input/aptos2019/train_images/train_images/360832d84ce0.png\n/kaggle/input/aptos2019/train_images/train_images/537e5c578f40.png\n/kaggle/input/aptos2019/train_images/train_images/b460ca9fa26f.png\n/kaggle/input/aptos2019/train_images/train_images/462937ece243.png\n/kaggle/input/aptos2019/train_images/train_images/cd5714db652d.png\n/kaggle/input/aptos2019/train_images/train_images/a7ec056502e7.png\n/kaggle/input/aptos2019/train_images/train_images/a01024054596.png\n/kaggle/input/aptos2019/train_images/train_images/cd972e5639e0.png\n/kaggle/input/aptos2019/train_images/train_images/3c326543fff6.png\n/kaggle/input/aptos2019/train_images/train_images/7526c59c36d3.png\n/kaggle/input/aptos2019/train_images/train_images/51d0034d177d.png\n/kaggle/input/aptos2019/train_images/train_images/b2b79b37d314.png\n/kaggle/input/aptos2019/train_images/train_images/7bc2e0fa3f72.png\n/kaggle/input/aptos2019/train_images/train_images/b13d72ceea26.png\n/kaggle/input/aptos2019/train_images/train_images/b65ff67743b2.png\n/kaggle/input/aptos2019/train_images/train_images/664b1f9a2087.png\n/kaggle/input/aptos2019/train_images/train_images/c56293f53191.png\n/kaggle/input/aptos2019/train_images/train_images/606daaf0bfc7.png\n/kaggle/input/aptos2019/train_images/train_images/afc744fad65e.png\n/kaggle/input/aptos2019/train_images/train_images/38f1901f214a.png\n/kaggle/input/aptos2019/train_images/train_images/a0adbe677508.png\n/kaggle/input/aptos2019/train_images/train_images/cd3fd04d72f5.png\n/kaggle/input/aptos2019/train_images/train_images/6e92b1c5ac8e.png\n/kaggle/input/aptos2019/train_images/train_images/cb602182cde3.png\n/kaggle/input/aptos2019/train_images/train_images/2408799a09b2.png\n/kaggle/input/aptos2019/train_images/train_images/38e111cac46f.png\n/kaggle/input/aptos2019/train_images/train_images/937bc1b924b1.png\n/kaggle/input/aptos2019/train_images/train_images/8958a4d17b7e.png\n/kaggle/input/aptos2019/train_images/train_images/add1d681d712.png\n/kaggle/input/aptos2019/train_images/train_images/46acc506fa61.png\n/kaggle/input/aptos2019/train_images/train_images/54cab3596214.png\n/kaggle/input/aptos2019/train_images/train_images/af6a1508cd95.png\n/kaggle/input/aptos2019/train_images/train_images/cb0cc98d7e35.png\n/kaggle/input/aptos2019/train_images/train_images/7c629b491d1a.png\n/kaggle/input/aptos2019/train_images/train_images/e0d229db881a.png\n/kaggle/input/aptos2019/train_images/train_images/a07d571bf7ba.png\n/kaggle/input/aptos2019/train_images/train_images/aa841de1ee82.png\n/kaggle/input/aptos2019/train_images/train_images/4926dea289f8.png\n/kaggle/input/aptos2019/train_images/train_images/be6cbf6e5b10.png\n/kaggle/input/aptos2019/train_images/train_images/a80dab8eddf4.png\n/kaggle/input/aptos2019/train_images/train_images/8d62ba9cb22a.png\n/kaggle/input/aptos2019/train_images/train_images/c26f98f58350.png\n/kaggle/input/aptos2019/train_images/train_images/7f6690fa390a.png\n/kaggle/input/aptos2019/train_images/train_images/2b21d293fdf2.png\n/kaggle/input/aptos2019/train_images/train_images/a8b637abd96b.png\n/kaggle/input/aptos2019/train_images/train_images/d81338217fc5.png\n/kaggle/input/aptos2019/train_images/train_images/7f6ce40f306b.png\n/kaggle/input/aptos2019/train_images/train_images/4242c0d87f57.png\n/kaggle/input/aptos2019/train_images/train_images/58a9e0d7f7af.png\n/kaggle/input/aptos2019/train_images/train_images/d26bb2ed6e71.png\n/kaggle/input/aptos2019/train_images/train_images/1df3e03a8f5f.png\n/kaggle/input/aptos2019/train_images/train_images/922586d86cd8.png\n/kaggle/input/aptos2019/train_images/train_images/b0acd3593310.png\n/kaggle/input/aptos2019/train_images/train_images/a763661f98a5.png\n/kaggle/input/aptos2019/train_images/train_images/a9a28c37c8c4.png\n/kaggle/input/aptos2019/train_images/train_images/7550966ef777.png\n/kaggle/input/aptos2019/train_images/train_images/478fc46eaa49.png\n/kaggle/input/aptos2019/train_images/train_images/42b9c1977681.png\n/kaggle/input/aptos2019/train_images/train_images/b759cbef90c5.png\n/kaggle/input/aptos2019/train_images/train_images/78a577c3e0bf.png\n/kaggle/input/aptos2019/train_images/train_images/a150ff5dfe07.png\n/kaggle/input/aptos2019/train_images/train_images/3c42512c81e0.png\n/kaggle/input/aptos2019/train_images/train_images/79be2ff796bf.png\n/kaggle/input/aptos2019/train_images/train_images/62318d514160.png\n/kaggle/input/aptos2019/train_images/train_images/d1cf31577a59.png\n/kaggle/input/aptos2019/train_images/train_images/b0b3b16fc305.png\n/kaggle/input/aptos2019/train_images/train_images/53327edb9e4d.png\n/kaggle/input/aptos2019/train_images/train_images/df3adfd6ba36.png\n/kaggle/input/aptos2019/train_images/train_images/27fca9f12b3c.png\n/kaggle/input/aptos2019/train_images/train_images/ab88081e5654.png\n/kaggle/input/aptos2019/train_images/train_images/803120c5d287.png\n/kaggle/input/aptos2019/train_images/train_images/e251bdf05b85.png\n/kaggle/input/aptos2019/train_images/train_images/34a7dbd3f05c.png\n/kaggle/input/aptos2019/train_images/train_images/58b866484a05.png\n/kaggle/input/aptos2019/train_images/train_images/838c87c63422.png\n/kaggle/input/aptos2019/train_images/train_images/84b4da14bc23.png\n/kaggle/input/aptos2019/train_images/train_images/b72a86d61959.png\n/kaggle/input/aptos2019/train_images/train_images/d6f6bdfd8011.png\n/kaggle/input/aptos2019/train_images/train_images/a821b6ecef33.png\n/kaggle/input/aptos2019/train_images/train_images/77543f66a84a.png\n/kaggle/input/aptos2019/train_images/train_images/3b5dffe159b6.png\n/kaggle/input/aptos2019/train_images/train_images/46c1548d730e.png\n/kaggle/input/aptos2019/train_images/train_images/43e9c66eb0f3.png\n/kaggle/input/aptos2019/train_images/train_images/36b5b3c9fb32.png\n/kaggle/input/aptos2019/train_images/train_images/5257cb536da2.png\n/kaggle/input/aptos2019/train_images/train_images/c280730cc211.png\n/kaggle/input/aptos2019/train_images/train_images/e0b5a982a018.png\n/kaggle/input/aptos2019/train_images/train_images/b7983cb3f270.png\n/kaggle/input/aptos2019/train_images/train_images/e39b627cf648.png\n/kaggle/input/aptos2019/train_images/train_images/5445255635f0.png\n/kaggle/input/aptos2019/train_images/train_images/6daef3e5ca22.png\n/kaggle/input/aptos2019/train_images/train_images/9e2058917304.png\n/kaggle/input/aptos2019/train_images/train_images/a7673ac44509.png\n/kaggle/input/aptos2019/train_images/train_images/da9574d35b82.png\n/kaggle/input/aptos2019/train_images/train_images/314862758acf.png\n/kaggle/input/aptos2019/train_images/train_images/8eb3337a54e9.png\n/kaggle/input/aptos2019/train_images/train_images/d5b4705ac2ee.png\n/kaggle/input/aptos2019/train_images/train_images/84b88e8d3bca.png\n/kaggle/input/aptos2019/train_images/train_images/5cf9127f251a.png\n/kaggle/input/aptos2019/train_images/train_images/840a06a9c690.png\n/kaggle/input/aptos2019/train_images/train_images/d952dbfb0fe4.png\n/kaggle/input/aptos2019/train_images/train_images/36677b70b1ef.png\n/kaggle/input/aptos2019/train_images/train_images/a19ecd0a706e.png\n/kaggle/input/aptos2019/train_images/train_images/4e6071b73120.png\n/kaggle/input/aptos2019/train_images/train_images/b762c29cf2f3.png\n/kaggle/input/aptos2019/train_images/train_images/e2c39ed0c941.png\n/kaggle/input/aptos2019/train_images/train_images/c546670d9684.png\n/kaggle/input/aptos2019/train_images/train_images/99240ee00485.png\n/kaggle/input/aptos2019/train_images/train_images/6cfb7b44ef6f.png\n/kaggle/input/aptos2019/train_images/train_images/2821998fc002.png\n/kaggle/input/aptos2019/train_images/train_images/42af7282349b.png\n/kaggle/input/aptos2019/train_images/train_images/99132193eaa0.png\n/kaggle/input/aptos2019/train_images/train_images/222f3ee3a1e8.png\n/kaggle/input/aptos2019/train_images/train_images/b95d4dd8e5e2.png\n/kaggle/input/aptos2019/train_images/train_images/b22cc1bf0b8a.png\n/kaggle/input/aptos2019/train_images/train_images/36ec36c301c1.png\n/kaggle/input/aptos2019/train_images/train_images/5069feccd866.png\n/kaggle/input/aptos2019/train_images/train_images/a9bc2f892cb3.png\n/kaggle/input/aptos2019/train_images/train_images/87b1938994b5.png\n/kaggle/input/aptos2019/train_images/train_images/8a8a251770cd.png\n/kaggle/input/aptos2019/train_images/train_images/7e4019ac7f5a.png\n/kaggle/input/aptos2019/train_images/train_images/47d1603a555b.png\n/kaggle/input/aptos2019/train_images/train_images/7d8f67cadc29.png\n/kaggle/input/aptos2019/train_images/train_images/74418f620068.png\n/kaggle/input/aptos2019/train_images/train_images/20c883d3bd38.png\n/kaggle/input/aptos2019/train_images/train_images/2fb3a8606a77.png\n/kaggle/input/aptos2019/train_images/train_images/891329021e12.png\n/kaggle/input/aptos2019/train_images/train_images/ae975c43bd8b.png\n/kaggle/input/aptos2019/train_images/train_images/4704dbb59536.png\n/kaggle/input/aptos2019/train_images/train_images/29f9e1ac9507.png\n/kaggle/input/aptos2019/train_images/train_images/76e5b50f95a7.png\n/kaggle/input/aptos2019/train_images/train_images/494fc9c745a3.png\n/kaggle/input/aptos2019/train_images/train_images/cb75210abebe.png\n/kaggle/input/aptos2019/train_images/train_images/d994203deb64.png\n/kaggle/input/aptos2019/train_images/train_images/97fdee242fea.png\n/kaggle/input/aptos2019/train_images/train_images/8ceff4c4c860.png\n/kaggle/input/aptos2019/train_images/train_images/2cfe8703f265.png\n/kaggle/input/aptos2019/train_images/train_images/9f5a8665cf2e.png\n/kaggle/input/aptos2019/train_images/train_images/e3a7671f787b.png\n/kaggle/input/aptos2019/train_images/train_images/6b128e648646.png\n/kaggle/input/aptos2019/train_images/train_images/ac1667fac512.png\n/kaggle/input/aptos2019/train_images/train_images/8e20b8fac7c3.png\n/kaggle/input/aptos2019/train_images/train_images/31360e44ac64.png\n/kaggle/input/aptos2019/train_images/train_images/86d58f850a0c.png\n/kaggle/input/aptos2019/train_images/train_images/5de4615a5161.png\n/kaggle/input/aptos2019/train_images/train_images/2fdfb80ea53c.png\n/kaggle/input/aptos2019/train_images/train_images/de730033c683.png\n/kaggle/input/aptos2019/train_images/train_images/5486da4273d7.png\n/kaggle/input/aptos2019/train_images/train_images/2665f72e2dd3.png\n/kaggle/input/aptos2019/train_images/train_images/81d79d53ed7b.png\n/kaggle/input/aptos2019/train_images/train_images/b2b7ccd34cbd.png\n/kaggle/input/aptos2019/train_images/train_images/8bf05909e1e1.png\n/kaggle/input/aptos2019/train_images/train_images/cbd0870aa933.png\n/kaggle/input/aptos2019/train_images/train_images/8b568d47a1fd.png\n/kaggle/input/aptos2019/train_images/train_images/39b5b05d6cd9.png\n/kaggle/input/aptos2019/train_images/train_images/b2ffa3e18559.png\n/kaggle/input/aptos2019/train_images/train_images/7ccb267fd394.png\n/kaggle/input/aptos2019/train_images/train_images/9033f1493da1.png\n/kaggle/input/aptos2019/train_images/train_images/a6c9e96a10d7.png\n/kaggle/input/aptos2019/train_images/train_images/dc0f6e5b489b.png\n/kaggle/input/aptos2019/train_images/train_images/9a326446c431.png\n/kaggle/input/aptos2019/train_images/train_images/2a47e5b21791.png\n/kaggle/input/aptos2019/train_images/train_images/7e9081e95bf6.png\n/kaggle/input/aptos2019/train_images/train_images/55968f0e63c4.png\n/kaggle/input/aptos2019/train_images/train_images/6735931000ec.png\n/kaggle/input/aptos2019/train_images/train_images/d9c9b9786da3.png\n/kaggle/input/aptos2019/train_images/train_images/a4ee03ecff60.png\n/kaggle/input/aptos2019/train_images/train_images/39f8935185e6.png\n/kaggle/input/aptos2019/train_images/train_images/96a9706b8534.png\n/kaggle/input/aptos2019/train_images/train_images/891392c9683c.png\n/kaggle/input/aptos2019/train_images/train_images/9ed666e982cd.png\n/kaggle/input/aptos2019/train_images/train_images/9ab18a4a957f.png\n/kaggle/input/aptos2019/train_images/train_images/a86c6283fd78.png\n/kaggle/input/aptos2019/train_images/train_images/c3d12a23f451.png\n/kaggle/input/aptos2019/train_images/train_images/69f43381317b.png\n/kaggle/input/aptos2019/train_images/train_images/82d364726a58.png\n/kaggle/input/aptos2019/train_images/train_images/c87493ed320c.png\n/kaggle/input/aptos2019/train_images/train_images/28a4d00927b7.png\n/kaggle/input/aptos2019/train_images/train_images/e38f3a65b02b.png\n/kaggle/input/aptos2019/train_images/train_images/81b0a2651c45.png\n/kaggle/input/aptos2019/train_images/train_images/d7e5fe5245e0.png\n/kaggle/input/aptos2019/train_images/train_images/2dc647e00ad3.png\n/kaggle/input/aptos2019/train_images/train_images/218c822a3dd9.png\n/kaggle/input/aptos2019/train_images/train_images/8ead17dfb6a6.png\n/kaggle/input/aptos2019/train_images/train_images/98441214557f.png\n/kaggle/input/aptos2019/train_images/train_images/ca7140ecf389.png\n/kaggle/input/aptos2019/train_images/train_images/b402b18d99a5.png\n/kaggle/input/aptos2019/train_images/train_images/57469423a012.png\n/kaggle/input/aptos2019/train_images/train_images/289a47dcbb82.png\n/kaggle/input/aptos2019/train_images/train_images/a8aed92940fb.png\n/kaggle/input/aptos2019/train_images/train_images/1df0431bfa73.png\n/kaggle/input/aptos2019/train_images/train_images/83d6e40c869f.png\n/kaggle/input/aptos2019/train_images/train_images/3f0d3629d69e.png\n/kaggle/input/aptos2019/train_images/train_images/2c8101f14723.png\n/kaggle/input/aptos2019/train_images/train_images/5a93c0f783c4.png\n/kaggle/input/aptos2019/train_images/train_images/842d697884f6.png\n/kaggle/input/aptos2019/train_images/train_images/359bab5d784b.png\n/kaggle/input/aptos2019/train_images/train_images/904b03ad5594.png\n/kaggle/input/aptos2019/train_images/train_images/b9519abce0c1.png\n/kaggle/input/aptos2019/train_images/train_images/53c874dbc594.png\n/kaggle/input/aptos2019/train_images/train_images/98277aeb96a7.png\n/kaggle/input/aptos2019/train_images/train_images/61ac9b0dc6b9.png\n/kaggle/input/aptos2019/train_images/train_images/92b0d27fc0ec.png\n/kaggle/input/aptos2019/train_images/train_images/36865bbc64d6.png\n/kaggle/input/aptos2019/train_images/train_images/bebb3f167654.png\n/kaggle/input/aptos2019/train_images/train_images/3dec415b188a.png\n/kaggle/input/aptos2019/train_images/train_images/9c6512166557.png\n/kaggle/input/aptos2019/train_images/train_images/b99afe7137fb.png\n/kaggle/input/aptos2019/train_images/train_images/a6d45de20e4d.png\n/kaggle/input/aptos2019/train_images/train_images/7f2cce721e19.png\n/kaggle/input/aptos2019/train_images/train_images/1b8ad0afe9fb.png\n/kaggle/input/aptos2019/train_images/train_images/dd110d2b8c21.png\n/kaggle/input/aptos2019/train_images/train_images/8aa3c4681542.png\n/kaggle/input/aptos2019/train_images/train_images/39923b29988a.png\n/kaggle/input/aptos2019/train_images/train_images/a77dbec966d4.png\n/kaggle/input/aptos2019/train_images/train_images/bebfbd907cac.png\n/kaggle/input/aptos2019/train_images/train_images/60f15dd68d30.png\n/kaggle/input/aptos2019/train_images/train_images/b0d6417bad3e.png\n/kaggle/input/aptos2019/train_images/train_images/2221cf5c7935.png\n/kaggle/input/aptos2019/train_images/train_images/a8b3c0961d42.png\n/kaggle/input/aptos2019/train_images/train_images/9039cbfcbb2f.png\n/kaggle/input/aptos2019/train_images/train_images/3b2b91590590.png\n/kaggle/input/aptos2019/train_images/train_images/abbb8791785e.png\n/kaggle/input/aptos2019/train_images/train_images/c85b79d70079.png\n/kaggle/input/aptos2019/train_images/train_images/780f9c237c56.png\n/kaggle/input/aptos2019/train_images/train_images/9b57e43b44e7.png\n/kaggle/input/aptos2019/train_images/train_images/a3802934bad7.png\n/kaggle/input/aptos2019/train_images/train_images/4276b82e4489.png\n/kaggle/input/aptos2019/train_images/train_images/57db4781e7ec.png\n/kaggle/input/aptos2019/train_images/train_images/9d75de31f1b8.png\n/kaggle/input/aptos2019/train_images/train_images/318eb706a134.png\n/kaggle/input/aptos2019/train_images/train_images/35beb47fe159.png\n/kaggle/input/aptos2019/train_images/train_images/5321ab64f9ea.png\n/kaggle/input/aptos2019/train_images/train_images/435414ccccf7.png\n/kaggle/input/aptos2019/train_images/train_images/50f5201fd18a.png\n/kaggle/input/aptos2019/train_images/train_images/79059d0592c4.png\n/kaggle/input/aptos2019/train_images/train_images/683023cda6a5.png\n/kaggle/input/aptos2019/train_images/train_images/1eee55494271.png\n/kaggle/input/aptos2019/train_images/train_images/b35cad8fe2d7.png\n/kaggle/input/aptos2019/train_images/train_images/5b3e7197ac1c.png\n/kaggle/input/aptos2019/train_images/train_images/5a11d21c2828.png\n/kaggle/input/aptos2019/train_images/train_images/4ab8c0cece7f.png\n/kaggle/input/aptos2019/train_images/train_images/6d3d1fe6c32a.png\n/kaggle/input/aptos2019/train_images/train_images/8dba09a4e5ed.png\n/kaggle/input/aptos2019/train_images/train_images/a14bbd9a583e.png\n/kaggle/input/aptos2019/train_images/train_images/2d558de2cabe.png\n/kaggle/input/aptos2019/train_images/train_images/2cf18033da31.png\n/kaggle/input/aptos2019/train_images/train_images/e387311a840e.png\n/kaggle/input/aptos2019/train_images/train_images/7179f85bfd6f.png\n/kaggle/input/aptos2019/train_images/train_images/4eabad7948cf.png\n/kaggle/input/aptos2019/train_images/train_images/1e036f2e7095.png\n/kaggle/input/aptos2019/train_images/train_images/dd90c321d7bc.png\n/kaggle/input/aptos2019/train_images/train_images/5728b8aa98ef.png\n/kaggle/input/aptos2019/train_images/train_images/c97472ef2c66.png\n/kaggle/input/aptos2019/train_images/train_images/40c24aded50c.png\n/kaggle/input/aptos2019/train_images/train_images/7a06ea127e02.png\n/kaggle/input/aptos2019/train_images/train_images/d97911a32918.png\n/kaggle/input/aptos2019/train_images/train_images/8564b7aa3c1a.png\n/kaggle/input/aptos2019/train_images/train_images/3d3e288d490e.png\n/kaggle/input/aptos2019/train_images/train_images/49a4765f8822.png\n/kaggle/input/aptos2019/train_images/train_images/d74ccc796517.png\n/kaggle/input/aptos2019/train_images/train_images/3e3a3955b9c5.png\n/kaggle/input/aptos2019/train_images/train_images/3f6bccf21ce8.png\n/kaggle/input/aptos2019/train_images/train_images/4ec7796df40e.png\n/kaggle/input/aptos2019/train_images/train_images/d1cad012a254.png\n/kaggle/input/aptos2019/train_images/train_images/9be71d6d7e59.png\n/kaggle/input/aptos2019/train_images/train_images/1bf30c84bbad.png\n/kaggle/input/aptos2019/train_images/train_images/4c389d033cb0.png\n/kaggle/input/aptos2019/train_images/train_images/c3a82acb7d7a.png\n/kaggle/input/aptos2019/train_images/train_images/a62ea0043aa7.png\n/kaggle/input/aptos2019/train_images/train_images/5f70ad48a525.png\n/kaggle/input/aptos2019/train_images/train_images/415d5c5e785f.png\n/kaggle/input/aptos2019/train_images/train_images/2f5c9cdfb333.png\n/kaggle/input/aptos2019/train_images/train_images/81914ceb4e74.png\n/kaggle/input/aptos2019/train_images/train_images/306c841af3fc.png\n/kaggle/input/aptos2019/train_images/train_images/7e77b61e1639.png\n/kaggle/input/aptos2019/train_images/train_images/848e66b9e199.png\n/kaggle/input/aptos2019/train_images/train_images/8fbb2ca39911.png\n/kaggle/input/aptos2019/train_images/train_images/45c39ab9e797.png\n/kaggle/input/aptos2019/train_images/train_images/999115d9386b.png\n/kaggle/input/aptos2019/train_images/train_images/bdf47b9f10c4.png\n/kaggle/input/aptos2019/train_images/train_images/511fd66b2df8.png\n/kaggle/input/aptos2019/train_images/train_images/69df7ade0575.png\n/kaggle/input/aptos2019/train_images/train_images/5f6db235c04d.png\n/kaggle/input/aptos2019/train_images/train_images/cd8da43e3069.png\n/kaggle/input/aptos2019/train_images/train_images/840527bc6628.png\n/kaggle/input/aptos2019/train_images/train_images/66a0bf258013.png\n/kaggle/input/aptos2019/train_images/train_images/a1e236fbc863.png\n/kaggle/input/aptos2019/train_images/train_images/94111ed3d276.png\n/kaggle/input/aptos2019/train_images/train_images/b8297a2291f5.png\n/kaggle/input/aptos2019/train_images/train_images/ad1f7445b1a8.png\n/kaggle/input/aptos2019/train_images/train_images/aa0afc41ed19.png\n/kaggle/input/aptos2019/train_images/train_images/2d07162a13b1.png\n/kaggle/input/aptos2019/train_images/train_images/6733544ae7a6.png\n/kaggle/input/aptos2019/train_images/train_images/a0e635689259.png\n/kaggle/input/aptos2019/train_images/train_images/e150935f66a6.png\n/kaggle/input/aptos2019/train_images/train_images/ba735b286d62.png\n/kaggle/input/aptos2019/train_images/train_images/55fd453001cc.png\n/kaggle/input/aptos2019/train_images/train_images/875d2ffcbf47.png\n/kaggle/input/aptos2019/train_images/train_images/aa9cfe639ef1.png\n/kaggle/input/aptos2019/train_images/train_images/3b018e8b7303.png\n/kaggle/input/aptos2019/train_images/train_images/5eb8fb1aad41.png\n/kaggle/input/aptos2019/train_images/train_images/d25b8a8ad3c4.png\n/kaggle/input/aptos2019/train_images/train_images/ae5d31979f19.png\n/kaggle/input/aptos2019/train_images/train_images/405085b53d7b.png\n/kaggle/input/aptos2019/train_images/train_images/e23add229074.png\n/kaggle/input/aptos2019/train_images/train_images/527bea76116c.png\n/kaggle/input/aptos2019/train_images/train_images/94dcb491143f.png\n/kaggle/input/aptos2019/train_images/train_images/e037643244b7.png\n/kaggle/input/aptos2019/train_images/train_images/687759336b0d.png\n/kaggle/input/aptos2019/train_images/train_images/b50b30aa6e6c.png\n/kaggle/input/aptos2019/train_images/train_images/51d780864365.png\n/kaggle/input/aptos2019/train_images/train_images/63d217b059b6.png\n/kaggle/input/aptos2019/train_images/train_images/1ee1eb7943db.png\n/kaggle/input/aptos2019/train_images/train_images/48543037d0b3.png\n/kaggle/input/aptos2019/train_images/train_images/a11c62cb3f86.png\n/kaggle/input/aptos2019/train_images/train_images/df5ce3ea7820.png\n/kaggle/input/aptos2019/train_images/train_images/24b943fe725e.png\n/kaggle/input/aptos2019/train_images/train_images/4beeca5cc859.png\n/kaggle/input/aptos2019/train_images/train_images/26453eb7e989.png\n/kaggle/input/aptos2019/train_images/train_images/3f98be586fe3.png\n/kaggle/input/aptos2019/train_images/train_images/5299a532f0e0.png\n/kaggle/input/aptos2019/train_images/train_images/201f882365d3.png\n/kaggle/input/aptos2019/train_images/train_images/c21eb81de9fc.png\n/kaggle/input/aptos2019/train_images/train_images/d264396d8d1a.png\n/kaggle/input/aptos2019/train_images/train_images/a1872f9c0cba.png\n/kaggle/input/aptos2019/train_images/train_images/8ab3faa3701f.png\n/kaggle/input/aptos2019/train_images/train_images/910bfd38e2f5.png\n/kaggle/input/aptos2019/train_images/train_images/83df53d58f28.png\n/kaggle/input/aptos2019/train_images/train_images/404ede327e98.png\n/kaggle/input/aptos2019/train_images/train_images/780f9daaa24b.png\n/kaggle/input/aptos2019/train_images/train_images/cb2f3c5d71a7.png\n/kaggle/input/aptos2019/train_images/train_images/1b32e1d775ea.png\n/kaggle/input/aptos2019/train_images/train_images/857230f64a2e.png\n/kaggle/input/aptos2019/train_images/train_images/a96ff96bbae5.png\n/kaggle/input/aptos2019/train_images/train_images/2a93334f663a.png\n/kaggle/input/aptos2019/train_images/train_images/4faf4063db8c.png\n/kaggle/input/aptos2019/train_images/train_images/c4a8f2fcf6e8.png\n/kaggle/input/aptos2019/train_images/train_images/5152bf091152.png\n/kaggle/input/aptos2019/train_images/train_images/94145d1f42cf.png\n/kaggle/input/aptos2019/train_images/train_images/6d5a8362dd1e.png\n/kaggle/input/aptos2019/train_images/train_images/29580bed2f7d.png\n/kaggle/input/aptos2019/train_images/train_images/5723d0ec895e.png\n/kaggle/input/aptos2019/train_images/train_images/702de9dcde32.png\n/kaggle/input/aptos2019/train_images/train_images/aa4407aab872.png\n/kaggle/input/aptos2019/train_images/train_images/295fdc964f6e.png\n/kaggle/input/aptos2019/train_images/train_images/8cb6b0efaaac.png\n/kaggle/input/aptos2019/train_images/train_images/79d44db3da2d.png\n/kaggle/input/aptos2019/train_images/train_images/c406325360b1.png\n/kaggle/input/aptos2019/train_images/train_images/4cae247d9909.png\n/kaggle/input/aptos2019/train_images/train_images/4294a14c656a.png\n/kaggle/input/aptos2019/train_images/train_images/240b25a7debe.png\n/kaggle/input/aptos2019/train_images/train_images/7635921c5efb.png\n/kaggle/input/aptos2019/train_images/train_images/5d74f98d62be.png\n/kaggle/input/aptos2019/train_images/train_images/3b73a3a4a734.png\n/kaggle/input/aptos2019/train_images/train_images/6d6fcf49e515.png\n/kaggle/input/aptos2019/train_images/train_images/7d261f986bef.png\n/kaggle/input/aptos2019/train_images/train_images/b3f31c371e59.png\n/kaggle/input/aptos2019/train_images/train_images/1c13a1483f4a.png\n/kaggle/input/aptos2019/train_images/train_images/cd9e2190c73f.png\n/kaggle/input/aptos2019/train_images/train_images/59928f999ae7.png\n/kaggle/input/aptos2019/train_images/train_images/88e5051f65bd.png\n/kaggle/input/aptos2019/train_images/train_images/5d5b5da5f939.png\n/kaggle/input/aptos2019/train_images/train_images/9ac2e3e9fca5.png\n/kaggle/input/aptos2019/train_images/train_images/7ef5ff774a48.png\n/kaggle/input/aptos2019/train_images/train_images/2463bb04ebc3.png\n/kaggle/input/aptos2019/train_images/train_images/99c6a123ed6a.png\n/kaggle/input/aptos2019/train_images/train_images/48fda42bd5d4.png\n/kaggle/input/aptos2019/train_images/train_images/d2dc86021c67.png\n/kaggle/input/aptos2019/train_images/train_images/acf976efd7ce.png\n/kaggle/input/aptos2019/train_images/train_images/a476fd984005.png\n/kaggle/input/aptos2019/train_images/train_images/d881c04f01fe.png\n/kaggle/input/aptos2019/train_images/train_images/86410aa13b3e.png\n/kaggle/input/aptos2019/train_images/train_images/5b32ece9c627.png\n/kaggle/input/aptos2019/train_images/train_images/2585bbc91909.png\n/kaggle/input/aptos2019/train_images/train_images/c1e6fa1ad314.png\n/kaggle/input/aptos2019/train_images/train_images/cd01f4f83336.png\n/kaggle/input/aptos2019/train_images/train_images/60edda7b4871.png\n/kaggle/input/aptos2019/train_images/train_images/3232b34cbe99.png\n/kaggle/input/aptos2019/train_images/train_images/bda91b76095b.png\n/kaggle/input/aptos2019/train_images/train_images/8f10e41a2f02.png\n/kaggle/input/aptos2019/train_images/train_images/e2265c383348.png\n/kaggle/input/aptos2019/train_images/train_images/6f923b60934b.png\n/kaggle/input/aptos2019/train_images/train_images/7a46cfa69bae.png\n/kaggle/input/aptos2019/train_images/train_images/64c6c6ee0d98.png\n/kaggle/input/aptos2019/train_images/train_images/dccdf750c962.png\n/kaggle/input/aptos2019/train_images/train_images/66366a90d1ef.png\n/kaggle/input/aptos2019/train_images/train_images/ca30a97e9d13.png\n/kaggle/input/aptos2019/train_images/train_images/4a0bba3b7d83.png\n/kaggle/input/aptos2019/train_images/train_images/ac81fc200162.png\n/kaggle/input/aptos2019/train_images/train_images/269f0792f11f.png\n/kaggle/input/aptos2019/train_images/train_images/3e86335bc2fd.png\n/kaggle/input/aptos2019/train_images/train_images/af345c68e836.png\n/kaggle/input/aptos2019/train_images/train_images/bf9cba745efc.png\n/kaggle/input/aptos2019/train_images/train_images/682312e82ee3.png\n/kaggle/input/aptos2019/train_images/train_images/b0f0fa677d5f.png\n/kaggle/input/aptos2019/train_images/train_images/6cee2e148520.png\n/kaggle/input/aptos2019/train_images/train_images/b11dcdcbc8c8.png\n/kaggle/input/aptos2019/train_images/train_images/da2bdf4236ac.png\n/kaggle/input/aptos2019/train_images/train_images/8d8aca52c07b.png\n/kaggle/input/aptos2019/train_images/train_images/bb752b179751.png\n/kaggle/input/aptos2019/train_images/train_images/c06024f05a16.png\n/kaggle/input/aptos2019/train_images/train_images/76fe19ff64fb.png\n/kaggle/input/aptos2019/train_images/train_images/bab776139279.png\n/kaggle/input/aptos2019/train_images/train_images/7fc3a8bb40de.png\n/kaggle/input/aptos2019/train_images/train_images/c01eae4b4939.png\n/kaggle/input/aptos2019/train_images/train_images/d0926ed2c8e5.png\n/kaggle/input/aptos2019/train_images/train_images/4df6a81b476e.png\n/kaggle/input/aptos2019/train_images/train_images/e3ab63dc9a60.png\n/kaggle/input/aptos2019/train_images/train_images/b5e6ae31493c.png\n/kaggle/input/aptos2019/train_images/train_images/e19936582c61.png\n/kaggle/input/aptos2019/train_images/train_images/9f1efb799b7b.png\n/kaggle/input/aptos2019/train_images/train_images/b5834ee64541.png\n/kaggle/input/aptos2019/train_images/train_images/8ae049175db6.png\n/kaggle/input/aptos2019/train_images/train_images/c18a006f7f1d.png\n/kaggle/input/aptos2019/train_images/train_images/da1fb35f5df9.png\n/kaggle/input/aptos2019/train_images/train_images/cad5b1a82e60.png\n/kaggle/input/aptos2019/train_images/train_images/96c3e3db68bc.png\n/kaggle/input/aptos2019/train_images/train_images/54bbe3da103e.png\n/kaggle/input/aptos2019/train_images/train_images/2bb063318cf1.png\n/kaggle/input/aptos2019/train_images/train_images/5e7630f8438e.png\n/kaggle/input/aptos2019/train_images/train_images/3486f7096276.png\n/kaggle/input/aptos2019/train_images/train_images/b99c825b93c5.png\n/kaggle/input/aptos2019/train_images/train_images/8e76054f0831.png\n/kaggle/input/aptos2019/train_images/train_images/a4012932e18d.png\n/kaggle/input/aptos2019/train_images/train_images/8dc22e65c06f.png\n/kaggle/input/aptos2019/train_images/train_images/7b87b0015282.png\n/kaggle/input/aptos2019/train_images/train_images/510aa0a898fa.png\n/kaggle/input/aptos2019/train_images/train_images/46f56c38051f.png\n/kaggle/input/aptos2019/train_images/train_images/8bc6716c2238.png\n/kaggle/input/aptos2019/train_images/train_images/80e6e425f966.png\n/kaggle/input/aptos2019/train_images/train_images/b665041e1633.png\n/kaggle/input/aptos2019/train_images/train_images/e17507a4a1f5.png\n/kaggle/input/aptos2019/train_images/train_images/917f76f360b6.png\n/kaggle/input/aptos2019/train_images/train_images/1faf8664816c.png\n/kaggle/input/aptos2019/train_images/train_images/7d11dbc1e738.png\n/kaggle/input/aptos2019/train_images/train_images/7ad0c4975890.png\n/kaggle/input/aptos2019/train_images/train_images/db6207e62c7b.png\n/kaggle/input/aptos2019/train_images/train_images/7e980424868e.png\n/kaggle/input/aptos2019/train_images/train_images/9a4f370d341b.png\n/kaggle/input/aptos2019/train_images/train_images/a95858e052d6.png\n/kaggle/input/aptos2019/train_images/train_images/8ac0c44bbf24.png\n/kaggle/input/aptos2019/train_images/train_images/a14fcf84bfe1.png\n/kaggle/input/aptos2019/train_images/train_images/96d48b073f18.png\n/kaggle/input/aptos2019/train_images/train_images/6e1db8711879.png\n/kaggle/input/aptos2019/train_images/train_images/8c7c26c52a6c.png\n/kaggle/input/aptos2019/train_images/train_images/5eb311bcb5f9.png\n/kaggle/input/aptos2019/train_images/train_images/658ad9f09f5d.png\n/kaggle/input/aptos2019/train_images/train_images/b8e9a8f4617d.png\n/kaggle/input/aptos2019/train_images/train_images/2399d68d407f.png\n/kaggle/input/aptos2019/train_images/train_images/72595230840c.png\n/kaggle/input/aptos2019/train_images/train_images/28f98cfe3858.png\n/kaggle/input/aptos2019/train_images/train_images/4fa26d065ad3.png\n/kaggle/input/aptos2019/train_images/train_images/7c2f820a6425.png\n/kaggle/input/aptos2019/train_images/train_images/c81c6911f5e0.png\n/kaggle/input/aptos2019/train_images/train_images/45693d027798.png\n/kaggle/input/aptos2019/train_images/train_images/d271d3a2b552.png\n/kaggle/input/aptos2019/train_images/train_images/500aad15b7c8.png\n/kaggle/input/aptos2019/train_images/train_images/d1f7ea924a01.png\n/kaggle/input/aptos2019/train_images/train_images/ce754234d760.png\n/kaggle/input/aptos2019/train_images/train_images/711d1480d2e3.png\n/kaggle/input/aptos2019/train_images/train_images/50a2aef380c8.png\n/kaggle/input/aptos2019/train_images/train_images/5b994ff78547.png\n/kaggle/input/aptos2019/train_images/train_images/2628305cbb29.png\n/kaggle/input/aptos2019/train_images/train_images/9f37c98b8187.png\n/kaggle/input/aptos2019/train_images/train_images/b66f23ffa730.png\n/kaggle/input/aptos2019/train_images/train_images/7e5a76c4e103.png\n/kaggle/input/aptos2019/train_images/train_images/9a3c03a5ad0f.png\n/kaggle/input/aptos2019/train_images/train_images/571bbdbf585e.png\n/kaggle/input/aptos2019/train_images/train_images/93be637084a2.png\n/kaggle/input/aptos2019/train_images/train_images/4b1001050f1d.png\n/kaggle/input/aptos2019/train_images/train_images/e0313be77035.png\n/kaggle/input/aptos2019/train_images/train_images/475c7ded0f7a.png\n/kaggle/input/aptos2019/train_images/train_images/65e530ee2e79.png\n/kaggle/input/aptos2019/train_images/train_images/d3dfd0a2dee6.png\n/kaggle/input/aptos2019/train_images/train_images/94ef1d14597f.png\n/kaggle/input/aptos2019/train_images/train_images/668a319c2d23.png\n/kaggle/input/aptos2019/train_images/train_images/4ad6109706e8.png\n/kaggle/input/aptos2019/train_images/train_images/cbf0394039f8.png\n/kaggle/input/aptos2019/train_images/train_images/9878db94d9f3.png\n/kaggle/input/aptos2019/train_images/train_images/a15470303941.png\n/kaggle/input/aptos2019/train_images/train_images/785777558f05.png\n/kaggle/input/aptos2019/train_images/train_images/555d0bef3c5b.png\n/kaggle/input/aptos2019/train_images/train_images/436e1793d240.png\n/kaggle/input/aptos2019/train_images/train_images/ad1aa75d5630.png\n/kaggle/input/aptos2019/train_images/train_images/b310bd564329.png\n/kaggle/input/aptos2019/train_images/train_images/c7c3d363bc86.png\n/kaggle/input/aptos2019/train_images/train_images/4b5ffea77373.png\n/kaggle/input/aptos2019/train_images/train_images/53f6c1c65c04.png\n/kaggle/input/aptos2019/train_images/train_images/6d4f6c9a8406.png\n/kaggle/input/aptos2019/train_images/train_images/7fdb177b8f7d.png\n/kaggle/input/aptos2019/train_images/train_images/8d4ff745a409.png\n/kaggle/input/aptos2019/train_images/train_images/4da6e2089d57.png\n/kaggle/input/aptos2019/train_images/train_images/7356dd08b0ae.png\n/kaggle/input/aptos2019/train_images/train_images/bc23f74e14dd.png\n/kaggle/input/aptos2019/train_images/train_images/7c3747c0b2c3.png\n/kaggle/input/aptos2019/train_images/train_images/a47432cd41e7.png\n/kaggle/input/aptos2019/train_images/train_images/d51e5d7484ea.png\n/kaggle/input/aptos2019/train_images/train_images/810ed108f5b7.png\n/kaggle/input/aptos2019/train_images/train_images/76df141d966b.png\n/kaggle/input/aptos2019/train_images/train_images/6c4ec95dd8ba.png\n/kaggle/input/aptos2019/train_images/train_images/b6fd109b1bc9.png\n/kaggle/input/aptos2019/train_images/train_images/b019a49787c1.png\n/kaggle/input/aptos2019/train_images/train_images/67c03349bb31.png\n/kaggle/input/aptos2019/train_images/train_images/aa5ce75edcf5.png\n/kaggle/input/aptos2019/train_images/train_images/461fa5292fda.png\n/kaggle/input/aptos2019/train_images/train_images/43823561c3f0.png\n/kaggle/input/aptos2019/train_images/train_images/9b0eb9f41da4.png\n/kaggle/input/aptos2019/train_images/train_images/9a496b1e20f9.png\n/kaggle/input/aptos2019/train_images/train_images/3fd7df6099e3.png\n/kaggle/input/aptos2019/train_images/train_images/3a4cfea0a766.png\n/kaggle/input/aptos2019/train_images/train_images/80ed04a84a16.png\n/kaggle/input/aptos2019/train_images/train_images/6028a575dc27.png\n/kaggle/input/aptos2019/train_images/train_images/65e51e18242b.png\n/kaggle/input/aptos2019/train_images/train_images/1b4625877527.png\n/kaggle/input/aptos2019/train_images/train_images/a6b6d27c1b32.png\n/kaggle/input/aptos2019/train_images/train_images/39134907127a.png\n/kaggle/input/aptos2019/train_images/train_images/e03a74e7d74f.png\n/kaggle/input/aptos2019/train_images/train_images/ad944bd56bb6.png\n/kaggle/input/aptos2019/train_images/train_images/959dc602febc.png\n/kaggle/input/aptos2019/train_images/train_images/a386ec9aabde.png\n/kaggle/input/aptos2019/train_images/train_images/e03e70bc8bba.png\n/kaggle/input/aptos2019/train_images/train_images/720b5f62ce80.png\n/kaggle/input/aptos2019/train_images/train_images/3ffa14d60b24.png\n/kaggle/input/aptos2019/train_images/train_images/523ff163211b.png\n/kaggle/input/aptos2019/train_images/train_images/3a1ecf5e2839.png\n/kaggle/input/aptos2019/train_images/train_images/8273fdb4405e.png\n/kaggle/input/aptos2019/train_images/train_images/7877be80901c.png\n/kaggle/input/aptos2019/train_images/train_images/7ee6de71c140.png\n/kaggle/input/aptos2019/train_images/train_images/af828dab3ffc.png\n/kaggle/input/aptos2019/train_images/train_images/c48ae5da188e.png\n/kaggle/input/aptos2019/train_images/train_images/9bf060db8376.png\n/kaggle/input/aptos2019/train_images/train_images/5712e2aa73a2.png\n/kaggle/input/aptos2019/train_images/train_images/8d4d14a4ab07.png\n/kaggle/input/aptos2019/train_images/train_images/7b20210d9120.png\n/kaggle/input/aptos2019/train_images/train_images/b82dfa63a75f.png\n/kaggle/input/aptos2019/train_images/train_images/e06d3d4733f0.png\n/kaggle/input/aptos2019/train_images/train_images/387138ddf43d.png\n/kaggle/input/aptos2019/train_images/train_images/b1f4122fd36a.png\n/kaggle/input/aptos2019/train_images/train_images/956765d5f46d.png\n/kaggle/input/aptos2019/train_images/train_images/5a03fe3ed15c.png\n/kaggle/input/aptos2019/train_images/train_images/d160ebef4117.png\n/kaggle/input/aptos2019/train_images/train_images/789c60cba801.png\n/kaggle/input/aptos2019/train_images/train_images/d16e59a2b33a.png\n/kaggle/input/aptos2019/train_images/train_images/5b0e53f53ef3.png\n/kaggle/input/aptos2019/train_images/train_images/67f5d89da548.png\n/kaggle/input/aptos2019/train_images/train_images/a2ddabee14e9.png\n/kaggle/input/aptos2019/train_images/train_images/30cab14951ac.png\n/kaggle/input/aptos2019/train_images/train_images/6d6be4cfc73f.png\n/kaggle/input/aptos2019/train_images/train_images/72d98188648f.png\n/kaggle/input/aptos2019/train_images/train_images/259d30f693b6.png\n/kaggle/input/aptos2019/train_images/train_images/4036471a1bb7.png\n/kaggle/input/aptos2019/train_images/train_images/9ae54843c69a.png\n/kaggle/input/aptos2019/train_images/train_images/83a63c4a3e4a.png\n/kaggle/input/aptos2019/train_images/train_images/25dc1b41ed9c.png\n/kaggle/input/aptos2019/train_images/train_images/3c53198519f7.png\n/kaggle/input/aptos2019/train_images/train_images/38e0e28d35d3.png\n/kaggle/input/aptos2019/train_images/train_images/da6bbb76d562.png\n/kaggle/input/aptos2019/train_images/train_images/2d552318eb07.png\n/kaggle/input/aptos2019/train_images/train_images/61bbc11fe503.png\n/kaggle/input/aptos2019/train_images/train_images/1d2472849dce.png\n/kaggle/input/aptos2019/train_images/train_images/90a9a41eec6d.png\n/kaggle/input/aptos2019/train_images/train_images/5e7cc6ab4ac4.png\n/kaggle/input/aptos2019/train_images/train_images/b6bfe9db60e5.png\n/kaggle/input/aptos2019/train_images/train_images/8ab8d9b3ce3f.png\n/kaggle/input/aptos2019/train_images/train_images/2c77bf969079.png\n/kaggle/input/aptos2019/train_images/train_images/b71428739d4e.png\n/kaggle/input/aptos2019/train_images/train_images/3178559fbf57.png\n/kaggle/input/aptos2019/train_images/train_images/87295c5fa1cc.png\n/kaggle/input/aptos2019/train_images/train_images/b3d135bd3bb5.png\n/kaggle/input/aptos2019/train_images/train_images/441848e0f308.png\n/kaggle/input/aptos2019/train_images/train_images/e2856afe62c5.png\n/kaggle/input/aptos2019/train_images/train_images/331121c65e88.png\n/kaggle/input/aptos2019/train_images/train_images/a88365134c3c.png\n/kaggle/input/aptos2019/train_images/train_images/af133a85ea0c.png\n/kaggle/input/aptos2019/train_images/train_images/acc9f29538c4.png\n/kaggle/input/aptos2019/train_images/train_images/4b618537d52f.png\n/kaggle/input/aptos2019/train_images/train_images/4b237b958555.png\n/kaggle/input/aptos2019/train_images/train_images/67844c46bc61.png\n/kaggle/input/aptos2019/train_images/train_images/a00b4cb250a7.png\n/kaggle/input/aptos2019/train_images/train_images/b8dab47a260e.png\n/kaggle/input/aptos2019/train_images/train_images/b3c0c3330278.png\n/kaggle/input/aptos2019/train_images/train_images/ab3c505b624f.png\n/kaggle/input/aptos2019/train_images/train_images/da0a1043abf7.png\n/kaggle/input/aptos2019/train_images/train_images/7831ce1d895e.png\n/kaggle/input/aptos2019/train_images/train_images/addf66a50f42.png\n/kaggle/input/aptos2019/train_images/train_images/b7f0bc7d399e.png\n/kaggle/input/aptos2019/train_images/train_images/7b9d519cbd66.png\n/kaggle/input/aptos2019/train_images/train_images/70d0392397de.png\n/kaggle/input/aptos2019/train_images/train_images/a47878630dc2.png\n/kaggle/input/aptos2019/train_images/train_images/936299166bea.png\n/kaggle/input/aptos2019/train_images/train_images/6b91e99c9408.png\n/kaggle/input/aptos2019/train_images/train_images/4a0890b08532.png\n/kaggle/input/aptos2019/train_images/train_images/780be525036d.png\n/kaggle/input/aptos2019/train_images/train_images/b7e0f95353f2.png\n/kaggle/input/aptos2019/train_images/train_images/d2cd47ed2c1d.png\n/kaggle/input/aptos2019/train_images/train_images/6efa36d59ada.png\n/kaggle/input/aptos2019/train_images/train_images/a2d349f567a6.png\n/kaggle/input/aptos2019/train_images/train_images/de4cdabbce6d.png\n/kaggle/input/aptos2019/train_images/train_images/929cd3867815.png\n/kaggle/input/aptos2019/train_images/train_images/98f7136d2e7a.png\n/kaggle/input/aptos2019/train_images/train_images/df4913ca3712.png\n/kaggle/input/aptos2019/train_images/train_images/668e853258cd.png\n/kaggle/input/aptos2019/train_images/train_images/dc6fa1b38b83.png\n/kaggle/input/aptos2019/train_images/train_images/cca626a0e19a.png\n/kaggle/input/aptos2019/train_images/train_images/6df8b7b6e837.png\n/kaggle/input/aptos2019/train_images/train_images/3128eb593012.png\n/kaggle/input/aptos2019/train_images/train_images/e42d9a94a66d.png\n/kaggle/input/aptos2019/train_images/train_images/29bc0e721cfe.png\n/kaggle/input/aptos2019/train_images/train_images/b77b88926843.png\n/kaggle/input/aptos2019/train_images/train_images/a790a3b36390.png\n/kaggle/input/aptos2019/train_images/train_images/d85a842d20bd.png\n/kaggle/input/aptos2019/train_images/train_images/b8f1b30877db.png\n/kaggle/input/aptos2019/train_images/train_images/486e852a3b4d.png\n/kaggle/input/aptos2019/train_images/train_images/dfc7ec7db0e0.png\n/kaggle/input/aptos2019/train_images/train_images/ac0a48ccbf70.png\n/kaggle/input/aptos2019/train_images/train_images/4210809074c1.png\n/kaggle/input/aptos2019/train_images/train_images/4cddfc22b0ad.png\n/kaggle/input/aptos2019/train_images/train_images/76cfe8967f7d.png\n/kaggle/input/aptos2019/train_images/train_images/1da4a17c18c9.png\n/kaggle/input/aptos2019/train_images/train_images/e2c3b037413b.png\n/kaggle/input/aptos2019/train_images/train_images/aec51513cf45.png\n/kaggle/input/aptos2019/train_images/train_images/92889b863ae6.png\n/kaggle/input/aptos2019/train_images/train_images/dde43aa22ae6.png\n/kaggle/input/aptos2019/train_images/train_images/c1437a7a52c9.png\n/kaggle/input/aptos2019/train_images/train_images/dee31065f8fe.png\n/kaggle/input/aptos2019/train_images/train_images/5293576816aa.png\n/kaggle/input/aptos2019/train_images/train_images/a3132c8828e4.png\n/kaggle/input/aptos2019/train_images/train_images/c613db1cab27.png\n/kaggle/input/aptos2019/train_images/train_images/4dd7b322f342.png\n/kaggle/input/aptos2019/train_images/train_images/ac5b5dddf91b.png\n/kaggle/input/aptos2019/train_images/train_images/3bf2deaa5ef0.png\n/kaggle/input/aptos2019/train_images/train_images/35ac70c0d08f.png\n/kaggle/input/aptos2019/train_images/train_images/4478b870e549.png\n/kaggle/input/aptos2019/train_images/train_images/9ce46d400cd6.png\n/kaggle/input/aptos2019/train_images/train_images/8c4ceddeb1c6.png\n/kaggle/input/aptos2019/train_images/train_images/be197b663520.png\n/kaggle/input/aptos2019/train_images/train_images/ae1344610ebe.png\n/kaggle/input/aptos2019/train_images/train_images/bdff5d8bddf8.png\n/kaggle/input/aptos2019/train_images/train_images/941d874c8afb.png\n/kaggle/input/aptos2019/train_images/train_images/3cd9713c0ecb.png\n/kaggle/input/aptos2019/train_images/train_images/677f087cd697.png\n/kaggle/input/aptos2019/train_images/train_images/81704925f759.png\n/kaggle/input/aptos2019/train_images/train_images/a93f1ea3ff4a.png\n/kaggle/input/aptos2019/train_images/train_images/1c5ad36fb799.png\n/kaggle/input/aptos2019/train_images/train_images/c0968d41eb93.png\n/kaggle/input/aptos2019/train_images/train_images/a94da3d3b5c0.png\n/kaggle/input/aptos2019/train_images/train_images/8693ab1fd2be.png\n/kaggle/input/aptos2019/train_images/train_images/51cfeccaf40d.png\n/kaggle/input/aptos2019/train_images/train_images/d6803e467592.png\n/kaggle/input/aptos2019/train_images/train_images/e32dc722eca5.png\n/kaggle/input/aptos2019/train_images/train_images/8c29a76fa08c.png\n/kaggle/input/aptos2019/train_images/train_images/a9d0c900b6a9.png\n/kaggle/input/aptos2019/train_images/train_images/53ddae6a619e.png\n/kaggle/input/aptos2019/train_images/train_images/6107a2e9f60e.png\n/kaggle/input/aptos2019/train_images/train_images/d6b109c82067.png\n/kaggle/input/aptos2019/train_images/train_images/5b3d41626ec5.png\n/kaggle/input/aptos2019/train_images/train_images/92587e494d51.png\n/kaggle/input/aptos2019/train_images/train_images/8650d32f4a9e.png\n/kaggle/input/aptos2019/train_images/train_images/217dad18a5ed.png\n/kaggle/input/aptos2019/train_images/train_images/3ac3fbfca7d4.png\n/kaggle/input/aptos2019/train_images/train_images/dec5595e6154.png\n/kaggle/input/aptos2019/train_images/train_images/d5c63a8d9e94.png\n/kaggle/input/aptos2019/train_images/train_images/838b3e4d0bb4.png\n/kaggle/input/aptos2019/train_images/train_images/ad12cde115ab.png\n/kaggle/input/aptos2019/train_images/train_images/3cdef7c591cc.png\n/kaggle/input/aptos2019/train_images/train_images/4c52922f3bfd.png\n/kaggle/input/aptos2019/train_images/train_images/5264a54e1830.png\n/kaggle/input/aptos2019/train_images/train_images/653534ded339.png\n/kaggle/input/aptos2019/train_images/train_images/3f47f83217b5.png\n/kaggle/input/aptos2019/train_images/train_images/4661006f3ba6.png\n/kaggle/input/aptos2019/train_images/train_images/1b495ac025b7.png\n/kaggle/input/aptos2019/train_images/train_images/bf7221a016b5.png\n/kaggle/input/aptos2019/train_images/train_images/ad2f0b9d059c.png\n/kaggle/input/aptos2019/train_images/train_images/6cdd0f985270.png\n/kaggle/input/aptos2019/train_images/train_images/cd29c88c9e36.png\n/kaggle/input/aptos2019/train_images/train_images/b9c7c5182075.png\n/kaggle/input/aptos2019/train_images/train_images/bfdee9be1f1d.png\n/kaggle/input/aptos2019/train_images/train_images/9858cc2ae073.png\n/kaggle/input/aptos2019/train_images/train_images/bb45257258cc.png\n/kaggle/input/aptos2019/train_images/train_images/bf18ff30a8f6.png\n/kaggle/input/aptos2019/train_images/train_images/281d7b7c7676.png\n/kaggle/input/aptos2019/train_images/train_images/62e6f814c8f5.png\n/kaggle/input/aptos2019/train_images/train_images/de2eb5c8aa83.png\n/kaggle/input/aptos2019/train_images/train_images/c6e1e9fbf39b.png\n/kaggle/input/aptos2019/train_images/train_images/7c6594b50690.png\n/kaggle/input/aptos2019/train_images/train_images/93421787f520.png\n/kaggle/input/aptos2019/train_images/train_images/5a179c123fd8.png\n/kaggle/input/aptos2019/train_images/train_images/6baafa56895c.png\n/kaggle/input/aptos2019/train_images/train_images/1e742358e0b9.png\n/kaggle/input/aptos2019/train_images/train_images/7a6495a39d87.png\n/kaggle/input/aptos2019/train_images/train_images/9e99ae6ee7af.png\n/kaggle/input/aptos2019/train_images/train_images/d6dbb0820ea5.png\n/kaggle/input/aptos2019/train_images/train_images/8fc09fecd22f.png\n/kaggle/input/aptos2019/train_images/train_images/4e43d05cc2ef.png\n/kaggle/input/aptos2019/train_images/train_images/5cc6dea19614.png\n/kaggle/input/aptos2019/train_images/train_images/c3aa424eff9a.png\n/kaggle/input/aptos2019/train_images/train_images/bc8c6a778cde.png\n/kaggle/input/aptos2019/train_images/train_images/daad7b617f21.png\n/kaggle/input/aptos2019/train_images/train_images/de38adaae009.png\n/kaggle/input/aptos2019/train_images/train_images/c6c2bad91f23.png\n/kaggle/input/aptos2019/train_images/train_images/6e44f6d04fc9.png\n/kaggle/input/aptos2019/train_images/train_images/6194e0fff071.png\n/kaggle/input/aptos2019/train_images/train_images/8aab201c0691.png\n/kaggle/input/aptos2019/train_images/train_images/c1896142a20a.png\n/kaggle/input/aptos2019/train_images/train_images/3e6bfc4d5c65.png\n/kaggle/input/aptos2019/train_images/train_images/59bd19c1c5bb.png\n/kaggle/input/aptos2019/train_images/train_images/5d3c8c1f57da.png\n/kaggle/input/aptos2019/train_images/train_images/29f44aea93a4.png\n/kaggle/input/aptos2019/train_images/train_images/cc964bf04dbc.png\n/kaggle/input/aptos2019/train_images/train_images/cffc50047828.png\n/kaggle/input/aptos2019/train_images/train_images/8114d6a160df.png\n/kaggle/input/aptos2019/train_images/train_images/66b88a4bc474.png\n/kaggle/input/aptos2019/train_images/train_images/879744b9dc65.png\n/kaggle/input/aptos2019/train_images/train_images/a0fd94e2ad76.png\n/kaggle/input/aptos2019/train_images/train_images/c6a8f8f998a2.png\n/kaggle/input/aptos2019/train_images/train_images/2c2aa057afc5.png\n/kaggle/input/aptos2019/train_images/train_images/1ca62b3e4fd3.png\n/kaggle/input/aptos2019/train_images/train_images/bc34f52c37c7.png\n/kaggle/input/aptos2019/train_images/train_images/cd93a472e5cd.png\n/kaggle/input/aptos2019/train_images/train_images/a15652b22ab8.png\n/kaggle/input/aptos2019/train_images/train_images/523b3f0fc646.png\n/kaggle/input/aptos2019/train_images/train_images/42b93b574f23.png\n/kaggle/input/aptos2019/train_images/train_images/2a099b247b10.png\n/kaggle/input/aptos2019/train_images/train_images/384e6c915722.png\n/kaggle/input/aptos2019/train_images/train_images/599b89048034.png\n/kaggle/input/aptos2019/train_images/train_images/96b5474ae604.png\n/kaggle/input/aptos2019/train_images/train_images/ab78a66dee6a.png\n/kaggle/input/aptos2019/train_images/train_images/32d7d360d891.png\n/kaggle/input/aptos2019/train_images/train_images/51269b77d312.png\n/kaggle/input/aptos2019/train_images/train_images/8e2a3978c244.png\n/kaggle/input/aptos2019/train_images/train_images/5bf3357a2823.png\n/kaggle/input/aptos2019/train_images/train_images/a528be013a04.png\n/kaggle/input/aptos2019/train_images/train_images/7b211d8bd249.png\n/kaggle/input/aptos2019/train_images/train_images/9e2ba2b979f1.png\n/kaggle/input/aptos2019/train_images/train_images/3c72f580d4ba.png\n/kaggle/input/aptos2019/train_images/train_images/bb733062f494.png\n/kaggle/input/aptos2019/train_images/train_images/d67374d3fa2a.png\n/kaggle/input/aptos2019/train_images/train_images/32a3eb37ff40.png\n/kaggle/input/aptos2019/train_images/train_images/a07efb1ecfc0.png\n/kaggle/input/aptos2019/train_images/train_images/1d0b93317aa8.png\n/kaggle/input/aptos2019/train_images/train_images/3b4a5fcbe5e0.png\n/kaggle/input/aptos2019/train_images/train_images/9232dc06cfdc.png\n/kaggle/input/aptos2019/train_images/train_images/d9e58e4d8689.png\n/kaggle/input/aptos2019/train_images/train_images/d4583e9525dc.png\n/kaggle/input/aptos2019/train_images/train_images/9688c6ef5dc5.png\n/kaggle/input/aptos2019/train_images/train_images/4fecf87184e6.png\n/kaggle/input/aptos2019/train_images/train_images/2b2f5a0f880d.png\n/kaggle/input/aptos2019/train_images/train_images/237c078d00fc.png\n/kaggle/input/aptos2019/train_images/train_images/384631079d1e.png\n/kaggle/input/aptos2019/train_images/train_images/af831c158744.png\n/kaggle/input/aptos2019/train_images/train_images/357f02a779d7.png\n/kaggle/input/aptos2019/train_images/train_images/4464bb62bf20.png\n/kaggle/input/aptos2019/train_images/train_images/aea59ebec445.png\n/kaggle/input/aptos2019/train_images/train_images/bf7b4eae7ad0.png\n/kaggle/input/aptos2019/train_images/train_images/1e1fb019710d.png\n/kaggle/input/aptos2019/train_images/train_images/2a5a8b744f08.png\n/kaggle/input/aptos2019/train_images/train_images/757572337fd0.png\n/kaggle/input/aptos2019/train_images/train_images/d48178e4a49b.png\n/kaggle/input/aptos2019/train_images/train_images/4f0866b90c27.png\n/kaggle/input/aptos2019/train_images/train_images/35df2bc6ae95.png\n/kaggle/input/aptos2019/train_images/train_images/9568eb7e9c08.png\n/kaggle/input/aptos2019/train_images/train_images/c561bcd519e9.png\n/kaggle/input/aptos2019/train_images/train_images/bd5013540a13.png\n/kaggle/input/aptos2019/train_images/train_images/b1b3e7d0a5f3.png\n/kaggle/input/aptos2019/train_images/train_images/67ed8cc78b97.png\n/kaggle/input/aptos2019/train_images/train_images/c5b58cc992af.png\n/kaggle/input/aptos2019/train_images/train_images/665ce639a331.png\n/kaggle/input/aptos2019/train_images/train_images/4d1cf360b2d7.png\n/kaggle/input/aptos2019/train_images/train_images/4aa07d720638.png\n/kaggle/input/aptos2019/train_images/train_images/686ed1dbae20.png\n/kaggle/input/aptos2019/train_images/train_images/8db2ce991101.png\n/kaggle/input/aptos2019/train_images/train_images/83fda7c0500b.png\n/kaggle/input/aptos2019/train_images/train_images/6a91eb157f47.png\n/kaggle/input/aptos2019/train_images/train_images/4c9f0fdaaef7.png\n/kaggle/input/aptos2019/train_images/train_images/d1fb4efb117c.png\n/kaggle/input/aptos2019/train_images/train_images/6f0463c1ff18.png\n/kaggle/input/aptos2019/train_images/train_images/94f9ecf4b8d2.png\n/kaggle/input/aptos2019/train_images/train_images/54dc6e8107cd.png\n/kaggle/input/aptos2019/train_images/train_images/7214fc7cbe03.png\n/kaggle/input/aptos2019/train_images/train_images/5c817060c0ed.png\n/kaggle/input/aptos2019/train_images/train_images/b085caa513a8.png\n/kaggle/input/aptos2019/train_images/train_images/962cf85e4f6d.png\n/kaggle/input/aptos2019/train_images/train_images/2f2e1949ad56.png\n/kaggle/input/aptos2019/train_images/train_images/8bad12d70368.png\n/kaggle/input/aptos2019/train_images/train_images/8a7765e785fb.png\n/kaggle/input/aptos2019/train_images/train_images/674057ab250c.png\n/kaggle/input/aptos2019/train_images/train_images/5baed382f062.png\n/kaggle/input/aptos2019/train_images/train_images/83e529e95b0e.png\n/kaggle/input/aptos2019/train_images/train_images/6de39b94f634.png\n/kaggle/input/aptos2019/train_images/train_images/9b95d6203406.png\n/kaggle/input/aptos2019/train_images/train_images/cb2201c226d6.png\n/kaggle/input/aptos2019/train_images/train_images/65f69234c8a7.png\n/kaggle/input/aptos2019/train_images/train_images/d844a7252f4e.png\n/kaggle/input/aptos2019/train_images/train_images/bff51afc76d4.png\n/kaggle/input/aptos2019/train_images/train_images/69fff98cb32a.png\n/kaggle/input/aptos2019/train_images/train_images/c80b0f27541a.png\n/kaggle/input/aptos2019/train_images/train_images/cfb17a7cc8d4.png\n/kaggle/input/aptos2019/train_images/train_images/c1799a6f5c65.png\n/kaggle/input/aptos2019/train_images/train_images/594f69b503ad.png\n/kaggle/input/aptos2019/train_images/train_images/1da25637859b.png\n/kaggle/input/aptos2019/train_images/train_images/291e2ff3d834.png\n/kaggle/input/aptos2019/train_images/train_images/1cc58b15f466.png\n/kaggle/input/aptos2019/train_images/train_images/b22354b5f94b.png\n/kaggle/input/aptos2019/train_images/train_images/3c311c9109b0.png\n/kaggle/input/aptos2019/train_images/train_images/498f143c0374.png\n/kaggle/input/aptos2019/train_images/train_images/bb08949dd70a.png\n/kaggle/input/aptos2019/train_images/train_images/3ee4841936ef.png\n/kaggle/input/aptos2019/train_images/train_images/384db24ebbd7.png\n/kaggle/input/aptos2019/train_images/train_images/aafe980edd0c.png\n/kaggle/input/aptos2019/train_images/train_images/5f51192841f7.png\n/kaggle/input/aptos2019/train_images/train_images/70f5caf5f305.png\n/kaggle/input/aptos2019/train_images/train_images/a247961a5cd9.png\n/kaggle/input/aptos2019/train_images/train_images/d141728fa392.png\n/kaggle/input/aptos2019/train_images/train_images/6e3526053de0.png\n/kaggle/input/aptos2019/train_images/train_images/29d059522fa1.png\n/kaggle/input/aptos2019/train_images/train_images/38c7153457e2.png\n/kaggle/input/aptos2019/train_images/train_images/6a2642131e4a.png\n/kaggle/input/aptos2019/train_images/train_images/b74de20d73de.png\n/kaggle/input/aptos2019/train_images/train_images/28f73575e1f2.png\n/kaggle/input/aptos2019/train_images/train_images/8ef2eb8c51c4.png\n/kaggle/input/aptos2019/train_images/train_images/38fe9f854046.png\n/kaggle/input/aptos2019/train_images/train_images/8d7bb0649a02.png\n/kaggle/input/aptos2019/train_images/train_images/e4210e7fe587.png\n/kaggle/input/aptos2019/train_images/train_images/7005be54cab1.png\n/kaggle/input/aptos2019/train_images/train_images/4c78d9d18da9.png\n/kaggle/input/aptos2019/train_images/train_images/51a078d6d43a.png\n/kaggle/input/aptos2019/train_images/train_images/9c514d2d5b3f.png\n/kaggle/input/aptos2019/train_images/train_images/7c52fe73e748.png\n/kaggle/input/aptos2019/train_images/train_images/22ce8ef69357.png\n/kaggle/input/aptos2019/train_images/train_images/26cd40b57ad1.png\n/kaggle/input/aptos2019/train_images/train_images/4f5dd7660b17.png\n/kaggle/input/aptos2019/train_images/train_images/aad0c0ee9268.png\n/kaggle/input/aptos2019/train_images/train_images/966c07831334.png\n/kaggle/input/aptos2019/train_images/train_images/b3d12069e1c5.png\n/kaggle/input/aptos2019/train_images/train_images/7335a2d43ada.png\n/kaggle/input/aptos2019/train_images/train_images/cff262ed8f4c.png\n/kaggle/input/aptos2019/train_images/train_images/9837048b85dc.png\n/kaggle/input/aptos2019/train_images/train_images/887c26fc0e1f.png\n/kaggle/input/aptos2019/train_images/train_images/8191ae701985.png\n/kaggle/input/aptos2019/train_images/train_images/a1edf0e66592.png\n/kaggle/input/aptos2019/train_images/train_images/4fd5ec0dca09.png\n/kaggle/input/aptos2019/train_images/train_images/5a2c27b95c7c.png\n/kaggle/input/aptos2019/train_images/train_images/5188a8afa879.png\n/kaggle/input/aptos2019/train_images/train_images/77f69c7ff324.png\n/kaggle/input/aptos2019/train_images/train_images/8be6629a6039.png\n/kaggle/input/aptos2019/train_images/train_images/484dbeb9bf2a.png\n/kaggle/input/aptos2019/train_images/train_images/ce887b196c23.png\n/kaggle/input/aptos2019/train_images/train_images/63b4d030b016.png\n/kaggle/input/aptos2019/train_images/train_images/599498e9e4bc.png\n/kaggle/input/aptos2019/train_images/train_images/d473f6fafba0.png\n/kaggle/input/aptos2019/train_images/train_images/3a6e9730b298.png\n/kaggle/input/aptos2019/train_images/train_images/4860f7813654.png\n/kaggle/input/aptos2019/train_images/train_images/77e7c7a160c8.png\n/kaggle/input/aptos2019/train_images/train_images/9095d43fb132.png\n/kaggle/input/aptos2019/train_images/train_images/a3b2e93d058b.png\n/kaggle/input/aptos2019/train_images/train_images/84e8c62165b5.png\n/kaggle/input/aptos2019/train_images/train_images/d838d5b9f571.png\n/kaggle/input/aptos2019/train_images/train_images/a06e5ac695ce.png\n/kaggle/input/aptos2019/train_images/train_images/d8404680bba6.png\n/kaggle/input/aptos2019/train_images/train_images/71c1a3cdbe47.png\n/kaggle/input/aptos2019/train_images/train_images/98fbe56dcc2c.png\n/kaggle/input/aptos2019/train_images/train_images/8688f3d0fcaf.png\n/kaggle/input/aptos2019/train_images/train_images/66d2ca47aa44.png\n/kaggle/input/aptos2019/train_images/train_images/6d9effbcde78.png\n/kaggle/input/aptos2019/train_images/train_images/ad93d88c87ea.png\n/kaggle/input/aptos2019/train_images/train_images/a9e3d186cd1b.png\n/kaggle/input/aptos2019/train_images/train_images/cf603a9ef2d5.png\n/kaggle/input/aptos2019/train_images/train_images/d3de0d313d61.png\n/kaggle/input/aptos2019/train_images/train_images/44f4ae58990e.png\n/kaggle/input/aptos2019/train_images/train_images/35777eb7859d.png\n/kaggle/input/aptos2019/train_images/train_images/7be1b9aa78aa.png\n/kaggle/input/aptos2019/train_images/train_images/5db895d3f1fc.png\n/kaggle/input/aptos2019/train_images/train_images/2f42e20db938.png\n/kaggle/input/aptos2019/train_images/train_images/45ae04cfde5d.png\n/kaggle/input/aptos2019/train_images/train_images/da3a2275c850.png\n/kaggle/input/aptos2019/train_images/train_images/8860c7b11530.png\n/kaggle/input/aptos2019/train_images/train_images/6c00dd8bf708.png\n/kaggle/input/aptos2019/train_images/train_images/2ecbc2e3f239.png\n/kaggle/input/aptos2019/train_images/train_images/a443c4fd489c.png\n/kaggle/input/aptos2019/train_images/train_images/495255c7492f.png\n/kaggle/input/aptos2019/train_images/train_images/6e092b306fe1.png\n/kaggle/input/aptos2019/train_images/train_images/6762b2b48ea5.png\n/kaggle/input/aptos2019/train_images/train_images/d28bd830c171.png\n/kaggle/input/aptos2019/train_images/train_images/5b5b80a3edee.png\n/kaggle/input/aptos2019/train_images/train_images/d18f6431ebce.png\n/kaggle/input/aptos2019/train_images/train_images/aae8f9f3ef8c.png\n/kaggle/input/aptos2019/train_images/train_images/a34fc5376669.png\n/kaggle/input/aptos2019/train_images/train_images/57a5e4274275.png\n/kaggle/input/aptos2019/train_images/train_images/650104ede84c.png\n/kaggle/input/aptos2019/train_images/train_images/260a455692b5.png\n/kaggle/input/aptos2019/train_images/train_images/75a4343b12f9.png\n/kaggle/input/aptos2019/train_images/train_images/8543a801dce0.png\n/kaggle/input/aptos2019/train_images/train_images/95e732e043a1.png\n/kaggle/input/aptos2019/train_images/train_images/87b671c6d4c5.png\n/kaggle/input/aptos2019/train_images/train_images/33d72035c27a.png\n/kaggle/input/aptos2019/train_images/train_images/89b725411cee.png\n/kaggle/input/aptos2019/train_images/train_images/234399352d36.png\n/kaggle/input/aptos2019/train_images/train_images/612f2df37a1d.png\n/kaggle/input/aptos2019/train_images/train_images/33105f9b3a04.png\n/kaggle/input/aptos2019/train_images/train_images/3428230bf1bd.png\n/kaggle/input/aptos2019/train_images/train_images/8b58f9a338e8.png\n/kaggle/input/aptos2019/train_images/train_images/6a2c3f4ef329.png\n/kaggle/input/aptos2019/train_images/train_images/4a4cb731f91a.png\n/kaggle/input/aptos2019/train_images/train_images/94372043d55b.png\n/kaggle/input/aptos2019/train_images/train_images/99c626e58464.png\n/kaggle/input/aptos2019/train_images/train_images/c5bec7f1e5f3.png\n/kaggle/input/aptos2019/train_images/train_images/9782c0489eca.png\n/kaggle/input/aptos2019/train_images/train_images/2241b7e90782.png\n/kaggle/input/aptos2019/train_images/train_images/3dfc50108072.png\n/kaggle/input/aptos2019/train_images/train_images/39fd8ef3a45c.png\n/kaggle/input/aptos2019/train_images/train_images/496155f71d0a.png\n/kaggle/input/aptos2019/train_images/train_images/bfaa0080ab61.png\n/kaggle/input/aptos2019/train_images/train_images/76b950c6ed5e.png\n/kaggle/input/aptos2019/train_images/train_images/b963a11638f2.png\n/kaggle/input/aptos2019/train_images/train_images/996f57c86ba5.png\n/kaggle/input/aptos2019/train_images/train_images/247e98aba610.png\n/kaggle/input/aptos2019/train_images/train_images/a01c590c444f.png\n/kaggle/input/aptos2019/train_images/train_images/4029d70e9d8a.png\n/kaggle/input/aptos2019/train_images/train_images/6165081b9021.png\n/kaggle/input/aptos2019/train_images/train_images/4ad8d3ec8789.png\n/kaggle/input/aptos2019/train_images/train_images/2cacdb0dffae.png\n/kaggle/input/aptos2019/train_images/train_images/d803598dabda.png\n/kaggle/input/aptos2019/train_images/train_images/4384fa687afa.png\n/kaggle/input/aptos2019/train_images/train_images/47b756014447.png\n/kaggle/input/aptos2019/train_images/train_images/c5ad60521f8c.png\n/kaggle/input/aptos2019/train_images/train_images/870fbe6eaa68.png\n/kaggle/input/aptos2019/train_images/train_images/44a86263117b.png\n/kaggle/input/aptos2019/train_images/train_images/4927945ecfed.png\n/kaggle/input/aptos2019/train_images/train_images/7131bf4c9e6f.png\n/kaggle/input/aptos2019/train_images/train_images/d6e26fe51dce.png\n/kaggle/input/aptos2019/train_images/train_images/70ed3ec68b94.png\n/kaggle/input/aptos2019/train_images/train_images/cd4e7f9fa1a9.png\n/kaggle/input/aptos2019/train_images/train_images/d3e56584a481.png\n/kaggle/input/aptos2019/train_images/train_images/a9a3225cf4b5.png\n/kaggle/input/aptos2019/train_images/train_images/e1900014dabf.png\n/kaggle/input/aptos2019/train_images/train_images/ca9c912ebad7.png\n/kaggle/input/aptos2019/train_images/train_images/2776d70724d3.png\n/kaggle/input/aptos2019/train_images/train_images/1dfe599d12a9.png\n/kaggle/input/aptos2019/train_images/train_images/c1c8550508e0.png\n/kaggle/input/aptos2019/train_images/train_images/a646c084928c.png\n/kaggle/input/aptos2019/train_images/train_images/a8e08e7fe016.png\n/kaggle/input/aptos2019/train_images/train_images/1caba2fb38f6.png\n/kaggle/input/aptos2019/train_images/train_images/1f31701dd61b.png\n/kaggle/input/aptos2019/train_images/train_images/beeca5f14618.png\n/kaggle/input/aptos2019/train_images/train_images/2f81ee5f2926.png\n/kaggle/input/aptos2019/train_images/train_images/58529a8638d0.png\n/kaggle/input/aptos2019/train_images/train_images/51d574513bcb.png\n/kaggle/input/aptos2019/train_images/train_images/8329e80c10ac.png\n/kaggle/input/aptos2019/train_images/train_images/6653ad026901.png\n/kaggle/input/aptos2019/train_images/train_images/6180920bc224.png\n/kaggle/input/aptos2019/train_images/train_images/3b9817a39adf.png\n/kaggle/input/aptos2019/train_images/train_images/78b3f819dcc5.png\n/kaggle/input/aptos2019/train_images/train_images/3823acc4e464.png\n/kaggle/input/aptos2019/train_images/train_images/3b10191dfd25.png\n/kaggle/input/aptos2019/train_images/train_images/d2c2f02bb313.png\n/kaggle/input/aptos2019/train_images/train_images/46d3316c4857.png\n/kaggle/input/aptos2019/train_images/train_images/c0e509786f7f.png\n/kaggle/input/aptos2019/train_images/train_images/b60dbf9f0744.png\n/kaggle/input/aptos2019/train_images/train_images/7350c50667c5.png\n/kaggle/input/aptos2019/train_images/train_images/bf6cbccacf39.png\n/kaggle/input/aptos2019/train_images/train_images/b06dabab4f09.png\n/kaggle/input/aptos2019/train_images/train_images/e087bd4b88f2.png\n/kaggle/input/aptos2019/train_images/train_images/b4f41b5bf0ef.png\n/kaggle/input/aptos2019/train_images/train_images/b086c7cd3868.png\n/kaggle/input/aptos2019/train_images/train_images/352e4a939242.png\n/kaggle/input/aptos2019/train_images/train_images/85cbb84ac8e0.png\n/kaggle/input/aptos2019/train_images/train_images/b16dd4483ca5.png\n/kaggle/input/aptos2019/train_images/train_images/5b804948e35f.png\n/kaggle/input/aptos2019/train_images/train_images/b89938407ee6.png\n/kaggle/input/aptos2019/train_images/train_images/5cbe88914a72.png\n/kaggle/input/aptos2019/train_images/train_images/d2d523e9f669.png\n/kaggle/input/aptos2019/train_images/train_images/e06cccc08c59.png\n/kaggle/input/aptos2019/train_images/train_images/22325552a4e3.png\n/kaggle/input/aptos2019/train_images/train_images/96793edb1003.png\n/kaggle/input/aptos2019/train_images/train_images/57a5f1015504.png\n/kaggle/input/aptos2019/train_images/train_images/962c0fc85e13.png\n/kaggle/input/aptos2019/train_images/train_images/73ba798fee25.png\n/kaggle/input/aptos2019/train_images/train_images/1df1530b9b8d.png\n/kaggle/input/aptos2019/train_images/train_images/31cb39681f6a.png\n/kaggle/input/aptos2019/train_images/train_images/daeaa5d8cf70.png\n/kaggle/input/aptos2019/train_images/train_images/5dc23e440de3.png\n/kaggle/input/aptos2019/train_images/train_images/a2ad3da4c7d6.png\n/kaggle/input/aptos2019/train_images/train_images/91cbe1c775ef.png\n/kaggle/input/aptos2019/train_images/train_images/48a45619d1a3.png\n/kaggle/input/aptos2019/train_images/train_images/cd314653a4d8.png\n/kaggle/input/aptos2019/train_images/train_images/6fe4751a3b42.png\n/kaggle/input/aptos2019/train_images/train_images/cd941e5bc659.png\n/kaggle/input/aptos2019/train_images/train_images/33e7bf536fc5.png\n/kaggle/input/aptos2019/train_images/train_images/c62cef02efa2.png\n/kaggle/input/aptos2019/train_images/train_images/3a1d3ce00f0c.png\n/kaggle/input/aptos2019/train_images/train_images/ac17cc18a994.png\n/kaggle/input/aptos2019/train_images/train_images/4a213b405ee4.png\n/kaggle/input/aptos2019/train_images/train_images/7f0ffeb0a333.png\n/kaggle/input/aptos2019/train_images/train_images/57a710de68a4.png\n/kaggle/input/aptos2019/train_images/train_images/e1b8acb1cea1.png\n/kaggle/input/aptos2019/train_images/train_images/89d2a7403a06.png\n/kaggle/input/aptos2019/train_images/train_images/454a944eb557.png\n/kaggle/input/aptos2019/train_images/train_images/abe940882578.png\n/kaggle/input/aptos2019/train_images/train_images/90960ddf4d14.png\n/kaggle/input/aptos2019/train_images/train_images/a45c30da0c72.png\n/kaggle/input/aptos2019/train_images/train_images/cf0824f53dd9.png\n/kaggle/input/aptos2019/train_images/train_images/d39752cb6e57.png\n/kaggle/input/aptos2019/train_images/train_images/9859e2a6cc24.png\n/kaggle/input/aptos2019/train_images/train_images/d83d0695e215.png\n/kaggle/input/aptos2019/train_images/train_images/dee687c6e88a.png\n/kaggle/input/aptos2019/train_images/train_images/25e9fd872182.png\n/kaggle/input/aptos2019/train_images/train_images/453d553b0a94.png\n/kaggle/input/aptos2019/train_images/train_images/1bb0ddfe753a.png\n/kaggle/input/aptos2019/train_images/train_images/7663aba8d762.png\n/kaggle/input/aptos2019/train_images/train_images/d866c26d76f0.png\n/kaggle/input/aptos2019/train_images/train_images/2a08ed6bbcbc.png\n/kaggle/input/aptos2019/train_images/train_images/cac80797770f.png\n/kaggle/input/aptos2019/train_images/train_images/7270367410a1.png\n/kaggle/input/aptos2019/train_images/train_images/4fef9ed8a4c5.png\n/kaggle/input/aptos2019/train_images/train_images/a8eb35b3bcd2.png\n/kaggle/input/aptos2019/train_images/train_images/5ca779ace6e7.png\n/kaggle/input/aptos2019/train_images/train_images/974c7d7b9c64.png\n/kaggle/input/aptos2019/train_images/train_images/6298468d7d75.png\n/kaggle/input/aptos2019/train_images/train_images/5cde55f745af.png\n/kaggle/input/aptos2019/train_images/train_images/4c6c5a1bf5ab.png\n/kaggle/input/aptos2019/train_images/train_images/482088e6be44.png\n/kaggle/input/aptos2019/train_images/train_images/4246ed634f25.png\n/kaggle/input/aptos2019/train_images/train_images/94a67ec0714f.png\n/kaggle/input/aptos2019/train_images/train_images/aa6673241154.png\n/kaggle/input/aptos2019/train_images/train_images/9041eb43456e.png\n/kaggle/input/aptos2019/train_images/train_images/27bab1432f61.png\n/kaggle/input/aptos2019/train_images/train_images/9a7bd084395e.png\n/kaggle/input/aptos2019/train_images/train_images/b7ce561a7328.png\n/kaggle/input/aptos2019/train_images/train_images/8f9819752ca0.png\n/kaggle/input/aptos2019/train_images/train_images/b92eacd1392a.png\n/kaggle/input/aptos2019/train_images/train_images/d9ad2a0ec026.png\n/kaggle/input/aptos2019/train_images/train_images/2cceb07ff706.png\n/kaggle/input/aptos2019/train_images/train_images/4dbce359d0e1.png\n/kaggle/input/aptos2019/train_images/train_images/5a0fe0ee4301.png\n/kaggle/input/aptos2019/train_images/train_images/4a44cc840ebe.png\n/kaggle/input/aptos2019/train_images/train_images/535682537302.png\n/kaggle/input/aptos2019/train_images/train_images/4d009cebabc9.png\n/kaggle/input/aptos2019/train_images/train_images/405b4f78658f.png\n/kaggle/input/aptos2019/train_images/train_images/d30d079e6f9a.png\n/kaggle/input/aptos2019/train_images/train_images/5a5d3798c357.png\n/kaggle/input/aptos2019/train_images/train_images/1ee355480567.png\n/kaggle/input/aptos2019/train_images/train_images/c6a2975228af.png\n/kaggle/input/aptos2019/train_images/train_images/276b14f72328.png\n/kaggle/input/aptos2019/train_images/train_images/210bfe0127c6.png\n/kaggle/input/aptos2019/train_images/train_images/86e7f98f73f1.png\n/kaggle/input/aptos2019/train_images/train_images/b77b8a1f09f1.png\n/kaggle/input/aptos2019/train_images/train_images/37de05ef12a5.png\n/kaggle/input/aptos2019/train_images/train_images/91e8af9ceee9.png\n/kaggle/input/aptos2019/train_images/train_images/4c53cc97ea13.png\n/kaggle/input/aptos2019/train_images/train_images/b187b3c93afb.png\n/kaggle/input/aptos2019/train_images/train_images/c9d42d7534e0.png\n/kaggle/input/aptos2019/train_images/train_images/71f6a6e4620a.png\n/kaggle/input/aptos2019/train_images/train_images/44855f666225.png\n/kaggle/input/aptos2019/train_images/train_images/7fe7309d0b4f.png\n/kaggle/input/aptos2019/train_images/train_images/bc73ce76ec43.png\n/kaggle/input/aptos2019/train_images/train_images/300305ce82d2.png\n/kaggle/input/aptos2019/train_images/train_images/253e96488cfb.png\n/kaggle/input/aptos2019/train_images/train_images/6c6505a0c637.png\n/kaggle/input/aptos2019/train_images/train_images/e4151feb8443.png\n/kaggle/input/aptos2019/train_images/train_images/6c9c902a97de.png\n/kaggle/input/aptos2019/train_images/train_images/a02dfd67a925.png\n/kaggle/input/aptos2019/train_images/train_images/5c6194562ed2.png\n/kaggle/input/aptos2019/train_images/train_images/7a12f49e29df.png\n/kaggle/input/aptos2019/train_images/train_images/b99794a0beed.png\n/kaggle/input/aptos2019/train_images/train_images/e3e490babc0c.png\n/kaggle/input/aptos2019/train_images/train_images/b55d2ddb3e75.png\n/kaggle/input/aptos2019/train_images/train_images/42985aa2e32f.png\n/kaggle/input/aptos2019/train_images/train_images/23175b7ef453.png\n/kaggle/input/aptos2019/train_images/train_images/bde1063a5dd7.png\n/kaggle/input/aptos2019/train_images/train_images/c013e869acce.png\n/kaggle/input/aptos2019/train_images/train_images/4eaf2f81819d.png\n/kaggle/input/aptos2019/train_images/train_images/e04f3c6619a3.png\n/kaggle/input/aptos2019/train_images/train_images/8a81f62320d6.png\n/kaggle/input/aptos2019/train_images/train_images/2a7373eeb352.png\n/kaggle/input/aptos2019/train_images/train_images/54bdcdecd8f3.png\n/kaggle/input/aptos2019/train_images/train_images/5633ced07d8e.png\n/kaggle/input/aptos2019/train_images/train_images/4bd5d0b30198.png\n/kaggle/input/aptos2019/train_images/train_images/90a786abe58e.png\n/kaggle/input/aptos2019/train_images/train_images/d26bc6e1230d.png\n/kaggle/input/aptos2019/train_images/train_images/ab03d50bba2f.png\n/kaggle/input/aptos2019/train_images/train_images/6a57a3db3eff.png\n/kaggle/input/aptos2019/train_images/train_images/93802d1e3c41.png\n/kaggle/input/aptos2019/train_images/train_images/91e82fe4e434.png\n/kaggle/input/aptos2019/train_images/train_images/aa10a4b2e709.png\n/kaggle/input/aptos2019/train_images/train_images/7f84284598f5.png\n/kaggle/input/aptos2019/train_images/train_images/5ce5eeaf757a.png\n/kaggle/input/aptos2019/train_images/train_images/4a96c28f3f07.png\n/kaggle/input/aptos2019/train_images/train_images/94b1d8ad35ec.png\n/kaggle/input/aptos2019/train_images/train_images/6089fa333013.png\n/kaggle/input/aptos2019/train_images/train_images/de6210f88536.png\n/kaggle/input/aptos2019/train_images/train_images/2b3a4a81d748.png\n/kaggle/input/aptos2019/train_images/train_images/e31c42a8652b.png\n/kaggle/input/aptos2019/train_images/train_images/873dcc0b468f.png\n/kaggle/input/aptos2019/train_images/train_images/7bf981d9c7fe.png\n/kaggle/input/aptos2019/train_images/train_images/dce73d90c00c.png\n/kaggle/input/aptos2019/train_images/train_images/1f4fb37e0854.png\n/kaggle/input/aptos2019/train_images/train_images/80964d8e0863.png\n/kaggle/input/aptos2019/train_images/train_images/ae49cc60f251.png\n/kaggle/input/aptos2019/train_images/train_images/a76b69e443ce.png\n/kaggle/input/aptos2019/train_images/train_images/25b4080f598b.png\n/kaggle/input/aptos2019/train_images/train_images/8871e6a26596.png\n/kaggle/input/aptos2019/train_images/train_images/2b074afdf626.png\n/kaggle/input/aptos2019/train_images/train_images/94076a9fb9b5.png\n/kaggle/input/aptos2019/train_images/train_images/d5ad3362424c.png\n/kaggle/input/aptos2019/train_images/train_images/97f290d31813.png\n/kaggle/input/aptos2019/train_images/train_images/de18071c36e6.png\n/kaggle/input/aptos2019/train_images/train_images/1ffaa51a6245.png\n/kaggle/input/aptos2019/train_images/train_images/dd3dad6ca78f.png\n/kaggle/input/aptos2019/train_images/train_images/784d6d302f98.png\n/kaggle/input/aptos2019/train_images/train_images/be521870a0ea.png\n/kaggle/input/aptos2019/train_images/train_images/40527a5e95dd.png\n/kaggle/input/aptos2019/train_images/train_images/d66ccb75ada1.png\n/kaggle/input/aptos2019/train_images/train_images/5ead17e894ae.png\n/kaggle/input/aptos2019/train_images/train_images/789434d095d1.png\n/kaggle/input/aptos2019/train_images/train_images/c70d09370109.png\n/kaggle/input/aptos2019/train_images/train_images/851e40a21f81.png\n/kaggle/input/aptos2019/train_images/train_images/b46b09a45f39.png\n/kaggle/input/aptos2019/train_images/train_images/aa94cc4bfd84.png\n/kaggle/input/aptos2019/train_images/train_images/2f143453bb71.png\n/kaggle/input/aptos2019/train_images/train_images/9c088d2d1559.png\n/kaggle/input/aptos2019/train_images/train_images/38487e1a5b1f.png\n/kaggle/input/aptos2019/train_images/train_images/84a72e15b23c.png\n/kaggle/input/aptos2019/train_images/train_images/274f4de2a59d.png\n/kaggle/input/aptos2019/train_images/train_images/9b093fe95d6b.png\n/kaggle/input/aptos2019/train_images/train_images/bfd5c0e55420.png\n/kaggle/input/aptos2019/train_images/train_images/de55ed25e0e8.png\n/kaggle/input/aptos2019/train_images/train_images/6b30767595d8.png\n/kaggle/input/aptos2019/train_images/train_images/65e6f1bd9875.png\n/kaggle/input/aptos2019/train_images/train_images/8af6a4e5396f.png\n/kaggle/input/aptos2019/train_images/train_images/c7b622ec8104.png\n/kaggle/input/aptos2019/train_images/train_images/613028ede6a0.png\n/kaggle/input/aptos2019/train_images/train_images/393fa5a023a5.png\n/kaggle/input/aptos2019/train_images/train_images/457c7c927e27.png\n/kaggle/input/aptos2019/train_images/train_images/beb00fa6e7c9.png\n/kaggle/input/aptos2019/train_images/train_images/24b87f744598.png\n/kaggle/input/aptos2019/train_images/train_images/3a61e690f4bb.png\n/kaggle/input/aptos2019/train_images/train_images/91e2c2890c9f.png\n/kaggle/input/aptos2019/train_images/train_images/d667af5742f6.png\n/kaggle/input/aptos2019/train_images/train_images/8676427e4625.png\n/kaggle/input/aptos2019/train_images/train_images/1f3f32efaf20.png\n/kaggle/input/aptos2019/train_images/train_images/66375b3c64db.png\n/kaggle/input/aptos2019/train_images/train_images/70d657f8f503.png\n/kaggle/input/aptos2019/train_images/train_images/26fc2358a38d.png\n/kaggle/input/aptos2019/train_images/train_images/232549883508.png\n/kaggle/input/aptos2019/train_images/train_images/e3b47ed5b511.png\n/kaggle/input/aptos2019/train_images/train_images/21037f5c7790.png\n/kaggle/input/aptos2019/train_images/train_images/cac40227d3b2.png\n/kaggle/input/aptos2019/train_images/train_images/d10ef306996b.png\n/kaggle/input/aptos2019/train_images/train_images/76c0c7e1b6cb.png\n/kaggle/input/aptos2019/train_images/train_images/8c87bd748996.png\n/kaggle/input/aptos2019/train_images/train_images/6c315ad3d07f.png\n/kaggle/input/aptos2019/train_images/train_images/9e2a8135f471.png\n/kaggle/input/aptos2019/train_images/train_images/4a693dd3921a.png\n/kaggle/input/aptos2019/train_images/train_images/b0c9a492e068.png\n/kaggle/input/aptos2019/train_images/train_images/99e8bf998285.png\n/kaggle/input/aptos2019/train_images/train_images/5056fa7d505f.png\n/kaggle/input/aptos2019/train_images/train_images/4205e9deb058.png\n/kaggle/input/aptos2019/train_images/train_images/4ce74e5eb51d.png\n/kaggle/input/aptos2019/train_images/train_images/bcdc8db5423b.png\n/kaggle/input/aptos2019/train_images/train_images/a6731dd737af.png\n/kaggle/input/aptos2019/train_images/train_images/763ad1236efe.png\n/kaggle/input/aptos2019/train_images/train_images/9e3510963315.png\n/kaggle/input/aptos2019/train_images/train_images/b7aca95b97b9.png\n/kaggle/input/aptos2019/train_images/train_images/3c1efa38d0da.png\n/kaggle/input/aptos2019/train_images/train_images/a8e88d4891c4.png\n/kaggle/input/aptos2019/train_images/train_images/b43440c6ebe4.png\n/kaggle/input/aptos2019/train_images/train_images/24f271c87e73.png\n/kaggle/input/aptos2019/train_images/train_images/6d10709053ae.png\n/kaggle/input/aptos2019/train_images/train_images/d2c5fb82fe5f.png\n/kaggle/input/aptos2019/train_images/train_images/d88c4843aec3.png\n/kaggle/input/aptos2019/train_images/train_images/43f22d1be8dd.png\n/kaggle/input/aptos2019/train_images/train_images/bed8296c8dfe.png\n/kaggle/input/aptos2019/train_images/train_images/4c60b10a3a6a.png\n/kaggle/input/aptos2019/train_images/train_images/7a0cff4c24b2.png\n/kaggle/input/aptos2019/train_images/train_images/c8905b8d5cf1.png\n/kaggle/input/aptos2019/train_images/train_images/b17f0b81dab3.png\n/kaggle/input/aptos2019/train_images/train_images/6a905a7202d2.png\n/kaggle/input/aptos2019/train_images/train_images/50840c36f0b4.png\n/kaggle/input/aptos2019/train_images/train_images/465c618f7b23.png\n/kaggle/input/aptos2019/train_images/train_images/89b044cbaf85.png\n/kaggle/input/aptos2019/train_images/train_images/5995321563b7.png\n/kaggle/input/aptos2019/train_images/train_images/3694e8c8e09a.png\n/kaggle/input/aptos2019/train_images/train_images/d56d32a1d62d.png\n/kaggle/input/aptos2019/train_images/train_images/57ce57a8cfb0.png\n/kaggle/input/aptos2019/train_images/train_images/a0267206d51e.png\n/kaggle/input/aptos2019/train_images/train_images/65a7fe9482fe.png\n/kaggle/input/aptos2019/train_images/train_images/e13412678eff.png\n/kaggle/input/aptos2019/train_images/train_images/2cbfc6182ba2.png\n/kaggle/input/aptos2019/train_images/train_images/675de69373f8.png\n/kaggle/input/aptos2019/train_images/train_images/d18e5b68f6d2.png\n/kaggle/input/aptos2019/train_images/train_images/76095c338728.png\n/kaggle/input/aptos2019/train_images/train_images/52edbe29d655.png\n/kaggle/input/aptos2019/train_images/train_images/898f0bc8acfa.png\n/kaggle/input/aptos2019/train_images/train_images/db52626d450c.png\n/kaggle/input/aptos2019/train_images/train_images/312694ea8e6a.png\n/kaggle/input/aptos2019/train_images/train_images/870f433e8f37.png\n/kaggle/input/aptos2019/train_images/train_images/4f46d7ee61ed.png\n/kaggle/input/aptos2019/train_images/train_images/d99dd99be001.png\n/kaggle/input/aptos2019/train_images/train_images/6b66b0e86f7e.png\n/kaggle/input/aptos2019/train_images/train_images/378963f9df22.png\n/kaggle/input/aptos2019/train_images/train_images/b70e7c26f51e.png\n/kaggle/input/aptos2019/train_images/train_images/51131b48f9d4.png\n/kaggle/input/aptos2019/train_images/train_images/db690e2d02f8.png\n/kaggle/input/aptos2019/train_images/train_images/a188c60b93fb.png\n/kaggle/input/aptos2019/train_images/train_images/274f5029189b.png\n/kaggle/input/aptos2019/train_images/train_images/dbd062558b81.png\n/kaggle/input/aptos2019/train_images/train_images/248dec89b3a2.png\n/kaggle/input/aptos2019/train_images/train_images/881ec6186e68.png\n/kaggle/input/aptos2019/train_images/train_images/2d870833c0c9.png\n/kaggle/input/aptos2019/train_images/train_images/3246f07e65b4.png\n/kaggle/input/aptos2019/train_images/train_images/ca0f1a17c8e5.png\n/kaggle/input/aptos2019/train_images/train_images/518e880613de.png\n/kaggle/input/aptos2019/train_images/train_images/a70d0f12a641.png\n/kaggle/input/aptos2019/train_images/train_images/8f1e7433a95d.png\n/kaggle/input/aptos2019/train_images/train_images/a963ac561580.png\n/kaggle/input/aptos2019/train_images/train_images/80d24897669f.png\n/kaggle/input/aptos2019/train_images/train_images/92d9e9f08709.png\n/kaggle/input/aptos2019/train_images/train_images/a182b5b191de.png\n/kaggle/input/aptos2019/train_images/train_images/67d8f94f04e0.png\n/kaggle/input/aptos2019/train_images/train_images/4689b739d240.png\n/kaggle/input/aptos2019/train_images/train_images/e12b67835e03.png\n/kaggle/input/aptos2019/train_images/train_images/61e301bd3c25.png\n/kaggle/input/aptos2019/train_images/train_images/3206171db5be.png\n/kaggle/input/aptos2019/train_images/train_images/49419f8d5cb4.png\n/kaggle/input/aptos2019/train_images/train_images/b294927b14b0.png\n/kaggle/input/aptos2019/train_images/train_images/21abd36095a1.png\n/kaggle/input/aptos2019/train_images/train_images/e12f9f19d1be.png\n/kaggle/input/aptos2019/train_images/train_images/a8dea22ef903.png\n/kaggle/input/aptos2019/train_images/train_images/86fbac86ed3e.png\n/kaggle/input/aptos2019/train_images/train_images/215d2b7c3fde.png\n/kaggle/input/aptos2019/train_images/train_images/8b079e79035f.png\n/kaggle/input/aptos2019/train_images/train_images/c38dec54a9f7.png\n/kaggle/input/aptos2019/train_images/train_images/501c319f7a9f.png\n/kaggle/input/aptos2019/train_images/train_images/60eeae3ba23d.png\n/kaggle/input/aptos2019/test_images/test_images/ef476be214d4.png\n/kaggle/input/aptos2019/test_images/test_images/ec363f48867b.png\n/kaggle/input/aptos2019/test_images/test_images/f481f76a6b75.png\n/kaggle/input/aptos2019/test_images/test_images/fa7fa797c650.png\n/kaggle/input/aptos2019/test_images/test_images/e7578d8dba72.png\n/kaggle/input/aptos2019/test_images/test_images/eda29a9d78f3.png\n/kaggle/input/aptos2019/test_images/test_images/fe0e2dee1834.png\n/kaggle/input/aptos2019/test_images/test_images/e933923aab15.png\n/kaggle/input/aptos2019/test_images/test_images/ef99c499d665.png\n/kaggle/input/aptos2019/test_images/test_images/fc8fce67fbf8.png\n/kaggle/input/aptos2019/test_images/test_images/e9286ddf6ffe.png\n/kaggle/input/aptos2019/test_images/test_images/f0098e9d4aee.png\n/kaggle/input/aptos2019/test_images/test_images/ea1d045f9fea.png\n/kaggle/input/aptos2019/test_images/test_images/e7291472109b.png\n/kaggle/input/aptos2019/test_images/test_images/fd48cf452e9d.png\n/kaggle/input/aptos2019/test_images/test_images/eedae6b28f96.png\n/kaggle/input/aptos2019/test_images/test_images/ef81cd8854cb.png\n/kaggle/input/aptos2019/test_images/test_images/f366fb1cc475.png\n/kaggle/input/aptos2019/test_images/test_images/f7e9fa75c7c1.png\n/kaggle/input/aptos2019/test_images/test_images/e5de79795c1d.png\n/kaggle/input/aptos2019/test_images/test_images/eae70f527755.png\n/kaggle/input/aptos2019/test_images/test_images/f7116e7b2f4e.png\n/kaggle/input/aptos2019/test_images/test_images/fb696a8e055a.png\n/kaggle/input/aptos2019/test_images/test_images/e6a6acf7fca1.png\n/kaggle/input/aptos2019/test_images/test_images/f9aa35187bf3.png\n/kaggle/input/aptos2019/test_images/test_images/ea15a290eb96.png\n/kaggle/input/aptos2019/test_images/test_images/ed6bd9293a89.png\n/kaggle/input/aptos2019/test_images/test_images/fc898dfeb24f.png\n/kaggle/input/aptos2019/test_images/test_images/f6f433f3306f.png\n/kaggle/input/aptos2019/test_images/test_images/e60e4edb3ca9.png\n/kaggle/input/aptos2019/test_images/test_images/fa3e544a7401.png\n/kaggle/input/aptos2019/test_images/test_images/f56ff0440ed1.png\n/kaggle/input/aptos2019/test_images/test_images/f09fd9433dff.png\n/kaggle/input/aptos2019/test_images/test_images/ef8c39eb9157.png\n/kaggle/input/aptos2019/test_images/test_images/fdd18ccbbdc5.png\n/kaggle/input/aptos2019/test_images/test_images/f09cfc6a4dbd.png\n/kaggle/input/aptos2019/test_images/test_images/e9f3c85a2a02.png\n/kaggle/input/aptos2019/test_images/test_images/ecad6845f630.png\n/kaggle/input/aptos2019/test_images/test_images/e62490b7d0e9.png\n/kaggle/input/aptos2019/test_images/test_images/eb32a815f78c.png\n/kaggle/input/aptos2019/test_images/test_images/ef247f28004f.png\n/kaggle/input/aptos2019/test_images/test_images/e8ddfc9709ce.png\n/kaggle/input/aptos2019/test_images/test_images/fe06dad6851c.png\n/kaggle/input/aptos2019/test_images/test_images/f850cb51fdba.png\n/kaggle/input/aptos2019/test_images/test_images/fa9f1bc03f21.png\n/kaggle/input/aptos2019/test_images/test_images/fd62bd0db4f1.png\n/kaggle/input/aptos2019/test_images/test_images/f62b8a076833.png\n/kaggle/input/aptos2019/test_images/test_images/f0546a45ef10.png\n/kaggle/input/aptos2019/test_images/test_images/f6d760566a51.png\n/kaggle/input/aptos2019/test_images/test_images/f6f3ea0d2693.png\n/kaggle/input/aptos2019/test_images/test_images/e77a93c3d9a9.png\n/kaggle/input/aptos2019/test_images/test_images/fba493e17448.png\n/kaggle/input/aptos2019/test_images/test_images/e50b0174690d.png\n/kaggle/input/aptos2019/test_images/test_images/f0a2dc580009.png\n/kaggle/input/aptos2019/test_images/test_images/e4f12411fd85.png\n/kaggle/input/aptos2019/test_images/test_images/e8e44b3160e3.png\n/kaggle/input/aptos2019/test_images/test_images/ee3fe7809e6a.png\n/kaggle/input/aptos2019/test_images/test_images/f9ecf1795804.png\n/kaggle/input/aptos2019/test_images/test_images/f69835dc7c50.png\n/kaggle/input/aptos2019/test_images/test_images/f633c474e8b8.png\n/kaggle/input/aptos2019/test_images/test_images/f0860c21533b.png\n/kaggle/input/aptos2019/test_images/test_images/f4e68b61f480.png\n/kaggle/input/aptos2019/test_images/test_images/e82232a3c28b.png\n/kaggle/input/aptos2019/test_images/test_images/eed4afc8ec83.png\n/kaggle/input/aptos2019/test_images/test_images/eadfc8809ec8.png\n/kaggle/input/aptos2019/test_images/test_images/fe3b0e50be78.png\n/kaggle/input/aptos2019/test_images/test_images/eabc7c716255.png\n/kaggle/input/aptos2019/test_images/test_images/e632e38fd2d4.png\n/kaggle/input/aptos2019/test_images/test_images/ee2c2a5f7d0e.png\n/kaggle/input/aptos2019/test_images/test_images/fcc6aa6755e6.png\n/kaggle/input/aptos2019/test_images/test_images/e893e86dde94.png\n/kaggle/input/aptos2019/test_images/test_images/ed648b9bcd95.png\n/kaggle/input/aptos2019/test_images/test_images/e96bd80a8a53.png\n/kaggle/input/aptos2019/test_images/test_images/e5f73f2855c0.png\n/kaggle/input/aptos2019/test_images/test_images/ea5c42a78979.png\n/kaggle/input/aptos2019/test_images/test_images/fda39982a810.png\n/kaggle/input/aptos2019/test_images/test_images/ed88faaa325a.png\n/kaggle/input/aptos2019/test_images/test_images/ee77763a6afb.png\n/kaggle/input/aptos2019/test_images/test_images/fea14b3d44b0.png\n/kaggle/input/aptos2019/test_images/test_images/f580566e27f5.png\n/kaggle/input/aptos2019/test_images/test_images/fbfa925506f6.png\n/kaggle/input/aptos2019/test_images/test_images/e68746d426b2.png\n/kaggle/input/aptos2019/test_images/test_images/fe674c2f73f5.png\n/kaggle/input/aptos2019/test_images/test_images/ff77e8e5b5f3.png\n/kaggle/input/aptos2019/test_images/test_images/e7a372a1c3a4.png\n/kaggle/input/aptos2019/test_images/test_images/e66855a5c583.png\n/kaggle/input/aptos2019/test_images/test_images/ed246ae1ed08.png\n/kaggle/input/aptos2019/test_images/test_images/fa6f3d8bb1d5.png\n/kaggle/input/aptos2019/test_images/test_images/e724866f5084.png\n/kaggle/input/aptos2019/test_images/test_images/eb6b1f1c09db.png\n/kaggle/input/aptos2019/test_images/test_images/f0267c42907c.png\n/kaggle/input/aptos2019/test_images/test_images/e79e10907295.png\n/kaggle/input/aptos2019/test_images/test_images/f762c272c522.png\n/kaggle/input/aptos2019/test_images/test_images/ed2c06fcc573.png\n/kaggle/input/aptos2019/test_images/test_images/ed2c52c14493.png\n/kaggle/input/aptos2019/test_images/test_images/ea68b58a6e8f.png\n/kaggle/input/aptos2019/test_images/test_images/f71aca5a7dc3.png\n/kaggle/input/aptos2019/test_images/test_images/fb61230b99dd.png\n/kaggle/input/aptos2019/test_images/test_images/ff8a0b45c789.png\n/kaggle/input/aptos2019/test_images/test_images/fed5bb685832.png\n/kaggle/input/aptos2019/test_images/test_images/f26b02ead915.png\n/kaggle/input/aptos2019/test_images/test_images/e68bdd36e589.png\n/kaggle/input/aptos2019/test_images/test_images/e663c6627a95.png\n/kaggle/input/aptos2019/test_images/test_images/f9d8ff3e6592.png\n/kaggle/input/aptos2019/test_images/test_images/f3a268d2726d.png\n/kaggle/input/aptos2019/test_images/test_images/ff4955e76894.png\n/kaggle/input/aptos2019/test_images/test_images/f0c0f7b5e820.png\n/kaggle/input/aptos2019/test_images/test_images/f36cb007a1ef.png\n/kaggle/input/aptos2019/test_images/test_images/edceb0657d77.png\n/kaggle/input/aptos2019/test_images/test_images/f85fd4fac887.png\n/kaggle/input/aptos2019/test_images/test_images/f0e1201b5c1f.png\n/kaggle/input/aptos2019/test_images/test_images/e7fc93ac5b6d.png\n/kaggle/input/aptos2019/test_images/test_images/f66c4ee86629.png\n/kaggle/input/aptos2019/test_images/test_images/e66ad813a508.png\n/kaggle/input/aptos2019/test_images/test_images/ff344e5c9341.png\n/kaggle/input/aptos2019/test_images/test_images/fc782722a50c.png\n/kaggle/input/aptos2019/test_images/test_images/f5c953bee7cd.png\n/kaggle/input/aptos2019/test_images/test_images/e580676516b0.png\n/kaggle/input/aptos2019/test_images/test_images/f4d3777f2710.png\n/kaggle/input/aptos2019/test_images/test_images/fb88d23fc5fe.png\n/kaggle/input/aptos2019/test_images/test_images/e7a7187066ad.png\n/kaggle/input/aptos2019/test_images/test_images/ef7a4ed8d5d1.png\n/kaggle/input/aptos2019/test_images/test_images/e594c19e2e1d.png\n/kaggle/input/aptos2019/test_images/test_images/f1979147aad4.png\n/kaggle/input/aptos2019/test_images/test_images/fecf4c5ae84b.png\n/kaggle/input/aptos2019/test_images/test_images/e811f39a1243.png\n/kaggle/input/aptos2019/test_images/test_images/ff631653374e.png\n/kaggle/input/aptos2019/test_images/test_images/fcc55ae641ae.png\n/kaggle/input/aptos2019/test_images/test_images/e540d2e35d15.png\n/kaggle/input/aptos2019/test_images/test_images/f1a761c68559.png\n/kaggle/input/aptos2019/test_images/test_images/f58cdfa968be.png\n/kaggle/input/aptos2019/test_images/test_images/f3b27ac2d371.png\n/kaggle/input/aptos2019/test_images/test_images/f549294e12e1.png\n/kaggle/input/aptos2019/test_images/test_images/e7defafeb957.png\n/kaggle/input/aptos2019/test_images/test_images/e9faf0296643.png\n/kaggle/input/aptos2019/test_images/test_images/f35d80bb1a22.png\n/kaggle/input/aptos2019/test_images/test_images/f1d719c97838.png\n/kaggle/input/aptos2019/test_images/test_images/ee3f5cf52188.png\n/kaggle/input/aptos2019/test_images/test_images/febfb20dc311.png\n/kaggle/input/aptos2019/test_images/test_images/f7edc074f06b.png\n/kaggle/input/aptos2019/test_images/test_images/f71bea807c96.png\n/kaggle/input/aptos2019/test_images/test_images/e5f332efcbc7.png\n/kaggle/input/aptos2019/test_images/test_images/f819c65b803c.png\n/kaggle/input/aptos2019/test_images/test_images/fc1b1841eadf.png\n/kaggle/input/aptos2019/test_images/test_images/e9678824215d.png\n/kaggle/input/aptos2019/test_images/test_images/ffec9a18a3ce.png\n/kaggle/input/aptos2019/test_images/test_images/e9ab8413e771.png\n/kaggle/input/aptos2019/test_images/test_images/f48241b0c995.png\n/kaggle/input/aptos2019/test_images/test_images/ef8109305128.png\n/kaggle/input/aptos2019/test_images/test_images/e97ecf4355cb.png\n/kaggle/input/aptos2019/test_images/test_images/e69b48516577.png\n/kaggle/input/aptos2019/test_images/test_images/f64b6e85f1c9.png\n/kaggle/input/aptos2019/test_images/test_images/e868c3da340b.png\n/kaggle/input/aptos2019/test_images/test_images/f69400b316a7.png\n/kaggle/input/aptos2019/test_images/test_images/f64214bed40e.png\n/kaggle/input/aptos2019/test_images/test_images/f252046c0fe6.png\n/kaggle/input/aptos2019/test_images/test_images/f02057c41256.png\n/kaggle/input/aptos2019/test_images/test_images/f9d52509c571.png\n/kaggle/input/aptos2019/test_images/test_images/e4dcca36ceb4.png\n/kaggle/input/aptos2019/test_images/test_images/ea588d1e5d96.png\n/kaggle/input/aptos2019/test_images/test_images/fefded6bf135.png\n/kaggle/input/aptos2019/test_images/test_images/f47a2a4a0411.png\n/kaggle/input/aptos2019/test_images/test_images/e9129ce55fd7.png\n/kaggle/input/aptos2019/test_images/test_images/e93394175a19.png\n/kaggle/input/aptos2019/test_images/test_images/ea05c22d92e9.png\n/kaggle/input/aptos2019/test_images/test_images/e821c1b6417a.png\n/kaggle/input/aptos2019/test_images/test_images/ead23cc922ed.png\n/kaggle/input/aptos2019/test_images/test_images/ea6a53e54d0f.png\n/kaggle/input/aptos2019/test_images/test_images/f86d1c404acb.png\n/kaggle/input/aptos2019/test_images/test_images/fcc32dffd24d.png\n/kaggle/input/aptos2019/test_images/test_images/fa9bece586fc.png\n/kaggle/input/aptos2019/test_images/test_images/f994a3b07935.png\n/kaggle/input/aptos2019/test_images/test_images/e529c5757d64.png\n/kaggle/input/aptos2019/test_images/test_images/f4de9620e3f2.png\n/kaggle/input/aptos2019/test_images/test_images/ff59d44a70a7.png\n/kaggle/input/aptos2019/test_images/test_images/fd0a70082e7c.png\n/kaggle/input/aptos2019/test_images/test_images/f61bf44c677c.png\n/kaggle/input/aptos2019/test_images/test_images/eeaea2c5ff34.png\n/kaggle/input/aptos2019/test_images/test_images/ed3a0fc5b546.png\n/kaggle/input/aptos2019/test_images/test_images/eaa0dfbd5024.png\n/kaggle/input/aptos2019/test_images/test_images/f80118bbda18.png\n/kaggle/input/aptos2019/test_images/test_images/ec6659926105.png\n/kaggle/input/aptos2019/test_images/test_images/fbcbc81cf9be.png\n/kaggle/input/aptos2019/test_images/test_images/f4df3d86688d.png\n/kaggle/input/aptos2019/test_images/test_images/f06e7a9df795.png\n/kaggle/input/aptos2019/test_images/test_images/f23902998c21.png\n/kaggle/input/aptos2019/test_images/test_images/fce93caa4758.png\n/kaggle/input/aptos2019/test_images/test_images/eae901557a84.png\n/kaggle/input/aptos2019/test_images/test_images/f02babb3a023.png\n/kaggle/input/aptos2019/test_images/test_images/f71333204618.png\n/kaggle/input/aptos2019/test_images/test_images/f8fc411092c7.png\n/kaggle/input/aptos2019/test_images/test_images/f3a4751af42e.png\n/kaggle/input/aptos2019/test_images/test_images/f02956bd7c50.png\n/kaggle/input/aptos2019/test_images/test_images/ff1e940105f9.png\n/kaggle/input/aptos2019/test_images/test_images/ee059945b08a.png\n/kaggle/input/aptos2019/test_images/test_images/ed6704e3b72e.png\n/kaggle/input/aptos2019/test_images/test_images/fac399455195.png\n/kaggle/input/aptos2019/test_images/test_images/e6a58edc5b42.png\n/kaggle/input/aptos2019/test_images/test_images/e6a5e4718873.png\n/kaggle/input/aptos2019/test_images/test_images/f9156aeffc5e.png\n/kaggle/input/aptos2019/test_images/test_images/ef4121e9bb67.png\n/kaggle/input/aptos2019/test_images/test_images/f1dc26c4bfa3.png\n/kaggle/input/aptos2019/test_images/test_images/fe37f4492920.png\n/kaggle/input/aptos2019/test_images/test_images/f55e1d2a19e4.png\n/kaggle/input/aptos2019/test_images/test_images/f58d37d48e42.png\n/kaggle/input/aptos2019/test_images/test_images/ffa47f6a7bf4.png\n/kaggle/input/aptos2019/test_images/test_images/f0800723bc63.png\n/kaggle/input/aptos2019/test_images/test_images/f361060eda3e.png\n/kaggle/input/aptos2019/test_images/test_images/ec57cc20d776.png\n/kaggle/input/aptos2019/test_images/test_images/e9ce5bf645ab.png\n/kaggle/input/aptos2019/test_images/test_images/e582e56e7942.png\n/kaggle/input/aptos2019/test_images/test_images/e6552b7432b3.png\n/kaggle/input/aptos2019/test_images/test_images/ee02294cc3d9.png\n/kaggle/input/aptos2019/test_images/test_images/f91cfa82b9d4.png\n/kaggle/input/aptos2019/test_images/test_images/f233638e0e90.png\n/kaggle/input/aptos2019/test_images/test_images/eda1d75cbcf0.png\n/kaggle/input/aptos2019/test_images/test_images/f3a88d3026dc.png\n/kaggle/input/aptos2019/test_images/test_images/e7d2c2c3b30f.png\n/kaggle/input/aptos2019/test_images/test_images/f30f203ef51e.png\n/kaggle/input/aptos2019/test_images/test_images/ed3ce1674761.png\n/kaggle/input/aptos2019/test_images/test_images/f5e9a307288c.png\n/kaggle/input/aptos2019/test_images/test_images/f0c13be90519.png\n/kaggle/input/aptos2019/test_images/test_images/f4874247ede6.png\n/kaggle/input/aptos2019/test_images/test_images/e59c5f345bb0.png\n/kaggle/input/aptos2019/test_images/test_images/e9f82b5bbaf4.png\n/kaggle/input/aptos2019/test_images/test_images/f5a8c6426a71.png\n/kaggle/input/aptos2019/test_images/test_images/ef5155990874.png\n/kaggle/input/aptos2019/test_images/test_images/eb1ad14dd281.png\n/kaggle/input/aptos2019/test_images/test_images/ffcf7b45f213.png\n/kaggle/input/aptos2019/test_images/test_images/ec0c9f817b03.png\n/kaggle/input/aptos2019/test_images/test_images/e4e343eaae2a.png\n/kaggle/input/aptos2019/test_images/test_images/e966850247f4.png\n/kaggle/input/aptos2019/test_images/test_images/e96099b961b4.png\n/kaggle/input/aptos2019/test_images/test_images/f8d62557ad0c.png\n/kaggle/input/aptos2019/test_images/test_images/f2c0b41acd05.png\n/kaggle/input/aptos2019/test_images/test_images/ea9e0fb6fb0b.png\n/kaggle/input/aptos2019/test_images/test_images/eb175669d789.png\n/kaggle/input/aptos2019/test_images/test_images/e85d410d6836.png\n/kaggle/input/aptos2019/test_images/test_images/f85c78201a50.png\n/kaggle/input/aptos2019/test_images/test_images/ff4cd992667b.png\n/kaggle/input/aptos2019/test_images/test_images/f2d2a0c92034.png\n/kaggle/input/aptos2019/test_images/test_images/f3b6b7ca1eb1.png\n/kaggle/input/aptos2019/test_images/test_images/f7735b6d47f7.png\n/kaggle/input/aptos2019/test_images/test_images/eba3acc42197.png\n/kaggle/input/aptos2019/test_images/test_images/fb6b8200b7f8.png\n/kaggle/input/aptos2019/test_images/test_images/ec4649213ccf.png\n/kaggle/input/aptos2019/test_images/test_images/f7508f14dd7b.png\n/kaggle/input/aptos2019/test_images/test_images/fc603cbedb41.png\n/kaggle/input/aptos2019/test_images/test_images/f4c7ae514c54.png\n/kaggle/input/aptos2019/test_images/test_images/f531232ecb55.png\n/kaggle/input/aptos2019/test_images/test_images/f025f33b2c9b.png\n/kaggle/input/aptos2019/test_images/test_images/f9e1c439d4c8.png\n/kaggle/input/aptos2019/test_images/test_images/ea9c41e1ced0.png\n/kaggle/input/aptos2019/test_images/test_images/e81f4a2fbbdc.png\n/kaggle/input/aptos2019/test_images/test_images/e907d23cce3d.png\n/kaggle/input/aptos2019/test_images/test_images/ff4832d55461.png\n/kaggle/input/aptos2019/test_images/test_images/f460608cf4cc.png\n/kaggle/input/aptos2019/test_images/test_images/fca1a8738b8a.png\n/kaggle/input/aptos2019/test_images/test_images/fd87b6b2e664.png\n/kaggle/input/aptos2019/test_images/test_images/eb1d37b71fd1.png\n/kaggle/input/aptos2019/test_images/test_images/ff52392372d3.png\n/kaggle/input/aptos2019/test_images/test_images/fbdc796290d4.png\n/kaggle/input/aptos2019/test_images/test_images/f451eee2b66b.png\n/kaggle/input/aptos2019/test_images/test_images/ff0740cb484a.png\n/kaggle/input/aptos2019/test_images/test_images/fa0c87bd75ce.png\n/kaggle/input/aptos2019/test_images/test_images/ea4ce9516144.png\n/kaggle/input/aptos2019/test_images/test_images/f092febbf5c0.png\n/kaggle/input/aptos2019/test_images/test_images/ebe0175e530c.png\n/kaggle/input/aptos2019/test_images/test_images/e740af6ac6ea.png\n/kaggle/input/aptos2019/test_images/test_images/e52ed5c29c5e.png\n/kaggle/input/aptos2019/test_images/test_images/fca931da5c5e.png\n/kaggle/input/aptos2019/test_images/test_images/f080a22008be.png\n/kaggle/input/aptos2019/test_images/test_images/f3cd489acbee.png\n/kaggle/input/aptos2019/test_images/test_images/f999c6921e6d.png\n/kaggle/input/aptos2019/test_images/test_images/fd4c946c52bf.png\n/kaggle/input/aptos2019/test_images/test_images/f7defe70afc3.png\n/kaggle/input/aptos2019/test_images/test_images/fb767cea406c.png\n/kaggle/input/aptos2019/test_images/test_images/e5d56f4f359b.png\n/kaggle/input/aptos2019/test_images/test_images/ef7eb85b75fc.png\n/kaggle/input/aptos2019/test_images/test_images/ebd96d853918.png\n/kaggle/input/aptos2019/test_images/test_images/f2094a20b275.png\n/kaggle/input/aptos2019/test_images/test_images/efff2f1a35f5.png\n/kaggle/input/aptos2019/test_images/test_images/f90f8931a9bc.png\n/kaggle/input/aptos2019/test_images/test_images/e65a2ff90494.png\n/kaggle/input/aptos2019/test_images/test_images/f42b693a9414.png\n/kaggle/input/aptos2019/test_images/test_images/ebf4b22240f4.png\n/kaggle/input/aptos2019/test_images/test_images/e65f94ad9be3.png\n/kaggle/input/aptos2019/test_images/test_images/fdc685055659.png\n/kaggle/input/aptos2019/test_images/test_images/eadc57064154.png\n/kaggle/input/aptos2019/test_images/test_images/fe0fc67c7980.png\n/kaggle/input/aptos2019/test_images/test_images/ea7e21bab610.png\n/kaggle/input/aptos2019/test_images/test_images/e8d1c6c07cf2.png\n/kaggle/input/aptos2019/test_images/test_images/ee74c3b177e0.png\n/kaggle/input/aptos2019/test_images/test_images/f952ad2e4356.png\n/kaggle/input/aptos2019/test_images/test_images/f00ce9b9d6f4.png\n/kaggle/input/aptos2019/test_images/test_images/fb88783de055.png\n/kaggle/input/aptos2019/test_images/test_images/ef26625121b3.png\n/kaggle/input/aptos2019/test_images/test_images/ef48780f5d5f.png\n/kaggle/input/aptos2019/test_images/test_images/f8372e80f731.png\n/kaggle/input/aptos2019/test_images/test_images/f0f89314e860.png\n/kaggle/input/aptos2019/test_images/test_images/e76a9cbb2a8c.png\n/kaggle/input/aptos2019/test_images/test_images/f58f0b2fd718.png\n/kaggle/input/aptos2019/test_images/test_images/f5e6226bd2e0.png\n/kaggle/input/aptos2019/test_images/test_images/eabf421f94d0.png\n/kaggle/input/aptos2019/test_images/test_images/ffd97f8cd5aa.png\n/kaggle/input/aptos2019/test_images/test_images/fb1b8771c70a.png\n/kaggle/input/aptos2019/test_images/test_images/ffc04fed30e6.png\n/kaggle/input/aptos2019/test_images/test_images/ee1ec90b980f.png\n/kaggle/input/aptos2019/test_images/test_images/f2ee81781411.png\n/kaggle/input/aptos2019/test_images/test_images/ec6f1797a25a.png\n/kaggle/input/aptos2019/test_images/test_images/fce73678f650.png\n/kaggle/input/aptos2019/test_images/test_images/fe3f62695b2d.png\n/kaggle/input/aptos2019/test_images/test_images/ed2ef440d22c.png\n/kaggle/input/aptos2019/test_images/test_images/f72ef9ceeaa8.png\n/kaggle/input/aptos2019/test_images/test_images/e55188915f9d.png\n/kaggle/input/aptos2019/test_images/test_images/f4d3169b468a.png\n/kaggle/input/aptos2019/test_images/test_images/f576e45d1da2.png\n/kaggle/input/aptos2019/test_images/test_images/e6f0ce5bf282.png\n/kaggle/input/aptos2019/test_images/test_images/f5650eb52640.png\n/kaggle/input/aptos2019/test_images/test_images/fc4c2d35c6f8.png\n/kaggle/input/aptos2019/test_images/test_images/fdbc252813b1.png\n/kaggle/input/aptos2019/test_images/test_images/fd079d2e93a2.png\n/kaggle/input/aptos2019/test_images/test_images/f57cf3b6f48e.png\n/kaggle/input/aptos2019/test_images/test_images/f920ccd926db.png\n/kaggle/input/aptos2019/test_images/test_images/f002ce614c59.png\n/kaggle/input/aptos2019/test_images/test_images/f03d3c4ce7fb.png\n/kaggle/input/aptos2019/test_images/test_images/f5733f77273d.png\n/kaggle/input/aptos2019/test_images/test_images/fdd534271f3d.png\n/kaggle/input/aptos2019/test_images/test_images/e83d315d8f98.png\n/kaggle/input/aptos2019/test_images/test_images/f9e779a13204.png\n/kaggle/input/aptos2019/test_images/test_images/f8f5942b690e.png\n/kaggle/input/aptos2019/test_images/test_images/fa748b57262b.png\n/kaggle/input/aptos2019/test_images/test_images/f8cf7ed8ef00.png\n/kaggle/input/aptos2019/test_images/test_images/eb050765b323.png\n/kaggle/input/aptos2019/test_images/test_images/ea4dcb055139.png\n/kaggle/input/aptos2019/test_images/test_images/f7fec8935126.png\n/kaggle/input/aptos2019/test_images/test_images/fc4d69128e7c.png\n/kaggle/input/aptos2019/test_images/test_images/e5197d77ec68.png\n/kaggle/input/aptos2019/test_images/test_images/eeb231c3ef1f.png\n/kaggle/input/aptos2019/test_images/test_images/fe2df69676cf.png\n/kaggle/input/aptos2019/test_images/test_images/e7b5dd5bab1f.png\n/kaggle/input/aptos2019/test_images/test_images/ec01f0862669.png\n/kaggle/input/aptos2019/test_images/test_images/ecb4500285ed.png\n/kaggle/input/aptos2019/test_images/test_images/f901d460517c.png\n/kaggle/input/aptos2019/test_images/test_images/fa573163dd8b.png\n/kaggle/input/aptos2019/test_images/test_images/f298b7d05958.png\n/kaggle/input/aptos2019/test_images/test_images/e599151ca14b.png\n/kaggle/input/aptos2019/test_images/test_images/ee36ca728641.png\n/kaggle/input/aptos2019/test_images/test_images/fa59221cf464.png\n/kaggle/input/aptos2019/test_images/test_images/f4ea2a2cfbb9.png\n/kaggle/input/aptos2019/test_images/test_images/ff03f74667df.png\n/kaggle/input/aptos2019/test_images/test_images/f18abfa690ab.png\n/kaggle/input/aptos2019/test_images/test_images/f583a722434c.png\n/kaggle/input/aptos2019/test_images/test_images/e9ff9352ccb3.png\n/kaggle/input/aptos2019/test_images/test_images/f6f7dba7104d.png\n/kaggle/input/aptos2019/test_images/test_images/e756495c11cb.png\n/kaggle/input/aptos2019/test_images/test_images/ee78ce914066.png\n/kaggle/input/aptos2019/test_images/test_images/f2f569a64949.png\n/kaggle/input/aptos2019/test_images/test_images/f68690db78d3.png\n/kaggle/input/aptos2019/test_images/test_images/eebd1e195952.png\n/kaggle/input/aptos2019/test_images/test_images/f431f2e119d7.png\n/kaggle/input/aptos2019/test_images/test_images/ee6e39319b39.png\n/kaggle/input/aptos2019/test_images/test_images/f3bb996b45ce.png\n/kaggle/input/aptos2019/test_images/test_images/f066db7a2efe.png\n/kaggle/input/aptos2019/test_images/test_images/f45f1485c940.png\n/kaggle/input/aptos2019/test_images/test_images/f72adcac5638.png\n/kaggle/input/hiiiiii/best_model_try2_stage1.pth\n/kaggle/input/hiiiiii/best_model_final.pth\n/kaggle/input/hiiiiii/best_model_stage1.pth\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"        BASE_PATH = '/kaggle/input/aptos2019/'\n        train_csv_path = os.path.join(BASE_PATH, 'train_1.csv')\n        val_csv_path = os.path.join(BASE_PATH, 'valid.csv')\n        train_img_dir = os.path.join(BASE_PATH, 'train_images', 'train_images')\n        val_img_dir = os.path.join(BASE_PATH, 'val_images', 'val_images')\n\n        # Load dataframes using the provided train/validation split\n        train_df = pd.read_csv(train_csv_path)\n        val_df = pd.read_csv(val_csv_path)\n        train_df['diagnosis'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T07:13:11.804665Z","iopub.status.idle":"2025-09-10T07:13:11.804976Z","shell.execute_reply.started":"2025-09-10T07:13:11.804798Z","shell.execute_reply":"2025-09-10T07:13:11.804810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom PIL import Image\nimport gc\nimport os\nimport timm\nimport cv2\n\n# =============================================================================\n# DATASET CLASS\n# =============================================================================\n\nclass DiabeticRetinopathyDataset(Dataset):\n    \"\"\"Custom Dataset for Diabetic Retinopathy images.\"\"\"\n    def __init__(self, dataframe, img_dir, transform=None):\n        self.dataframe = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx]['id_code'] + '.png')\n        image = Image.open(img_name).convert('RGB')\n        label = torch.tensor(self.dataframe.iloc[idx]['diagnosis'], dtype=torch.long)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\n# =============================================================================\n# ADVANCED PREPROCESSING TRANSFORM\n# =============================================================================\n\nclass AdvancedBenGrahamPreprocess(object):\n    \"\"\"Applies a series of robust preprocessing steps for fundus images.\"\"\"\n    def __init__(self, output_size=256):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, image):\n        # Convert PIL image to numpy array\n        img_np = np.array(image)\n\n        # 1. Crop black borders\n        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            img_np = img_np[y:y+h, x:x+w]\n\n        # 2. Apply CLAHE to the green channel for contrast enhancement\n        b, g, r = cv2.split(img_np)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        g = clahe.apply(g)\n        img_clahe = cv2.merge((b, g, r))\n\n        # 3. Apply a gentle Gaussian blur to reduce noise\n        img_blur = cv2.GaussianBlur(img_clahe, (5, 5), 0)\n\n        # 4. Resize to target size\n        img_resized = cv2.resize(img_blur, self.output_size, interpolation=cv2.INTER_AREA)\n        \n        # Convert back to PIL Image\n        return Image.fromarray(img_resized)\n\n\n# =============================================================================\n# LOSS FUNCTIONS\n# =============================================================================\n\nclass OrdinalCrossEntropyLoss(nn.Module):\n    \"\"\"Cross Entropy Loss for Ordinal Regression.\"\"\"\n    def __init__(self, num_classes=5, class_weights=None):\n        super(OrdinalCrossEntropyLoss, self).__init__()\n        self.num_classes = num_classes\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n        self.class_weights = class_weights\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        outputs: (batch_size, num_classes-1) ordinal logits\n        targets: (batch_size,) class labels 0 to num_classes-1\n        \"\"\"\n        # Create ordinal targets\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, target in enumerate(targets):\n            if target > 0:\n                ordinal_targets[i, :target] = 1.0\n\n        losses = self.bce_loss(outputs, ordinal_targets)\n\n        if self.class_weights is not None:\n            weights = self.class_weights[targets]\n            return (losses.mean(dim=1) * weights).mean()\n        else:\n            return losses.mean()\n\nclass SmoothKappaLoss(nn.Module):\n    \"\"\"Smooth Quadratic Weighted Kappa Loss for Ordinal Regression.\"\"\"\n    def __init__(self, num_classes=5):\n        super(SmoothKappaLoss, self).__init__()\n        self.num_classes = num_classes\n\n        # Create quadratic weight matrix\n        weight_matrix = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                weight_matrix[i, j] = (i - j) ** 2\n        self.register_buffer('weight_matrix', weight_matrix)\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        outputs: (batch_size, num_classes-1) ordinal logits\n        targets: (batch_size,) class labels\n        \"\"\"\n        batch_size = outputs.size(0)\n\n        # Convert ordinal outputs to class probabilities\n        probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(batch_size, self.num_classes, device=outputs.device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes - 1):\n            class_probs[:, k] = probs[:, k-1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n\n        # Ensure probabilities are valid and sum to 1\n        class_probs = torch.clamp(class_probs, min=1e-7, max=1.0)\n        class_probs = class_probs / class_probs.sum(dim=1, keepdim=True)\n\n        # One-hot encode targets\n        one_hot_targets = F.one_hot(targets, num_classes=self.num_classes).float()\n\n        # Calculate observed agreement\n        observed_agreement = torch.sum(one_hot_targets * class_probs * self.weight_matrix[targets, :])\n\n        # Calculate expected agreement\n        true_marginals = one_hot_targets.sum(dim=0)\n        pred_marginals = class_probs.sum(dim=0)\n        expected_outer = torch.outer(true_marginals, pred_marginals)\n        expected_agreement = torch.sum(expected_outer * self.weight_matrix)\n\n        # QWK is 1 - (observed / expected), so loss is observed / (expected + epsilon)\n        loss = observed_agreement / (expected_agreement + 1e-7)\n        return loss\n\n# =============================================================================\n# MODEL\n# =============================================================================\n\nclass EfficientNetOrdinal(nn.Module):\n    \"\"\"EfficientNet for Ordinal Regression.\"\"\"\n    def __init__(self, model_name='efficientnet_b2', num_classes=5, pretrained=True):\n        super(EfficientNetOrdinal, self).__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,  # Remove classification head\n            global_pool='avg'\n        )\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes - 1)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        logits = self.classifier(features)\n        return logits\n\n# =============================================================================\n# UTILITY FUNCTIONS\n# =============================================================================\n\ndef ordinal_to_class(outputs):\n    \"\"\"Convert ordinal outputs to class predictions.\"\"\"\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\ndef calculate_metrics(outputs, targets):\n    \"\"\"Calculate accuracy, QWK, and within-1 accuracy.\"\"\"\n    preds = ordinal_to_class(outputs).cpu().numpy()\n    targets_np = targets.cpu().numpy()\n    accuracy = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    within1 = np.mean(np.abs(targets_np - preds) <= 1)\n    return accuracy, qwk, within1\n\ndef clear_memory():\n    \"\"\"Clear GPU memory.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# =============================================================================\n# TRAINING & VALIDATION LOOPS\n# =============================================================================\n\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    \"\"\"Train one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    all_outputs, all_targets = [], []\n\n    for images, targets in train_loader:\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        all_outputs.append(outputs.detach())\n        all_targets.append(targets.detach())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    accuracy, qwk, within1 = calculate_metrics(all_outputs, all_targets)\n    return running_loss / len(train_loader), accuracy, qwk, within1\n\ndef validate_epoch(model, val_loader, criterion, device):\n    \"\"\"Validate one epoch.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    all_outputs, all_targets = [], []\n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_outputs.append(outputs)\n            all_targets.append(targets)\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    accuracy, qwk, within1 = calculate_metrics(all_outputs, all_targets)\n    return running_loss / len(val_loader), accuracy, qwk, within1\n\n# =============================================================================\n# MAIN TRAINING PIPELINE\n# =============================================================================\n\ndef main_training_pipeline(train_loader, val_loader, train_df, device):\n    \"\"\"Complete 2-stage training pipeline.\"\"\"\n    print(\"--> STARTING ORDINAL REGRESSION TRAINING\")\n    print(\"=\" * 60)\n\n    # MODEL AND LOSSES\n    model = EfficientNetOrdinal('efficientnet_b2', num_classes=5).to(device)\n    print(f\"[INFO] Model created: EfficientNet-B2 with ordinal head\")\n\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    class_weights = torch.FloatTensor(class_weights).to(device)\n    print(f\"Class weights: {class_weights.cpu().numpy().round(2)}\")\n\n    ce_loss = OrdinalCrossEntropyLoss(num_classes=5, class_weights=class_weights)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    scaler = torch.amp.GradScaler('cuda')\n\n #   # STAGE 1: CROSS ENTROPY TRAINING\n #   print(\"\\n--> STAGE 1: CROSS ENTROPY TRAINING (30 EPOCHS)\")\n  #  print(\"=\" * 60)\n  #  optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n #   scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n  #  best_qwk_stage1 = 0\n  #  patience, patience_counter = 10, 0\n\n  #  for epoch in range(30):\n   #     clear_memory()\n  #      train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, optimizer, ce_loss, scaler, device)\n   #     val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, ce_loss, device)\n  #      scheduler.step()\n\n   #     print(f\"Epoch {epoch+1}/30:\")\n   #     print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, QWK={train_qwk:.4f}, \\u00b11={train_within1:.4f}\")\n   #     print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, QWK={val_qwk:.4f}, \\u00b11={val_within1:.4f}\")\n\n   #     if val_qwk > best_qwk_stage1:\n     #       best_qwk_stage1 = val_qwk\n     #       torch.save(model.state_dict(), 'best_model_stage1.pth')\n     #       patience_counter = 0\n     #       print(f\"  [SAVE] New best QWK: {best_qwk_stage1:.4f}. Model saved!\")\n    #    else:\n     #       patience_counter += 1\n\n     #   if patience_counter >= patience:\n       #     print(f\"Early stopping at epoch {epoch+1}\")\n     #       break\n\n    print(f\"\\n[INFO] Stage 1 completed! Best QWK: {0.9051}\")\n    model.load_state_dict(torch.load('best_model_stage1.pth'))\n\n    # STAGE 2: KAPPA LOSS FINE-TUNING\n    print(\"\\n--> STAGE 2: KAPPA LOSS FINE-TUNING (30 EPOCHS)\")\n    print(\"=\" * 60)\n    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n    best_qwk_stage2 = 0.9051\n    patience_counter = 0\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, optimizer, kappa_loss, scaler, device)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, kappa_loss, device)\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/30:\")\n        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, QWK={train_qwk:.4f}, \\u00b11={train_within1:.4f}\")\n        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, QWK={val_qwk:.4f}, \\u00b11={val_within1:.4f}\")\n\n        if val_qwk > best_qwk_stage2:\n            best_qwk_stage2 = val_qwk\n            torch.save(model.state_dict(), 'best_model_final.pth')\n            patience_counter = 0\n            print(f\"  [SAVE] New best QWK: {best_qwk_stage2:.4f}. Model saved!\")\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"\\n--> TRAINING COMPLETED!\")\n    print(\"=\" * 60)\n    print(f\"Stage 1 Best QWK: {best_qwk_stage1:.4f}\")\n    print(f\"Stage 2 Best QWK: {best_qwk_stage2:.4f}\")\n    print(f\"Improvement: {best_qwk_stage2 - best_qwk_stage1:.4f}\")\n\n    model.load_state_dict(torch.load('best_model_final.pth'))\n    return model\n\n# =============================================================================\n# RUN TRAINING\n# =============================================================================\n\nif __name__ == \"__main__\":\n    # --- CONFIGURATION ---\n    IMG_SIZE = 256\n    BATCH_SIZE = 16\n    NUM_WORKERS = 2 # On Kaggle, 2 is often a good choice\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Enable memory management for PyTorch\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n    # --- DATA PREPARATION ---\n    # This is an example. Replace with your actual data paths and dataframe loading.\n    # On Kaggle, paths are typically like '/kaggle/input/aptos2019-blindness-detection/'\n\n    try:\n        # Define paths based on the screenshot\n        BASE_PATH = '/kaggle/input/aptos2019/'\n        train_csv_path = os.path.join(BASE_PATH, 'train_1.csv')\n        val_csv_path = os.path.join(BASE_PATH, 'valid.csv')\n        train_img_dir = os.path.join(BASE_PATH, 'train_images', 'train_images')\n        val_img_dir = os.path.join(BASE_PATH, 'val_images', 'val_images')\n\n        # Load dataframes using the provided train/validation split\n        train_df = pd.read_csv(train_csv_path)\n        val_df = pd.read_csv(val_csv_path)\n\n        print(f\"Training data: {len(train_df)} samples\")\n        print(f\"Validation data: {len(val_df)} samples\")\n        print(f\"Class distribution in training:\\n{train_df['diagnosis'].value_counts().sort_index()}\")\n\n        # --- DATA AUGMENTATION & LOADERS ---\n        train_transforms = transforms.Compose([\n            AdvancedBenGrahamPreprocess(output_size=IMG_SIZE),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        val_transforms = transforms.Compose([\n            AdvancedBenGrahamPreprocess(output_size=IMG_SIZE),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        # Create separate datasets for training and validation\n        train_dataset = DiabeticRetinopathyDataset(train_df, train_img_dir, transform=train_transforms)\n        val_dataset = DiabeticRetinopathyDataset(val_df, val_img_dir, transform=val_transforms)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n        # --- RUN TRAINING ---\n        final_model = main_training_pipeline(train_loader, val_loader, train_df, device)\n\n        print(\"\\n[SUCCESS] Training pipeline completed successfully!\")\n        print(\"Models saved:\")\n        print(\" - best_model_stage1.pth (after cross-entropy training)\")\n        print(\" - best_model_final.pth (after kappa loss fine-tuning)\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(\"[NOTE] Please ensure your data paths are correct and you have run this in an environment with the data.\")\n\n    print(\"[INFO] To run the training, uncomment the code block in `if __name__ == '__main__':`\")\n    print(\"1. Set your BASE_PATH to the correct data directory.\")\n    print(\"2. Ensure your CSV file has 'id_code' and 'diagnosis' columns.\")\n    print(\"3. Run the script.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T07:13:11.805966Z","iopub.status.idle":"2025-09-10T07:13:11.806336Z","shell.execute_reply.started":"2025-09-10T07:13:11.806168Z","shell.execute_reply":"2025-09-10T07:13:11.806185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/working\"))   # check working directory\nwith open(\"/kaggle/working/\") as f:\n    data = f.read()\n    print(data[:200])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T07:13:11.807085Z","iopub.status.idle":"2025-09-10T07:13:11.807347Z","shell.execute_reply.started":"2025-09-10T07:13:11.807235Z","shell.execute_reply":"2025-09-10T07:13:11.807249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom PIL import Image\nimport gc\nimport os\nimport timm\nimport cv2\n\n# =============================================================================\n# DATASET CLASS\n# =============================================================================\n\nclass DiabeticRetinopathyDataset(Dataset):\n    \"\"\"Custom Dataset for Diabetic Retinopathy images.\"\"\"\n    def __init__(self, dataframe, img_dir, transform=None):\n        self.dataframe = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx]['id_code'] + '.png')\n        image = Image.open(img_name).convert('RGB')\n        label = torch.tensor(self.dataframe.iloc[idx]['diagnosis'], dtype=torch.long)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\n# =============================================================================\n# ADVANCED PREPROCESSING TRANSFORM\n# =============================================================================\n\nclass AdvancedBenGrahamPreprocess(object):\n    \"\"\"Applies a series of robust preprocessing steps for fundus images.\"\"\"\n    def __init__(self, output_size=256):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, image):\n        # Convert PIL image to numpy array\n        img_np = np.array(image)\n\n        # 1. Crop black borders\n        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            img_np = img_np[y:y+h, x:x+w]\n\n        # 2. Apply CLAHE to the green channel for contrast enhancement\n        b, g, r = cv2.split(img_np)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        g = clahe.apply(g)\n        img_clahe = cv2.merge((b, g, r))\n\n        # 3. Apply a gentle Gaussian blur to reduce noise\n        img_blur = cv2.GaussianBlur(img_clahe, (5, 5), 0)\n\n        # 4. Resize to target size\n        img_resized = cv2.resize(img_blur, self.output_size, interpolation=cv2.INTER_AREA)\n        \n        # Convert back to PIL Image\n        return Image.fromarray(img_resized)\n\n\n# =============================================================================\n# LOSS FUNCTIONS\n# =============================================================================\n\nclass OrdinalCrossEntropyLoss(nn.Module):\n    \"\"\"Cross Entropy Loss for Ordinal Regression.\"\"\"\n    def __init__(self, num_classes=5, class_weights=None):\n        super(OrdinalCrossEntropyLoss, self).__init__()\n        self.num_classes = num_classes\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n        self.class_weights = class_weights\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        outputs: (batch_size, num_classes-1) ordinal logits\n        targets: (batch_size,) class labels 0 to num_classes-1\n        \"\"\"\n        # Create ordinal targets\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, target in enumerate(targets):\n            if target > 0:\n                ordinal_targets[i, :target] = 1.0\n\n        losses = self.bce_loss(outputs, ordinal_targets)\n\n        if self.class_weights is not None:\n            weights = self.class_weights[targets]\n            return (losses.mean(dim=1) * weights).mean()\n        else:\n            return losses.mean()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SmoothKappaLoss(nn.Module):\n    \"\"\"\n    Differentiable approximation of Quadratic Weighted Kappa (QWK) loss.\n    Works with ordinal regression outputs (num_classes - 1 logits).\n    \"\"\"\n    def __init__(self, num_classes=5, eps=1e-7):\n        super(SmoothKappaLoss, self).__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n\n        # Precompute quadratic weights (normalized)\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                W[i, j] = ((i - j) ** 2) / ((num_classes - 1) ** 2)\n        self.register_buffer(\"W\", W)  # buffer ensures same device as model\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        outputs: (batch_size, num_classes-1) ordinal logits\n        targets: (batch_size,) ground-truth labels\n        \"\"\"\n        device = outputs.device\n        B = outputs.size(0)\n\n        # Convert ordinal outputs -> class probabilities\n        probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes - 1):\n            class_probs[:, k] = probs[:, k - 1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        class_probs = class_probs / class_probs.sum(dim=1, keepdim=True)\n\n        # One-hot encode targets (make sure it's on the same device)\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n\n        # Confusion matrix (soft)\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        conf_mat = conf_mat / (conf_mat.sum() + self.eps)\n\n        # Expected agreement\n        hist_true = one_hot.sum(dim=0)\n        hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        expected = expected / (expected.sum() + self.eps)\n\n        # Weighted observed & expected\n        W = self.W.to(device)  # ✅ force on same device\n        obs = torch.sum(W * conf_mat)\n        exp = torch.sum(W * expected)\n\n        # Quadratic Kappa\n        kappa = 1.0 - obs / (exp + self.eps)\n\n        # Loss (want to maximize kappa, so minimize 1 - kappa)\n        return 1.0 - kappa\n\n\n\n# =============================================================================\n# MODEL\n# =============================================================================\n\nclass EfficientNetOrdinal(nn.Module):\n    \"\"\"EfficientNet for Ordinal Regression.\"\"\"\n    def __init__(self, model_name='efficientnet_b2', num_classes=5, pretrained=True):\n        super(EfficientNetOrdinal, self).__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,  # Remove classification head\n            global_pool='avg'\n        )\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes - 1)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        logits = self.classifier(features)\n        return logits\n\n# =============================================================================\n# UTILITY FUNCTIONS\n# =============================================================================\n\ndef ordinal_to_class(outputs):\n    \"\"\"Convert ordinal outputs to class predictions.\"\"\"\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\ndef calculate_metrics(outputs, targets):\n    \"\"\"Calculate accuracy, QWK, and within-1 accuracy.\"\"\"\n    preds = ordinal_to_class(outputs).cpu().numpy()\n    targets_np = targets.cpu().numpy()\n    accuracy = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    within1 = np.mean(np.abs(targets_np - preds) <= 1)\n    return accuracy, qwk, within1\n\ndef clear_memory():\n    \"\"\"Clear GPU memory.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# =============================================================================\n# TRAINING & VALIDATION LOOPS\n# =============================================================================\n\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    \"\"\"Train one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    all_outputs, all_targets = [], []\n\n    for images, targets in train_loader:\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        all_outputs.append(outputs.detach())\n        all_targets.append(targets.detach())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    accuracy, qwk, within1 = calculate_metrics(all_outputs, all_targets)\n    return running_loss / len(train_loader), accuracy, qwk, within1\n\ndef validate_epoch(model, val_loader, criterion, device):\n    \"\"\"Validate one epoch.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    all_outputs, all_targets = [], []\n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_outputs.append(outputs)\n            all_targets.append(targets)\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    accuracy, qwk, within1 = calculate_metrics(all_outputs, all_targets)\n    return running_loss / len(val_loader), accuracy, qwk, within1\n\n# =============================================================================\n# MAIN TRAINING PIPELINE\n# =============================================================================\n\ndef main_training_pipeline(train_loader, val_loader, train_df, device):\n    \"\"\"Complete 2-stage training pipeline.\"\"\"\n    print(\"--> STARTING ORDINAL REGRESSION TRAINING\")\n    print(\"=\" * 60)\n\n    # MODEL AND LOSSES\n    model = EfficientNetOrdinal('efficientnet_b2', num_classes=5).to(device)\n    print(f\"[INFO] Model created: EfficientNet-B2 with ordinal head\")\n\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    class_weights = torch.FloatTensor(class_weights).to(device)\n    print(f\"Class weights: {class_weights.cpu().numpy().round(2)}\")\n\n    ce_loss = OrdinalCrossEntropyLoss(num_classes=5, class_weights=class_weights)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    scaler = torch.amp.GradScaler('cuda')\n\n    # STAGE 1: CROSS ENTROPY TRAINING\n    if (0) :\n        print(\"\\n--> STAGE 1: CROSS ENTROPY TRAINING (30 EPOCHS)\")\n        print(\"=\" * 60)\n        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n        best_qwk_stage1 = 0\n        patience, patience_counter = 10, 0\n    \n        for epoch in range(30):\n            clear_memory()\n            train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, optimizer, ce_loss, scaler, device)\n            val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, ce_loss, device)\n            scheduler.step()\n    \n            print(f\"Epoch {epoch+1}/30:\")\n            print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, QWK={train_qwk:.4f}, \\u00b11={train_within1:.4f}\")\n            print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, QWK={val_qwk:.4f}, \\u00b11={val_within1:.4f}\")\n    \n            if val_qwk > best_qwk_stage1:\n                best_qwk_stage1 = val_qwk\n                torch.save(model.state_dict(), 'best_model_stage1.pth')\n                patience_counter = 0\n                print(f\"  [SAVE] New best QWK: {best_qwk_stage1:.4f}. Model saved!\")\n            else:\n                patience_counter += 1\n    \n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    print(f\"\\n[INFO] Stage 1 completed! Best QWK: {0.9051}\")\n    model.load_state_dict(torch.load('best_model_stage1.pth'))\n\n    # STAGE 2: KAPPA LOSS FINE-TUNING\n    print(\"\\n--> STAGE 2: KAPPA LOSS FINE-TUNING (30 EPOCHS)\")\n    print(\"=\" * 60)\n    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n    best_qwk_stage2 = 0.9051\n    patience_counter = 0\n    patience=10\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, optimizer, kappa_loss, scaler, device)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, kappa_loss, device)\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/30:\")\n        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, QWK={train_qwk:.4f}, \\u00b11={train_within1:.4f}\")\n        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, QWK={val_qwk:.4f}, \\u00b11={val_within1:.4f}\")\n\n        if val_qwk > best_qwk_stage2:\n            best_qwk_stage2 = val_qwk\n            torch.save(model.state_dict(), 'best_model_final.pth')\n            patience_counter = 0\n            print(f\"  [SAVE] New best QWK: {best_qwk_stage2:.4f}. Model saved!\")\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"\\n--> TRAINING COMPLETED!\")\n    print(\"=\" * 60)\n    print(f\"Stage 1 Best QWK: {best_qwk_stage1:.4f}\")\n    print(f\"Stage 2 Best QWK: {best_qwk_stage2:.4f}\")\n    print(f\"Improvement: {best_qwk_stage2 - best_qwk_stage1:.4f}\")\n\n    model.load_state_dict(torch.load('best_model_final.pth'))\n    return model\n\n# =============================================================================\n# RUN TRAINING\n# =============================================================================\n\nif __name__ == \"__main__\":\n    # --- CONFIGURATION ---\n    IMG_SIZE = 256\n    BATCH_SIZE = 16\n    NUM_WORKERS = 2 # On Kaggle, 2 is often a good choice\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Enable memory management for PyTorch\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n    # --- DATA PREPARATION ---\n    # This is an example. Replace with your actual data paths and dataframe loading.\n    # On Kaggle, paths are typically like '/kaggle/input/aptos2019-blindness-detection/'\n\n    try:\n        # Define paths based on the screenshot\n        BASE_PATH = '/kaggle/input/aptos2019/'\n        train_csv_path = os.path.join(BASE_PATH, 'train_1.csv')\n        val_csv_path = os.path.join(BASE_PATH, 'valid.csv')\n        train_img_dir = os.path.join(BASE_PATH, 'train_images', 'train_images')\n        val_img_dir = os.path.join(BASE_PATH, 'val_images', 'val_images')\n\n        # Load dataframes using the provided train/validation split\n        train_df = pd.read_csv(train_csv_path)\n        val_df = pd.read_csv(val_csv_path)\n\n        print(f\"Training data: {len(train_df)} samples\")\n        print(f\"Validation data: {len(val_df)} samples\")\n        print(f\"Class distribution in training:\\n{train_df['diagnosis'].value_counts().sort_index()}\")\n\n        # --- DATA AUGMENTATION & LOADERS ---\n        train_transforms = transforms.Compose([\n            AdvancedBenGrahamPreprocess(output_size=IMG_SIZE),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        val_transforms = transforms.Compose([\n            AdvancedBenGrahamPreprocess(output_size=IMG_SIZE),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        # Create separate datasets for training and validation\n        train_dataset = DiabeticRetinopathyDataset(train_df, train_img_dir, transform=train_transforms)\n        val_dataset = DiabeticRetinopathyDataset(val_df, val_img_dir, transform=val_transforms)\n\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n        # --- RUN TRAINING ---\n        final_model = main_training_pipeline(train_loader, val_loader, train_df, device)\n\n        print(\"\\n[SUCCESS] Training pipeline completed successfully!\")\n        print(\"Models saved:\")\n        print(\" - best_model_stage1.pth (after cross-entropy training)\")\n        print(\" - best_model_final.pth (after kappa loss fine-tuning)\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(\"[NOTE] Please ensure your data paths are correct and you have run this in an environment with the data.\")\n\n    print(\"[INFO] To run the training, uncomment the code block in `if __name__ == '__main__':`\")\n    print(\"1. Set your BASE_PATH to the correct data directory.\")\n    print(\"2. Ensure your CSV file has 'id_code' and 'diagnosis' columns.\")\n    print(\"3. Run the script.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T07:13:11.808228Z","iopub.status.idle":"2025-09-10T07:13:11.808441Z","shell.execute_reply.started":"2025-09-10T07:13:11.808339Z","shell.execute_reply":"2025-09-10T07:13:11.808348Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2nd try   \nstage 1=> BCE\nstage 2=> kappa loss\n\n+ extra preprocessing steps","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import WeightedRandomSampler\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom PIL import Image\nimport gc\nimport os\nimport timm\nimport cv2\n\n# =============================================================================\n# DATASET CLASS\n# =============================================================================\nclass DiabeticRetinopathyDataset(Dataset):\n    \"\"\"Custom Dataset for Diabetic Retinopathy images.\"\"\"\n    def __init__(self, dataframe, img_dir, transform=None):\n        self.dataframe = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx]['id_code'] + '.png')\n        image = Image.open(img_name).convert('RGB')\n        label = torch.tensor(self.dataframe.iloc[idx]['diagnosis'], dtype=torch.long)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# =============================================================================\n# ADVANCED PREPROCESSING TRANSFORM\n# =============================================================================\nclass AdvancedBenGrahamPreprocess(object):\n    \"\"\"Applies a series of robust preprocessing steps for fundus images.\"\"\"\n    def __init__(self, output_size=256):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, image):\n        img_np = np.array(image)\n        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            img_np = img_np[y:y+h, x:x+w]\n        b, g, r = cv2.split(img_np)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        g = clahe.apply(g)\n        img_clahe = cv2.merge((b, g, r))\n        img_blur = cv2.GaussianBlur(img_clahe, (5, 5), 0)\n        img_resized = cv2.resize(img_blur, self.output_size, interpolation=cv2.INTER_AREA)\n        return Image.fromarray(img_resized)\n\n# =============================================================================\n# LOSS FUNCTIONS\n# =============================================================================\nclass OrdinalCrossEntropyLoss(nn.Module):\n    \"\"\"Cross Entropy Loss for Ordinal Regression.\"\"\"\n    def __init__(self, num_classes=5, class_weights=None):\n        super(OrdinalCrossEntropyLoss, self).__init__()\n        self.num_classes = num_classes\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n        self.class_weights = class_weights\n\n    def forward(self, outputs, targets):\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, target in enumerate(targets):\n            if target > 0:\n                ordinal_targets[i, :target] = 1.0\n        losses = self.bce_loss(outputs, ordinal_targets)\n        if self.class_weights is not None:\n            weights = self.class_weights[targets]\n            return (losses.mean(dim=1) * weights).mean()\n        else:\n            return losses.mean()\n            \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SmoothKappaLoss(nn.Module):\n    \"\"\"\n    Differentiable approximation of Quadratic Weighted Kappa (QWK) loss.\n    \"\"\"\n    def __init__(self, num_classes=5, eps=1e-7):\n        super(SmoothKappaLoss, self).__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                W[i, j] = ((i - j) ** 2) / ((num_classes - 1) ** 2)\n        self.register_buffer(\"W\", W)\n\n    def forward(self, outputs, targets):\n        device = outputs.device\n        B = outputs.size(0)\n        probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes - 1):\n            class_probs[:, k] = probs[:, k - 1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        class_probs = class_probs / class_probs.sum(dim=1, keepdim=True)\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        conf_mat = conf_mat / (conf_mat.sum() + self.eps)\n        hist_true = one_hot.sum(dim=0)\n        hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        expected = expected / (expected.sum() + self.eps)\n        W = self.W.to(device)\n        obs = torch.sum(W * conf_mat)\n        exp = torch.sum(W * expected)\n        kappa = 1.0 - obs / (exp + self.eps)\n        return 1.0 - kappa\n\n# =============================================================================\n# MODEL\n# =============================================================================\nclass EfficientNetOrdinal(nn.Module):\n    \"\"\"EfficientNet for Ordinal Regression.\"\"\"\n    def __init__(self, model_name='efficientnet_b2', num_classes=5, pretrained=True):\n        super(EfficientNetOrdinal, self).__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool='avg'\n        )\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.4), # Increased dropout for better regularization\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3), # Increased dropout for better regularization\n            nn.Linear(256, num_classes - 1)\n        )\n    def forward(self, x):\n        features = self.backbone(x)\n        logits = self.classifier(features)\n        return logits\n\n# =============================================================================\n# UTILITY FUNCTIONS\n# =============================================================================\ndef ordinal_to_class(outputs):\n    \"\"\"Convert ordinal outputs to class predictions.\"\"\"\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\ndef calculate_metrics(outputs, targets):\n    \"\"\"Calculate accuracy, QWK, and within-1 accuracy.\"\"\"\n    preds = ordinal_to_class(outputs).cpu().numpy()\n    targets_np = targets.cpu().numpy()\n    accuracy = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    within1 = np.mean(np.abs(targets_np - preds) <= 1)\n    return accuracy, qwk, within1\n\ndef clear_memory():\n    \"\"\"Clear GPU memory.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    \ndef mixup_data(x, y, alpha=0.4):\n    '''Returns mixed inputs, targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n# =============================================================================\n# TRAINING & VALIDATION LOOPS - MODIFIED\n# =============================================================================\ndef train_epoch(model, train_loader, optimizer, criterion, scaler, device):\n    \"\"\"Train one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    all_outputs, all_targets = [], []\n    is_ce_loss = isinstance(criterion, OrdinalCrossEntropyLoss)\n\n    for images, targets in train_loader:\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad()\n        if is_ce_loss:\n            images, targets_a, targets_b, lam = mixup_data(images, targets)\n        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = model(images)\n            if is_ce_loss:\n                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n            else:\n                loss = criterion(outputs, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        all_outputs.append(outputs.detach())\n        all_targets.append(targets.detach())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    accuracy, qwk, within1 = calculate_metrics(all_outputs, all_targets)\n    return running_loss / len(train_loader), accuracy, qwk, within1\n\ndef validate_epoch(model, val_loader, criterion, device):\n    \"\"\"Validate one epoch.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    all_outputs, all_targets = [], []\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_outputs.append(outputs)\n            all_targets.append(targets)\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    accuracy, qwk, within1 = calculate_metrics(all_outputs, all_targets)\n    return running_loss / len(val_loader), accuracy, qwk, within1\n\n# =============================================================================\n# MAIN TRAINING PIPELINE\n# =============================================================================\ndef main_training_pipeline(train_loader, val_loader, train_df, device):\n    \"\"\"Complete 2-stage training pipeline.\"\"\"\n    print(\"--> STARTING ORDINAL REGRESSION TRAINING\")\n    print(\"=\" * 60)\n    model = EfficientNetOrdinal('efficientnet_b2', num_classes=5).to(device)\n    print(f\"[INFO] Model created: EfficientNet-B2 with ordinal head\")\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    class_weights = torch.FloatTensor(class_weights).to(device)\n    print(f\"Class weights: {class_weights.cpu().numpy().round(2)}\")\n    ce_loss = OrdinalCrossEntropyLoss(num_classes=5, class_weights=class_weights)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    scaler = torch.amp.GradScaler('cuda')\n\n    # STAGE 1: CROSS ENTROPY TRAINING - **FIXED**\n    if (0):\n        print(\"\\n--> STAGE 1: CROSS ENTROPY TRAINING (30 EPOCHS)\")\n        print(\"=\" * 60)\n        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=1e-3,\n            steps_per_epoch=len(train_loader),\n            epochs=30\n        )\n        best_qwk_stage1 = 0\n        patience, patience_counter = 10, 0\n        \n        for epoch in range(30):\n            clear_memory()\n            train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, optimizer, ce_loss, scaler, device)\n            val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, ce_loss, device)\n            scheduler.step()\n            \n            print(f\"Epoch {epoch+1}/30:\")\n            print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, QWK={train_qwk:.4f}, \\u00b11={train_within1:.4f}\")\n            print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, QWK={val_qwk:.4f}, \\u00b11={val_within1:.4f}\")\n            \n            if val_qwk > best_qwk_stage1:\n                best_qwk_stage1 = val_qwk\n                torch.save(model.state_dict(), 'best_model_try2_stage1.pth')\n                patience_counter = 0\n                print(f\"  [SAVE] New best QWK: {best_qwk_stage1:.4f}. Model saved!\")\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    print(f\"\\n[INFO] Stage 1 completed! Best QWK: {0.8984:.4f}\")\n    model.load_state_dict(torch.load('/kaggle/input/hiiiiii/best_model_try2_stage1.pth'))\n#######################################################\n    # STAGE 2: KAPPA LOSS FINE-TUNING\n    print(\"\\n--> STAGE 2: KAPPA LOSS FINE-TUNING (30 EPOCHS)\")\n    print(\"=\" * 60)\n    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=1e-4,\n        steps_per_epoch=len(train_loader),\n        epochs=30\n    )\n    best_qwk_stage2 = 0.8984\n    patience_counter = 0\n    patience=10\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, optimizer, kappa_loss, scaler, device)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, kappa_loss, device)\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/30:\")\n        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, QWK={train_qwk:.4f}, \\u00b11={train_within1:.4f}\")\n        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, QWK={val_qwk:.4f}, \\u00b11={val_within1:.4f}\")\n\n        if val_qwk > best_qwk_stage2:\n            best_qwk_stage2 = val_qwk\n            torch.save(model.state_dict(), 'best_model_final_try2.pth')\n            patience_counter = 0\n            print(f\"  [SAVE] New best QWK: {best_qwk_stage2:.4f}. Model saved!\")\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"\\n--> TRAINING COMPLETED!\")\n    print(\"=\" * 60)\n    print(f\"Stage 1 Best QWK: {best_qwk_stage1:.4f}\")\n    print(f\"Stage 2 Best QWK: {best_qwk_stage2:.4f}\")\n    print(f\"Improvement: {best_qwk_stage2 - best_qwk_stage1:.4f}\")\n\n    if os.path.exists('best_model_final_try2.pth'):\n        model.load_state_dict(torch.load('best_model_final_try2.pth'))\n    else:\n        print(\"Warning: Final model not found. Returning last model state.\")\n        \n    return model\n\n# =============================================================================\n# RUN TRAINING - FINAL\n# =============================================================================\nif __name__ == \"__main__\":\n    # --- CONFIGURATION ---\n    IMG_SIZE = 256\n    BATCH_SIZE = 16\n    NUM_WORKERS = 2 \n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    \n    try:\n        # --- CORRECTED DATA PATHS ---\n        BASE_PATH = '/kaggle/input/aptos2019/'\n        train_csv_path = os.path.join(BASE_PATH, 'train_1.csv')\n        val_csv_path = os.path.join(BASE_PATH, 'valid.csv')\n        train_img_dir = os.path.join(BASE_PATH, 'train_images', 'train_images')\n        val_img_dir = os.path.join(BASE_PATH, 'val_images', 'val_images')\n\n        train_df = pd.read_csv(train_csv_path)\n        val_df = pd.read_csv(val_csv_path)\n        \n        print(f\"Training data: {len(train_df)} samples\")\n        print(f\"Validation data: {len(val_df)} samples\")\n        print(f\"Class distribution in training:\\n{train_df['diagnosis'].value_counts().sort_index()}\")\n\n        # --- Weighted Random Sampler (for imbalanced data) ---\n        class_counts = train_df['diagnosis'].value_counts().sort_index()\n        num_samples = len(train_df)\n        class_weights_sampler = 1.0 / class_counts.values\n        sample_weights = np.array([class_weights_sampler[t] for t in train_df['diagnosis']])\n        sample_weights = torch.from_numpy(sample_weights).double()\n        \n        train_sampler = WeightedRandomSampler(\n            weights=sample_weights,\n            num_samples=num_samples,\n            replacement=True\n        )\n\n        # --- DATA AUGMENTATION & LOADERS ---\n        train_transforms = transforms.Compose([\n            AdvancedBenGrahamPreprocess(output_size=IMG_SIZE),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        val_transforms = transforms.Compose([\n            AdvancedBenGrahamPreprocess(output_size=IMG_SIZE),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        train_dataset = DiabeticRetinopathyDataset(train_df, train_img_dir, transform=train_transforms)\n        val_dataset = DiabeticRetinopathyDataset(val_df, val_img_dir, transform=val_transforms)\n\n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=BATCH_SIZE, \n            sampler=train_sampler,\n            num_workers=NUM_WORKERS, \n            pin_memory=True\n        )\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n        final_model = main_training_pipeline(train_loader, val_loader, train_df, device)\n        print(\"\\n[SUCCESS] Training pipeline completed successfully!\")\n        \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(\"[NOTE] Please ensure your data paths are correct and the required libraries are installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T07:13:59.460295Z","iopub.execute_input":"2025-09-10T07:13:59.460606Z","iopub.status.idle":"2025-09-10T08:13:43.608541Z","shell.execute_reply.started":"2025-09-10T07:13:59.460583Z","shell.execute_reply":"2025-09-10T08:13:43.607187Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining data: 2930 samples\nValidation data: 366 samples\nClass distribution in training:\ndiagnosis\n0    1434\n1     300\n2     808\n3     154\n4     234\nName: count, dtype: int64\n--> STARTING ORDINAL REGRESSION TRAINING\n============================================================\n[INFO] Model created: EfficientNet-B2 with ordinal head\nClass weights: [0.41 1.95 0.73 3.81 2.5 ]\n\n[INFO] Stage 1 completed! Best QWK: 0.8984\n\n--> STAGE 2: KAPPA LOSS FINE-TUNING (30 EPOCHS)\n============================================================\nEpoch 1/30:\n  Train: Loss=0.1836, Acc=0.8273, QWK=0.9437, ±1=0.9833\n  Val:   Loss=0.2576, Acc=0.7295, QWK=0.8526, ±1=0.9262\nEpoch 2/30:\n  Train: Loss=0.1501, Acc=0.8137, QWK=0.9361, ±1=0.9805\n  Val:   Loss=0.2133, Acc=0.7186, QWK=0.8651, ±1=0.9645\nEpoch 3/30:\n  Train: Loss=0.1125, Acc=0.8266, QWK=0.9485, ±1=0.9887\n  Val:   Loss=0.1931, Acc=0.7158, QWK=0.8688, ±1=0.9699\nEpoch 4/30:\n  Train: Loss=0.1010, Acc=0.8311, QWK=0.9510, ±1=0.9908\n  Val:   Loss=0.1808, Acc=0.7186, QWK=0.8596, ±1=0.9617\nEpoch 5/30:\n  Train: Loss=0.0983, Acc=0.8389, QWK=0.9487, ±1=0.9881\n  Val:   Loss=0.1776, Acc=0.7240, QWK=0.8578, ±1=0.9645\nEpoch 6/30:\n  Train: Loss=0.0893, Acc=0.8389, QWK=0.9503, ±1=0.9877\n  Val:   Loss=0.1580, Acc=0.7432, QWK=0.8707, ±1=0.9672\nEpoch 7/30:\n  Train: Loss=0.0804, Acc=0.8519, QWK=0.9548, ±1=0.9894\n  Val:   Loss=0.1567, Acc=0.7322, QWK=0.8599, ±1=0.9590\nEpoch 8/30:\n  Train: Loss=0.0811, Acc=0.8618, QWK=0.9555, ±1=0.9901\n  Val:   Loss=0.1383, Acc=0.7623, QWK=0.8854, ±1=0.9617\nEpoch 9/30:\n  Train: Loss=0.0740, Acc=0.8563, QWK=0.9605, ±1=0.9952\n  Val:   Loss=0.1452, Acc=0.7350, QWK=0.8603, ±1=0.9590\nEpoch 10/30:\n  Train: Loss=0.0725, Acc=0.8614, QWK=0.9562, ±1=0.9901\n  Val:   Loss=0.1395, Acc=0.7486, QWK=0.8766, ±1=0.9617\nEarly stopping at epoch 10\n\n--> TRAINING COMPLETED!\n============================================================\nAn error occurred: cannot access local variable 'best_qwk_stage1' where it is not associated with a value\n[NOTE] Please ensure your data paths are correct and the required libraries are installed.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"3rd try   \nstage 1=> focal loss\nstage 2=> 0.5 focal loss +0.5 kappa loss","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torchvision.transforms as transforms\n\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\n# ---------------------------\n# CONFIG\n# ---------------------------\nBASE_PATH = \"/kaggle/input/aptos2019\"\nTRAIN_CSV = os.path.join(BASE_PATH, \"train_1.csv\")\nVAL_CSV   = os.path.join(BASE_PATH, \"valid.csv\")\nTEST_CSV  = os.path.join(BASE_PATH, \"test.csv\")\n\nTRAIN_DIR = os.path.join(BASE_PATH, \"train_images\", \"train_images\")\nVAL_DIR   = os.path.join(BASE_PATH, \"val_images\", \"val_images\")\nTEST_DIR  = os.path.join(BASE_PATH, \"test_images\", \"test_images\")\n\nIMG_SIZE = 256\nBATCH_SIZE = 16\nNUM_WORKERS = 2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPRINT_FREQ = 1\n\n# ---------------------------\n# Preprocessing helpers\n# ---------------------------\n\ndef crop_black_border_and_center(img_np, thresh_val=10):\n    \"\"\"Crop black borders by thresholding then return cropped image.\"\"\"\n    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n    _, thresh = cv2.threshold(gray, thresh_val, 255, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if contours:\n        x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n        return img_np[y:y+h, x:x+w]\n    return img_np\n\ndef crop_circle_hough(img_np):\n    \"\"\"Attempt HoughCircle to center on retina and crop a circular region (useful when vinette exists).\"\"\"\n    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n    gray = cv2.medianBlur(gray, 5)\n    h, w = gray.shape\n    minr = int(min(h, w) * 0.2)\n    maxr = int(min(h, w) * 0.6)\n    circles = None\n    try:\n        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, dp=1, minDist=100,\n                                   param1=50, param2=30,\n                                   minRadius=minr, maxRadius=maxr)\n    except Exception:\n        circles = None\n    if circles is not None:\n        circles = np.uint16(np.around(circles))\n        x, y, r = circles[0][0]\n\n        # Explicitly cast to float to prevent the overflow warning\n        x_f, y_f, r_f = float(x), float(y), float(r)\n        \n        # Calculate coordinates\n        x1_f, y1_f = x_f - r_f, y_f - r_f\n        x2_f, y2_f = x_f + r_f, y_f + r_f\n        \n        # Clip and convert to int for slicing\n        x1 = int(np.clip(x1_f, 0, w))\n        y1 = int(np.clip(y1_f, 0, h))\n        x2 = int(np.clip(x2_f, 0, w))\n        y2 = int(np.clip(y2_f, 0, h))\n        \n        if x2 > x1 and y2 > y1:\n            return img_np[y1:y2, x1:x2]\n    return img_np\n\n\ndef apply_clahe(img_np):\n    if img_np is None or img_np.size == 0:\n        return img_np\n    if len(img_np.shape) != 3 or img_np.shape[2] != 3:\n        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    b, g, r = cv2.split(img_np)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\n\ndef multiscale_retinex(img_np, scales=[15,80,250], weights=None):\n    \"\"\"Simple Multiscale Retinex (MSR) implementation.\n       img_np is RGB uint8 [H,W,3]\n    \"\"\"\n    if weights is None:\n        weights = [1/len(scales)] * len(scales)\n    img = img_np.astype(np.float32) + 1.0\n    retinex = np.zeros_like(img)\n    for i, scale in enumerate(scales):\n        blur = cv2.GaussianBlur(img, (0,0), sigmaX=scale, sigmaY=scale)\n        # avoid log(0)\n        retinex += weights[i] * (np.log(img) - np.log(blur + 1e-6))\n    # color restore\n    for c in range(3):\n        retinex[:,:,c] = (retinex[:,:,c] - np.min(retinex[:,:,c])) / (np.max(retinex[:,:,c]) - np.min(retinex[:,:,c]) + 1e-9) * 255.0\n    return retinex.astype(np.uint8)\n\ndef gaussian_filter(img_np, k=5):\n    return cv2.GaussianBlur(img_np, (k,k), 0)\n\ndef denoise_nlmeans(img_np):\n    # OpenCV fast NL means for colored images - moderate cost\n    return cv2.fastNlMeansDenoisingColored(img_np, None, h=10, hColor=10, templateWindowSize=7, searchWindowSize=21)\n\n\n# Combined preprocess pipeline\ndef preprocess_image_pipeline(pil_image, output_size=IMG_SIZE, do_msr=True, do_hough=True):\n    \"\"\"Takes PIL.Image, returns PIL.Image after preprocessing.\"\"\"\n    img_np = np.array(pil_image.convert('RGB'))\n\n    # 1. Crop black border\n    img_np = crop_black_border_and_center(img_np, thresh_val=10)\n\n    # 2. Attempt Hough crop (if it finds a circle, crop tighter)\n    if do_hough:\n        img_np = crop_circle_hough(img_np)\n\n    # 3. CLAHE on green channel\n    img_np = apply_clahe(img_np)\n\n    # 4. Mild denoise\n    img_np = denoise_nlmeans(img_np)\n\n    # 5. Gaussian smoothing\n    img_np = gaussian_filter(img_np, k=3)\n\n    # 6. Multiscale Retinex enhance (optional)\n    if do_msr:\n        try:\n            img_np = multiscale_retinex(img_np)\n        except Exception:\n            # fallback if something goes wrong\n            pass\n\n    # 7. Resize to output_size\n    img_resized = cv2.resize(img_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    return Image.fromarray(img_resized)\n\n# ---------------------------\n# Dataset\n# ---------------------------\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None, preprocess_fn=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.preprocess_fn = preprocess_fn\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = Image.open(img_path).convert('RGB')\n\n        if self.preprocess_fn:\n            img = self.preprocess_fn(img)\n\n        if self.transform:\n            img = self.transform(img)\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\n# ---------------------------\n# Losses & Model\n# ---------------------------\nclass OrdinalFocalLoss(nn.Module):\n    def __init__(self, num_classes=5, gamma=2.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, outputs, targets):\n        # outputs: (B, C-1), targets: (B,)\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, t in enumerate(targets):\n            if t > 0:\n                ordinal_targets[i, :t] = 1.0\n        bce = self.bce(outputs, ordinal_targets)\n        pt = torch.exp(-bce)\n        focal = (1 - pt) ** self.gamma * bce\n        return focal.mean()\n\nclass SmoothKappaLoss(nn.Module):\n    def __init__(self, num_classes=5, eps=1e-7):\n        super().__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                W[i,j] = ((i - j)**2) / ((num_classes - 1)**2)\n        self.register_buffer(\"W\", W)\n\n    def forward(self, outputs, targets):\n        device = outputs.device\n        B = outputs.size(0)\n        probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes-1):\n            class_probs[:, k] = probs[:, k-1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        class_probs = class_probs / class_probs.sum(dim=1, keepdim=True)\n\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        conf_mat = conf_mat / (conf_mat.sum() + self.eps)\n\n        hist_true = one_hot.sum(dim=0)\n        hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        expected = expected / (expected.sum() + self.eps)\n\n        W = self.W.to(device)\n        obs = torch.sum(W * conf_mat)\n        exp = torch.sum(W * expected)\n        kappa = 1.0 - obs / (exp + self.eps)\n        return 1.0 - kappa\n\nclass EfficientNetOrdinal(nn.Module):\n    def __init__(self, model_name='efficientnet_b2', num_classes=5, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes - 1)\n        )\n    def forward(self, x):\n        feat = self.backbone(x)\n        return self.classifier(feat)\n\n# ---------------------------\n# Utilities\n# ---------------------------\ndef ordinal_to_class(outputs):\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\ndef calculate_metrics(outputs, targets):\n    preds = ordinal_to_class(outputs).cpu().numpy()\n    targets_np = targets.cpu().numpy()\n    acc = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    within1 = np.mean(np.abs(targets_np - preds) <= 1)\n    return acc, qwk, within1\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef mixup_data(x, y, alpha=0.4):\n    '''Returns mixed inputs, targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n# ---------------------------\n# Training loops\n# ---------------------------\ndef train_epoch(model, loader, optimizer, criterion, scaler, device, use_mixup=True):\n    model.train()\n    running_loss = 0.0\n    all_out, all_t = [], []\n    for images, targets in loader:\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad()\n        \n        if use_mixup:\n            images, targets_a, targets_b, lam = mixup_data(images, targets)\n        \n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            outputs = model(images)\n            if use_mixup:\n                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n            else:\n                loss = criterion(outputs, targets)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        \n        all_out.append(outputs.detach())\n        all_t.append(targets.detach())\n    \n    all_out = torch.cat(all_out)\n    all_t = torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_out, all_t = [], []\n    with torch.no_grad():\n        for images, targets in loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets) if not isinstance(criterion, nn.Module) else criterion(outputs, targets)\n            running_loss += loss.item()\n            all_out.append(outputs)\n            all_t.append(targets)\n    all_out = torch.cat(all_out)\n    all_t = torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\n# ---------------------------\n# Threshold optimization (simple grid) for ordinal outputs\n# ---------------------------\ndef optimize_thresholds(model, loader, device):\n    # Evaluate class probabilities on loader and perform simple rounding threshold search\n    model.eval()\n    probs_list, labels_list = [], []\n    with torch.no_grad():\n        for images, targets in loader:\n            images = images.to(device)\n            outputs = model(images)\n            probs = torch.sigmoid(outputs).cpu().numpy()\n            probs_list.append(probs)\n            labels_list.append(targets.numpy())\n    probs = np.vstack(probs_list)\n    labels = np.concatenate(labels_list)\n\n    # build class predictions by varying a simple single threshold t applied to each ordinal logit\n    best_t = 0.5\n    best_qwk = -1\n    for t in np.linspace(0.3, 0.7, 9):\n        preds = np.sum(probs > t, axis=1)\n        qwk = cohen_kappa_score(labels, preds, weights='quadratic')\n        if qwk > best_qwk:\n            best_qwk = qwk\n            best_t = t\n    return best_t, best_qwk\n\n# ---------------------------\n# TTA inference (averaging flips)\n# ---------------------------\ndef tta_predict(model, image_pil, device, tta_transforms):\n    model.eval()\n    probs_acc = None\n    with torch.no_grad():\n        for tf in tta_transforms:\n            x = tf(image_pil).unsqueeze(0).to(device)\n            out = model(x)\n            p = torch.sigmoid(out).cpu().numpy()\n            probs_acc = p if probs_acc is None else probs_acc + p\n    probs_acc /= len(tta_transforms)\n    return probs_acc[0]\n\n# ---------------------------\n# Main training pipeline\n# ---------------------------\ndef main():\n    print(\"Device:\", DEVICE)\n    # load csvs\n    train_df = pd.read_csv(TRAIN_CSV)\n    val_df = pd.read_csv(VAL_CSV)\n    test_df = pd.read_csv(TEST_CSV)\n\n    print(\"Train samples:\", len(train_df), \"Val samples:\", len(val_df))\n\n    # transforms\n    train_tf = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.12, hue=0.04),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n    val_tf = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n\n    # dataset\n    train_ds = DiabeticRetinopathyDataset(train_df, TRAIN_DIR, transform=train_tf, preprocess_fn=lambda p: preprocess_image_pipeline(p, output_size=IMG_SIZE))\n    val_ds   = DiabeticRetinopathyDataset(val_df, VAL_DIR, transform=val_tf, preprocess_fn=lambda p: preprocess_image_pipeline(p, output_size=IMG_SIZE))\n    test_ds  = DiabeticRetinopathyDataset(test_df, TEST_DIR, transform=val_tf, preprocess_fn=lambda p: preprocess_image_pipeline(p, output_size=IMG_SIZE))\n\n    # Weighted sampler to balance classes\n    class_counts = train_df['diagnosis'].value_counts().sort_index().values\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    sample_weights = np.array([class_weights[int(l)] for l in train_df['diagnosis']])\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n    # Model and losses\n    model = EfficientNetOrdinal('efficientnet_b2', num_classes=5).to(DEVICE)\n    focal = OrdinalFocalLoss(num_classes=5, gamma=2.0)\n    kappa = SmoothKappaLoss(num_classes=5)\n    # Using the new, non-deprecated syntax\n    scaler = torch.amp.GradScaler(device=\"cuda\", enabled=torch.cuda.is_available())\n\n    # STAGE 1: focal training\n    opt = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=30)\n    best_val_qwk = -1\n    patience = 5\n    wait = 0\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, opt, focal, scaler, DEVICE, use_mixup=True)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, focal, DEVICE)\n        sched.step()\n\n        if (epoch+1) % PRINT_FREQ == 0:\n            print(f\"[Stage1] Epoch {epoch+1:02d} Train QWK: {train_qwk:.4f} | Val QWK: {val_qwk:.4f} | Val Acc: {val_acc:.4f}\")\n\n        if val_qwk > best_val_qwk:\n            best_val_qwk = val_qwk\n            torch.save(model.state_dict(), \"best_model_stage1.pth\")\n            wait = 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\"Early stopping Stage 1\")\n                break\n\n    # reload best stage1\n    model.load_state_dict(torch.load(\"best_model_stage1.pth\"))\n\n    # STAGE 2: hybrid loss (0.5 kappa + 0.5 focal)\n    opt = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=30)\n    best_val_qwk_stage2 = best_val_qwk\n    wait = 0\n    def hybrid(outputs, targets):\n        return 0.5 * kappa(outputs, targets) + 0.5 * focal(outputs, targets)\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, opt, hybrid, scaler, DEVICE, use_mixup=False)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, hybrid, DEVICE)\n        sched.step()\n\n        if (epoch+1) % PRINT_FREQ == 0:\n            print(f\"[Stage2] Epoch {epoch+1:02d} Train QWK: {train_qwk:.4f} | Val QWK: {val_qwk:.4f} | Val Acc: {val_acc:.4f}\")\n\n        if val_qwk > best_val_qwk_stage2:\n            best_val_qwk_stage2 = val_qwk\n            torch.save(model.state_dict(), \"best_model_final.pth\")\n            wait = 0\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\"Early stopping Stage 2\")\n                break\n\n    # load best final\n    model.load_state_dict(torch.load(\"best_model_final.pth\"))\n    print(\"Best Stage1 QWK:\", best_val_qwk, \"Best Stage2 QWK:\", best_val_qwk_stage2)\n\n    # Find best threshold on validation (simple search)\n    best_t, best_qwk_found = optimize_thresholds(model, val_loader, DEVICE)\n    print(\"Optimized ordinal threshold:\", best_t, \"val qwk:\", best_qwk_found)\n\n    # TTA transforms (simple set)\n    tta_transforms = [\n        lambda x: val_tf(x),\n        lambda x: val_tf(x.transpose(Image.FLIP_LEFT_RIGHT)),\n        lambda x: val_tf(x.transpose(Image.FLIP_TOP_BOTTOM)),\n    ]\n\n    # Inference on test set using TTA and threshold\n    model.eval()\n    ids, preds = [], []\n    with torch.no_grad():\n        for img_idx in tqdm(range(len(test_df))):\n            row = test_df.iloc[img_idx]\n            img_path = os.path.join(TEST_DIR, row['id_code'] + '.png')\n            pil = Image.open(img_path).convert('RGB')\n            probs_acc = None\n            # perform TTA\n            for tf in tta_transforms:\n                x = tf(pil).unsqueeze(0).to(DEVICE)\n                out = model(x)\n                p = torch.sigmoid(out).cpu().numpy()\n                probs_acc = p if probs_acc is None else probs_acc + p\n            probs_acc /= len(tta_transforms)\n            pred = int(np.sum(probs_acc > best_t))\n            ids.append(row['id_code'])\n            preds.append(pred)\n\n    submission = pd.DataFrame({\"id_code\": ids, \"diagnosis\": preds})\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"Saved submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T08:13:43.611426Z","iopub.execute_input":"2025-09-10T08:13:43.611776Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nTrain samples: 2930 Val samples: 366\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_441/2751803802.py:307: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"4th try","metadata":{}},{"cell_type":"code","source":"# corrected_full_pipeline_kaggle.py\n# Paste into Kaggle notebook cell and run.\n\nimport os\nimport gc\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torchvision.transforms as transforms\n\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\n# ---------------------------\n# CONFIG\n# ---------------------------\nBASE_PATH = \"/kaggle/input/aptos2019\"\nTRAIN_CSV = os.path.join(BASE_PATH, \"train_1.csv\")\nVAL_CSV   = os.path.join(BASE_PATH, \"valid.csv\")\nTEST_CSV  = os.path.join(BASE_PATH, \"test.csv\")\n\nTRAIN_DIR = os.path.join(BASE_PATH, \"train_images\",\"train_images\")\nVAL_DIR   = os.path.join(BASE_PATH, \"val_images\",\"val_images\")\nTEST_DIR  = os.path.join(BASE_PATH, \"test_images\",\"test_images\")\n\nIMG_SIZE = 448           # you had tried larger sizes — choose based on memory\nBATCH_SIZE = 8\nNUM_WORKERS = 2\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPRINT_FREQ = 1\nSEED = 42\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# small global to avoid spammy logs from many broken images\n_bad_image_log_count = 0\n\n# ---------------------------\n# Preprocessing helpers (robust)\n# ---------------------------\ndef crop_black_border_and_center(img_np, thresh_val=10):\n    \"\"\"Crop black borders by thresholding then return cropped image.\"\"\"\n    if img_np is None or img_np.size == 0:\n        return img_np\n    try:\n        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n    except Exception:\n        # unexpected format - return as is\n        return img_np\n    _, thresh = cv2.threshold(gray, thresh_val, 255, cv2.THRESH_BINARY)\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if contours:\n        x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n        # validate coords\n        if w > 10 and h > 10:\n            return img_np[y:y+h, x:x+w]\n    return img_np\n\ndef crop_circle_hough(img_np):\n    \"\"\"Attempt HoughCircle to center on retina and crop a circular region safely.\"\"\"\n    if img_np is None or img_np.size == 0:\n        return img_np\n    try:\n        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n    except Exception:\n        return img_np\n\n    # small blur to reduce noise\n    gray = cv2.medianBlur(gray, 5)\n    h, w = gray.shape[:2]\n    minr = max(6, int(min(h, w) * 0.18))\n    maxr = max(minr+1, int(min(h, w) * 0.62))\n    try:\n        circles = cv2.HoughCircles(\n            gray, cv2.HOUGH_GRADIENT, dp=1.0, minDist=min(h, w)//8,\n            param1=50, param2=30, minRadius=minr, maxRadius=maxr\n        )\n    except Exception:\n        circles = None\n\n    if circles is not None and len(circles) > 0:\n        # select the best candidate — ensure numeric & valid\n        circles = np.round(circles).astype(int)\n        x, y, r = circles[0][0].tolist()\n        # clamp values and ensure positive\n        r = int(abs(r))\n        x = int(np.clip(x, 0, w-1))\n        y = int(np.clip(y, 0, h-1))\n        r = int(np.clip(r, 1, max(h, w)))\n        # compute bounding box with clamping\n        x1 = max(0, x - r)\n        y1 = max(0, y - r)\n        x2 = min(w, x + r)\n        y2 = min(h, y + r)\n        # ensure non-empty crop\n        if x2 > x1 + 4 and y2 > y1 + 4:\n            cropped = img_np[y1:y2, x1:x2]\n            if cropped.size != 0:\n                return cropped\n    return img_np\n\ndef apply_clahe(img_np):\n    \"\"\"Apply CLAHE to green channel. Be robust to shapes.\"\"\"\n    if img_np is None or img_np.size == 0:\n        return img_np\n    # if grayscale convert to 3 channel\n    if img_np.ndim == 2:\n        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    if img_np.ndim == 3 and img_np.shape[2] == 4:\n        # drop alpha\n        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)\n    if img_np.ndim != 3 or img_np.shape[2] != 3:\n        # fallback: try to convert with PIL\n        try:\n            img_np = np.array(Image.fromarray(img_np).convert('RGB'))\n        except Exception:\n            return img_np\n\n    try:\n        b, g, r = cv2.split(img_np)\n    except Exception:\n        return img_np\n\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    try:\n        g2 = clahe.apply(g)\n    except Exception:\n        g2 = g\n    img_clahe = cv2.merge((b, g2, r))\n    return img_clahe\n\ndef multiscale_retinex(img_np, scales=[15,80,250], weights=None):\n    \"\"\"Robust MSR; returns uint8.\"\"\"\n    if img_np is None or img_np.size == 0:\n        return img_np\n    if weights is None:\n        weights = [1.0 / len(scales)] * len(scales)\n    img = img_np.astype(np.float32) + 1.0\n    retinex = np.zeros_like(img)\n    for i, scale in enumerate(scales):\n        blur = cv2.GaussianBlur(img, (0,0), sigmaX=scale, sigmaY=scale)\n        retinex += weights[i] * (np.log(img) - np.log(blur + 1e-6))\n    out = np.zeros_like(retinex)\n    for c in range(retinex.shape[2]):\n        arr = retinex[:,:,c]\n        mn, mx = arr.min(), arr.max()\n        if mx - mn > 1e-6:\n            out[:,:,c] = (arr - mn) / (mx - mn) * 255.0\n        else:\n            out[:,:,c] = arr\n    return out.astype(np.uint8)\n\ndef denoise_nlmeans(img_np):\n    try:\n        return cv2.fastNlMeansDenoisingColored(img_np, None, h=8, hColor=8, templateWindowSize=7, searchWindowSize=21)\n    except Exception:\n        return img_np\n\ndef preprocess_image_pipeline(pil_image, output_size=IMG_SIZE, do_msr=True, do_hough=True):\n    \"\"\"Takes PIL.Image, returns PIL.Image after preprocessing with robust fallbacks.\"\"\"\n    global _bad_image_log_count\n    try:\n        img_np = np.array(pil_image.convert('RGB'))\n    except Exception:\n        # if PIL failed, return a gray image of target size\n        if _bad_image_log_count < 5:\n            warnings.warn(\"PIL conversion failed in preprocess_image_pipeline; returning gray image.\")\n            _bad_image_log_count += 1\n        return Image.fromarray(np.uint8(np.ones((output_size, output_size, 3)) * 127))\n\n    img_np = crop_black_border_and_center(img_np, thresh_val=10)\n\n    if do_hough:\n        img_np = crop_circle_hough(img_np)\n\n    img_np = apply_clahe(img_np)\n    img_np = denoise_nlmeans(img_np)\n    img_np = cv2.GaussianBlur(img_np, (3,3), 0)\n\n    if do_msr:\n        try:\n            img_np = multiscale_retinex(img_np)\n        except Exception:\n            # fall back silently\n            pass\n\n    # final safety: if shape invalid, make a gray image\n    if img_np is None or img_np.size == 0 or img_np.ndim != 3 or img_np.shape[2] != 3:\n        if _bad_image_log_count < 5:\n            warnings.warn(\"Preprocessing produced invalid image; using gray fallback.\")\n            _bad_image_log_count += 1\n        img_np = np.ones((output_size, output_size, 3), dtype=np.uint8) * 127\n\n    img_resized = cv2.resize(img_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    return Image.fromarray(img_resized)\n\n# ---------------------------\n# Dataset\n# ---------------------------\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None, preprocess_fn=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.preprocess_fn = preprocess_fn\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        global _bad_image_log_count\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, str(row['id_code']) + '.png')\n        try:\n            img = Image.open(img_path)\n            img = img.convert('RGB')\n        except Exception as e:\n            # broken file — return a gray image but don't crash the worker\n            if _bad_image_log_count < 10:\n                warnings.warn(f\"Failed to open image {img_path}: {e}. Returning gray fallback for this sample.\")\n                _bad_image_log_count += 1\n            img = Image.fromarray(np.uint8(np.ones((IMG_SIZE, IMG_SIZE, 3)) * 127))\n\n        try:\n            if self.preprocess_fn:\n                img = self.preprocess_fn(img)\n            if self.transform:\n                img = self.transform(img)\n        except Exception as e:\n            # Preprocessing/transforms crashed for this image -> fallback gray tensor\n            if _bad_image_log_count < 10:\n                warnings.warn(f\"Preprocess/transform failed for {img_path}: {e}. Returning gray fallback.\")\n                _bad_image_log_count += 1\n            # create a gray tensor directly respecting transform normalization if possible\n            img = Image.fromarray(np.uint8(np.ones((IMG_SIZE, IMG_SIZE, 3)) * 127))\n            if self.transform:\n                try:\n                    img = self.transform(img)\n                except Exception:\n                    # if transform also fails, create a simple tensor\n                    img = transforms.ToTensor()(Image.fromarray(np.uint8(np.ones((IMG_SIZE, IMG_SIZE, 3)) * 127)))\n\n        label = torch.tensor(int(row['diagnosis']), dtype=torch.long)\n        return img, label\n\n# ---------------------------\n# Losses & Model (kept as high-level multi-class now)\n# ---------------------------\nclass SmoothKappaLoss(nn.Module):\n    \"\"\"Differentiable QWK loss using softmax probabilities (multi-class).\"\"\"\n    def __init__(self, num_classes=5, eps=1e-7):\n        super().__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                W[i,j] = float((i - j)**2)\n        self.register_buffer('W', W)\n\n    def forward(self, logits, targets):\n        device = logits.device\n        probs = F.softmax(logits, dim=1)\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n        conf_mat = torch.matmul(one_hot.T, probs)\n        conf_mat = conf_mat / (conf_mat.sum() + self.eps)\n        hist_true = one_hot.sum(dim=0)\n        hist_pred = probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        expected = expected / (expected.sum() + self.eps)\n        W = self.W.to(device)\n        obs = torch.sum(W * conf_mat)\n        exp = torch.sum(W * expected)\n        kappa = 1.0 - obs / (exp + self.eps)\n        return 1.0 - kappa\n\nclass EfficientNetB3Classifier(nn.Module):\n    def __init__(self, model_name='tf_efficientnet_b3_ns', num_classes=5, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feat = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feat, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n    def forward(self, x):\n        f = self.backbone(x)\n        return self.classifier(f)\n\n# ---------------------------\n# Utilities & training loops (kept similar to your prior code)\n# ---------------------------\ndef calc_metrics_from_logits(logits, targets):\n    probs = F.softmax(logits, dim=1).cpu().numpy()\n    preds = np.argmax(probs, axis=1)\n    targets_np = targets.cpu().numpy()\n    acc = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    within1 = np.mean(np.abs(targets_np - preds) <= 1)\n    return acc, qwk, within1\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef mixup_data(x, y, alpha=0.2):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.0\n    batch_size = x.size(0)\n    if batch_size == 1:\n        return x, y, y, 1.0\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, device, use_mixup=False):\n    model.train()\n    total_loss = 0.0\n    all_logits = []\n    all_targets = []\n    for images, targets in loader:\n        images = images.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        if use_mixup:\n            images, y_a, y_b, lam = mixup_data(images, targets, alpha=0.2)\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                logits = model(images)\n                loss = lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)\n        else:\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                logits = model(images)\n                loss = criterion(logits, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item() * images.size(0)\n        all_logits.append(logits.detach().cpu())\n        all_targets.append(targets.detach().cpu())\n    if len(all_logits) == 0:\n        return 0.0, 0.0, 0.0, 0.0\n    all_logits = torch.cat(all_logits, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n    avg_loss = total_loss / len(loader.dataset)\n    acc, qwk, within1 = calc_metrics_from_logits(all_logits, all_targets)\n    return avg_loss, acc, qwk, within1\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    all_logits = []\n    all_targets = []\n    with torch.no_grad():\n        for images, targets in loader:\n            images = images.to(device)\n            targets = targets.to(device)\n            logits = model(images)\n            loss = criterion(logits, targets)\n            total_loss += loss.item() * images.size(0)\n            all_logits.append(logits.detach().cpu())\n            all_targets.append(targets.detach().cpu())\n    if len(all_logits) == 0:\n        return 0.0, 0.0, 0.0, 0.0\n    all_logits = torch.cat(all_logits, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n    avg_loss = total_loss / len(loader.dataset)\n    acc, qwk, within1 = calc_metrics_from_logits(all_logits, all_targets)\n    return avg_loss, acc, qwk, within1\n\n# ---------------------------\n# Main training pipeline (two stages)\n# ---------------------------\ndef main():\n    print(\"Device:\", DEVICE)\n    # load csvs - ensure these files exist in your Kaggle dataset\n    train_df = pd.read_csv(TRAIN_CSV)\n    val_df = pd.read_csv(VAL_CSV)\n    test_df = pd.read_csv(TEST_CSV)\n\n    print(\"Train samples:\", len(train_df), \"Val samples:\", len(val_df))\n\n    # Stronger transforms + RandomErasing\n    train_tf = transforms.Compose([\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7,1.0), ratio=(0.9,1.1)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(20),\n        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2, hue=0.02),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n        transforms.RandomErasing(p=0.2, scale=(0.02,0.2))\n    ])\n    val_tf = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n\n    train_ds = DiabeticRetinopathyDataset(train_df, TRAIN_DIR, transform=train_tf, preprocess_fn=lambda p: preprocess_image_pipeline(p, output_size=IMG_SIZE))\n    val_ds   = DiabeticRetinopathyDataset(val_df, VAL_DIR, transform=val_tf, preprocess_fn=lambda p: preprocess_image_pipeline(p, output_size=IMG_SIZE))\n\n    classes = np.unique(train_df['diagnosis'])\n    class_weights = compute_class_weight('balanced', classes=classes, y=train_df['diagnosis'])\n    sample_weights = np.array([class_weights[int(x)] for x in train_df['diagnosis']])\n    sampler = WeightedRandomSampler(sample_weights.astype(float), num_samples=len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=True, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\n    # Model + losses\n    model = EfficientNetB3Classifier(model_name='tf_efficientnet_b3_ns', num_classes=5).to(DEVICE)\n\n    ce_class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n    ce_loss_fn = nn.CrossEntropyLoss(weight=ce_class_weights)\n    kappa_loss_fn = SmoothKappaLoss(num_classes=5)\n\n    # Use recommended GradScaler API\n    scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n\n    # STAGE 1: CE with mixup\n    opt = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n    steps_per_epoch = max(1, len(train_loader))\n    sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=1e-3, steps_per_epoch=steps_per_epoch, epochs=30)\n\n    best_val_qwk = -1.0\n    best_val_acc = -1.0\n    patience = 7\n    wait = 0\n    use_mixup = True\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, opt, ce_loss_fn, scaler, DEVICE, use_mixup=use_mixup)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, ce_loss_fn, DEVICE)\n        try:\n            sched.step()\n        except Exception:\n            pass\n\n        if (epoch+1) % PRINT_FREQ == 0:\n            print(f\"[Stage1] Epoch {epoch+1:02d} Train acc: {train_acc:.4f} QWK: {train_qwk:.4f} | Val acc: {val_acc:.4f} QWK: {val_qwk:.4f}\")\n\n        score_for_save = val_qwk\n        if score_for_save > best_val_qwk or val_acc > best_val_acc:\n            best_val_qwk = max(best_val_qwk, val_qwk)\n            best_val_acc = max(best_val_acc, val_acc)\n            torch.save(model.state_dict(), \"best_stage1.pth\")\n            wait = 0\n            print(f\"  [SAVE] Stage1 model saved (val_qwk: {val_qwk:.4f}, val_acc: {val_acc:.4f})\")\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\"Early stopping Stage 1\")\n                break\n\n    # reload best stage1\n    if os.path.exists(\"best_stage1.pth\"):\n        model.load_state_dict(torch.load(\"best_stage1.pth\", map_location=DEVICE))\n    else:\n        print(\"Warning: best_stage1.pth not found, continuing with current weights.\")\n\n    # STAGE 2: hybrid (CE + kappa)\n    opt = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-5)\n    sched = optim.lr_scheduler.OneCycleLR(opt, max_lr=5e-4, steps_per_epoch=steps_per_epoch, epochs=30)\n\n    def hybrid_loss(logits, targets):\n        ce = nn.CrossEntropyLoss(weight=ce_class_weights)(logits, targets)\n        k = kappa_loss_fn(logits, targets)\n        return 0.6 * ce + 0.4 * k\n\n    best_val_qwk_stage2 = best_val_qwk\n    wait = 0\n    patience = 8\n\n    for epoch in range(30):\n        clear_memory()\n        train_loss, train_acc, train_qwk, train_within1 = train_epoch(model, train_loader, opt, hybrid_loss, scaler, DEVICE, use_mixup=False)\n        val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, hybrid_loss, DEVICE)\n        try:\n            sched.step()\n        except Exception:\n            pass\n\n        if (epoch+1) % PRINT_FREQ == 0:\n            print(f\"[Stage2] Epoch {epoch+1:02d} Train acc: {train_acc:.4f} QWK: {train_qwk:.4f} | Val acc: {val_acc:.4f} QWK: {val_qwk:.4f}\")\n\n        if val_qwk > best_val_qwk_stage2:\n            best_val_qwk_stage2 = val_qwk\n            torch.save(model.state_dict(), \"best_model_final.pth\")\n            wait = 0\n            print(f\"  [SAVE] Stage2 model saved (val_qwk: {val_qwk:.4f})\")\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\"Early stopping Stage 2\")\n                break\n\n    # finalize\n    if os.path.exists(\"best_model_final.pth\"):\n        model.load_state_dict(torch.load(\"best_model_final.pth\", map_location=DEVICE))\n    else:\n        print(\"Warning: best_model_final.pth not found. Using last model state.\")\n\n    print(\"Training finished. Best Stage1 QWK:\", best_val_qwk, \"Best Stage2 QWK:\", best_val_qwk_stage2)\n\n    # quick final CE eval on val set\n    val_loss, val_acc, val_qwk, val_within1 = validate_epoch(model, val_loader, nn.CrossEntropyLoss(weight=ce_class_weights), DEVICE)\n    print(\"Final evaluation (CE): Val acc: {:.4f} | Val QWK: {:.4f} | ±1: {:.4f}\".format(val_acc, val_qwk, val_within1))\n\n    torch.save(model.state_dict(), \"final_model.pth\")\n    print(\"Saved final_model.pth\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport torchvision.transforms as transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\nclass CFG:\n    # Data paths\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    TRAIN_CSV = os.path.join(BASE_PATH, \"train_1.csv\")\n    VAL_CSV   = os.path.join(BASE_PATH, \"valid.csv\")\n    TRAIN_DIR = os.path.join(BASE_PATH, \"train_images\", \"train_images\")\n    VAL_DIR   = os.path.join(BASE_PATH, \"val_images\", \"val_images\")\n\n    # Model & Training parameters\n    MODEL_NAME = 'efficientnet_b3' # Upgraded model\n    IMG_SIZE = 384                 # Increased image size\n    BATCH_SIZE = 8                 # Reduced batch size to fit larger images in memory\n    NUM_WORKERS = 2\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Stage 1: Weighted Focal Loss Training\n    S1_EPOCHS = 15\n    S1_LR = 1e-4\n    S1_USE_MIXUP = True\n    \n    # Stage 2: Hybrid Loss Fine-tuning\n    S2_EPOCHS = 15\n    S2_LR = 3e-5 \n    S2_USE_MIXUP = False\n    \n    # General\n    PATIENCE = 5\n    SEED = 42\n    LABEL_SMOOTHING = 0.05\n\n# Seed everything for reproducibility\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True # Can be True for performance if input sizes are fixed\n\nseed_everything(CFG.SEED)\n\n\n# =============================================================================\n# PREPROCESSING & AUGMENTATIONS\n# =============================================================================\ndef preprocess_ben_graham(image_np, output_size):\n    # This function now expects a numpy array from Albumentations\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n        if gray.mean() < 15: # Heuristic for nearly all-black images\n             return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception:\n        pass\n\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(image_resized)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\n# Advanced Augmentations using Albumentations\ndef get_transforms(img_size):\n    # Pre-normalization transforms (applied to raw image)\n    pre_transforms = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size), name=\"ben_graham_preprocess\"),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7),\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n    ])\n    # Post-normalization transforms\n    post_transforms = A.Compose([\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    val_transforms = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size), name=\"ben_graham_preprocess\"),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    return pre_transforms, post_transforms, val_transforms\n\n\n# =============================================================================\n# DATASET\n# =============================================================================\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, pre_transform=None, post_transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.pre_transform = pre_transform\n        self.post_transform = post_transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        # Load with OpenCV for Albumentations\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        if self.pre_transform:\n            augmented = self.pre_transform(image=img)\n            img = augmented['image']\n        \n        # Apply normalization and ToTensor after other augs\n        if self.post_transform:\n            tensor_aug = self.post_transform(image=img)\n            img = tensor_aug['image']\n\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\n# =============================================================================\n# LOSSES & MODEL\n# =============================================================================\nclass WeightedOrdinalFocalLoss(nn.Module):\n    def __init__(self, num_classes=5, gamma=2.0, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.gamma = gamma\n        self.class_weights = class_weights\n        self.label_smoothing = label_smoothing\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, outputs, targets):\n        # Create ordinal targets with label smoothing\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, t in enumerate(targets):\n            if t > 0:\n                ordinal_targets[i, :t] = 1.0\n        \n        # Apply label smoothing\n        if self.label_smoothing > 0.0:\n            ordinal_targets = ordinal_targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n\n        bce = self.bce(outputs, ordinal_targets)\n        \n        # Apply class weights to the loss for each sample\n        if self.class_weights is not None:\n            weights = self.class_weights[targets].view(-1, 1).expand(-1, outputs.shape[1])\n            bce = bce * weights\n\n        pt = torch.exp(-bce)\n        focal = (1 - pt) ** self.gamma * bce\n        return focal.mean()\n\n# SmoothKappaLoss and Model remain the same as before...\nclass SmoothKappaLoss(nn.Module):\n    def __init__(self, num_classes=5, eps=1e-7):\n        super().__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                W[i,j] = ((i - j)**2) / ((num_classes - 1)**2)\n        self.register_buffer(\"W\", W)\n\n    def forward(self, outputs, targets):\n        device = outputs.device\n        B = outputs.size(0)\n        probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes-1):\n            class_probs[:, k] = probs[:, k-1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        \n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        \n        hist_true = one_hot.sum(dim=0)\n        hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n\n        W = self.W.to(device)\n        obs = torch.sum(W * conf_mat)\n        exp = torch.sum(W * expected)\n        kappa = 1.0 - (B * obs) / (exp + self.eps)\n        return 1.0 - kappa\n        \nclass EfficientNetOrdinal(nn.Module):\n    def __init__(self, model_name='efficientnet_b3', num_classes=5, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes - 1)\n        )\n    def forward(self, x):\n        feat = self.backbone(x)\n        return self.classifier(feat)\n\n\n# Utilities and Training loops remain the same...\n\ndef ordinal_to_class(outputs):\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\ndef calculate_metrics(outputs, targets):\n    preds = ordinal_to_class(outputs).cpu().numpy()\n    targets_np = targets.cpu().numpy()\n    acc = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    return acc, qwk\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef mixup_data(x, y, alpha=0.4):\n    if alpha > 0: lam = np.random.beta(alpha, alpha)\n    else: lam = 1\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, device, use_mixup):\n    model.train()\n    running_loss = 0.0\n    all_out, all_t = [], []\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    for images, targets in pbar:\n        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if use_mixup: images, targets_a, targets_b, lam = mixup_data(images, targets)\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            if use_mixup: loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n            else: loss = criterion(outputs, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        all_out.append(outputs.detach())\n        all_t.append(targets.detach())\n        pbar.set_postfix(loss=loss.item())\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_out, all_t = [], []\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n        for images, targets in pbar:\n            images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_out.append(outputs)\n            all_t.append(targets)\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\n# =============================================================================\n# MAIN TRAINING PIPELINE\n# =============================================================================\ndef main():\n    print(f\"Device: {CFG.DEVICE}, Model: {CFG.MODEL_NAME}, Image Size: {CFG.IMG_SIZE}\")\n    train_df = pd.read_csv(CFG.TRAIN_CSV)\n    val_df = pd.read_csv(CFG.VAL_CSV)\n\n    pre_tf, post_tf, val_tf = get_transforms(CFG.IMG_SIZE)\n\n    train_ds = DiabeticRetinopathyDataset(train_df, CFG.TRAIN_DIR, pre_transform=pre_tf, post_transform=post_tf)\n    val_ds   = DiabeticRetinopathyDataset(val_df, CFG.VAL_DIR, pre_transform=val_tf) # val_tf does all steps\n\n    # Sampler for imbalance\n    class_weights_sampler = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    sample_weights = np.array([class_weights_sampler[int(l)] for l in train_df['diagnosis']])\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, sampler=sampler, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=CFG.BATCH_SIZE*2, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n\n    # Model and losses\n    model = EfficientNetOrdinal(CFG.MODEL_NAME, num_classes=5).to(CFG.DEVICE)\n    \n    # Class weights for the loss function\n    class_weights_loss = torch.tensor(class_weights_sampler, dtype=torch.float).to(CFG.DEVICE)\n    \n    # LOSSES\n    focal_loss = WeightedOrdinalFocalLoss(num_classes=5, gamma=2.0, class_weights=class_weights_loss, label_smoothing=CFG.LABEL_SMOOTHING)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    \n    # HYBRID LOSS for STAGE 2\n    def hybrid_loss(outputs, targets):\n        return 0.7 * kappa_loss(outputs, targets) + 0.3 * focal_loss(outputs, targets)\n    \n    scaler = torch.cuda.amp.GradScaler()\n\n    # --- STAGE 1: WEIGHTED FOCAL LOSS ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 1: WEIGHTED FOCAL LOSS\\n\" + \"=\"*50)\n    opt = optim.AdamW(model.parameters(), lr=CFG.S1_LR, weight_decay=1e-4)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S1_EPOCHS)\n    best_val_qwk, patience_counter = -1, 0\n\n    for epoch in range(CFG.S1_EPOCHS):\n        clear_memory()\n        print(f\"\\nEpoch {epoch+1}/{CFG.S1_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, focal_loss, scaler, CFG.DEVICE, CFG.S1_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, focal_loss, CFG.DEVICE)\n        sched.step()\n\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n\n        if val_qwk > best_val_qwk:\n            print(f\"Val QWK improved from {best_val_qwk:.4f} to {val_qwk:.4f}. Saving model...\")\n            best_val_qwk, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), \"best_model_stage1.pth\")\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 1.\"); break\n    \n    # --- STAGE 2: HYBRID LOSS FINE-TUNING ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 2: HYBRID LOSS FINE-TUNING\\n\" + \"=\"*50)\n    model.load_state_dict(torch.load(\"best_model_stage1.pth\"))\n    opt = optim.AdamW(model.parameters(), lr=CFG.S2_LR, weight_decay=1e-5)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S2_EPOCHS)\n    best_val_qwk_stage2, patience_counter = best_val_qwk, 0\n\n    for epoch in range(CFG.S2_EPOCHS):\n        clear_memory()\n        print(f\"\\nEpoch {epoch+1}/{CFG.S2_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, hybrid_loss, scaler, CFG.DEVICE, CFG.S2_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, hybrid_loss, CFG.DEVICE)\n        sched.step()\n        \n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n\n        if val_qwk > best_val_qwk_stage2:\n            print(f\"Val QWK improved from {best_val_qwk_stage2:.4f} to {val_qwk:.4f}. Saving final model...\")\n            best_val_qwk_stage2, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), \"best_model_final.pth\")\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 2.\"); break\n\n    print(f\"\\nTraining Finished!\\nBest Stage 1 QWK: {best_val_qwk:.4f}\\nFinal Best QWK: {best_val_qwk_stage2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T16:55:54.211111Z","iopub.execute_input":"2025-09-10T16:55:54.211422Z","iopub.status.idle":"2025-09-10T19:06:49.505448Z","shell.execute_reply.started":"2025-09-10T16:55:54.211394Z","shell.execute_reply":"2025-09-10T19:06:49.504557Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, Model: efficientnet_b3, Image Size: 384\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:98: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size), name=\"ben_graham_preprocess\"),\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_36/372004193.py:111: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size), name=\"ben_graham_preprocess\"),\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\n     STARTING STAGE 1: WEIGHTED FOCAL LOSS\n==================================================\n\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:339: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.4438, Acc: 0.2464, QWK: 0.1653\nValid -> Loss: 0.1856, Acc: 0.1503, QWK: 0.4354\nVal QWK improved from -1.0000 to 0.4354. Saving model...\n\nEpoch 2/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.3393, Acc: 0.3328, QWK: 0.3459\nValid -> Loss: 0.1556, Acc: 0.2022, QWK: 0.6399\nVal QWK improved from 0.4354 to 0.6399. Saving model...\n\nEpoch 3/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2983, Acc: 0.3509, QWK: 0.3603\nValid -> Loss: 0.1411, Acc: 0.2158, QWK: 0.6129\n\nEpoch 4/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2794, Acc: 0.3867, QWK: 0.4052\nValid -> Loss: 0.1416, Acc: 0.2186, QWK: 0.5195\n\nEpoch 5/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2482, Acc: 0.4109, QWK: 0.4062\nValid -> Loss: 0.1400, Acc: 0.2268, QWK: 0.5981\n\nEpoch 6/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2475, Acc: 0.4014, QWK: 0.4336\nValid -> Loss: 0.1432, Acc: 0.2350, QWK: 0.6654\nVal QWK improved from 0.6399 to 0.6654. Saving model...\n\nEpoch 7/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2167, Acc: 0.4195, QWK: 0.4325\nValid -> Loss: 0.1312, Acc: 0.3087, QWK: 0.6807\nVal QWK improved from 0.6654 to 0.6807. Saving model...\n\nEpoch 8/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2073, Acc: 0.4283, QWK: 0.4407\nValid -> Loss: 0.1271, Acc: 0.3525, QWK: 0.7137\nVal QWK improved from 0.6807 to 0.7137. Saving model...\n\nEpoch 9/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1960, Acc: 0.4096, QWK: 0.4376\nValid -> Loss: 0.1288, Acc: 0.4454, QWK: 0.7233\nVal QWK improved from 0.7137 to 0.7233. Saving model...\n\nEpoch 10/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd828028c6749abac102d2ec8105f15"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1978, Acc: 0.4427, QWK: 0.4771\nValid -> Loss: 0.1307, Acc: 0.2978, QWK: 0.6990\n\nEpoch 11/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03646bbd3d994027a5b7eb5492b93465"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf5004f324f4267aca72e053f3a52ea"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1991, Acc: 0.4471, QWK: 0.4641\nValid -> Loss: 0.1403, Acc: 0.3825, QWK: 0.6565\n\nEpoch 12/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ebad1ae227469e8e63b9c1c2da06bb"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb924be1f2f54500966ac003fa8cb32d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2065, Acc: 0.4447, QWK: 0.4840\nValid -> Loss: 0.1353, Acc: 0.3825, QWK: 0.6700\n\nEpoch 13/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1922, Acc: 0.4560, QWK: 0.4925\nValid -> Loss: 0.1325, Acc: 0.4290, QWK: 0.7101\n\nEpoch 14/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1886, Acc: 0.4567, QWK: 0.4739\nValid -> Loss: 0.1322, Acc: 0.4781, QWK: 0.7123\nEarly stopping in Stage 1.\n\n==================================================\n     STARTING STAGE 2: HYBRID LOSS FINE-TUNING\n==================================================\n\nEpoch 1/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1774, Acc: 0.6857, QWK: 0.9012\nValid -> Loss: 0.2355, Acc: 0.7213, QWK: 0.8650\nVal QWK improved from 0.7233 to 0.8650. Saving final model...\n\nEpoch 2/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1209, Acc: 0.8106, QWK: 0.9449\nValid -> Loss: 0.1931, Acc: 0.7705, QWK: 0.8943\nVal QWK improved from 0.8650 to 0.8943. Saving final model...\n\nEpoch 3/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1042, Acc: 0.8433, QWK: 0.9529\nValid -> Loss: 0.1684, Acc: 0.7650, QWK: 0.9013\nVal QWK improved from 0.8943 to 0.9013. Saving final model...\n\nEpoch 4/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0989, Acc: 0.8604, QWK: 0.9584\nValid -> Loss: 0.1904, Acc: 0.7760, QWK: 0.8877\n\nEpoch 5/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0870, Acc: 0.8720, QWK: 0.9621\nValid -> Loss: 0.1616, Acc: 0.7787, QWK: 0.9045\nVal QWK improved from 0.9013 to 0.9045. Saving final model...\n\nEpoch 6/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0791, Acc: 0.8925, QWK: 0.9681\nValid -> Loss: 0.1647, Acc: 0.7787, QWK: 0.9007\n\nEpoch 7/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0728, Acc: 0.8990, QWK: 0.9706\nValid -> Loss: 0.1576, Acc: 0.8060, QWK: 0.9139\nVal QWK improved from 0.9045 to 0.9139. Saving final model...\n\nEpoch 8/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0635, Acc: 0.9222, QWK: 0.9790\nValid -> Loss: 0.1589, Acc: 0.7923, QWK: 0.9076\n\nEpoch 9/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0727, Acc: 0.8986, QWK: 0.9690\nValid -> Loss: 0.1602, Acc: 0.8115, QWK: 0.9119\n\nEpoch 10/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0739, Acc: 0.9096, QWK: 0.9716\nValid -> Loss: 0.1653, Acc: 0.7978, QWK: 0.9043\n\nEpoch 11/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0676, Acc: 0.9068, QWK: 0.9715\nValid -> Loss: 0.1524, Acc: 0.7787, QWK: 0.9127\n\nEpoch 12/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:273: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/372004193.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.0645, Acc: 0.9232, QWK: 0.9787\nValid -> Loss: 0.1574, Acc: 0.7923, QWK: 0.9072\nEarly stopping in Stage 2.\n\nTraining Finished!\nBest Stage 1 QWK: 0.7233\nFinal Best QWK: 0.9139\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport cv2  # The library is imported as cv2\nimport os\nimport timm\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# =============================================================================\n# REUSED CLASSES AND FUNCTIONS\n# =============================================================================\n\nclass CFG:\n    # Set these to match your final successful training run\n    MODEL_NAME = 'efficientnet_b3'\n    IMG_SIZE = 384\n    \n    # CORRECTED Paths to your test data\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n    TEST_DIR = os.path.join(BASE_PATH, \"test_images\", \"test_images\")\n    \n    # Path to your saved model\n    MODEL_PATH = \"best_model_final.pth\"\n    \n    BATCH_SIZE = 16 \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Model Class ---\nclass EfficientNetOrdinal(nn.Module):\n    def __init__(self, model_name, num_classes=5, pretrained=False):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes - 1)\n        )\n    def forward(self, x):\n        feat = self.backbone(x)\n        return self.classifier(feat)\n\n# --- Preprocessing and Dataset ---\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\n\ndef preprocess_ben_graham(image_np, output_size):\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY) # FIXED: cv2\n        if gray.mean() < 15:\n             return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA) # FIXED: cv2\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY) # FIXED: cv2\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # FIXED: cv2\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour) # FIXED: cv2\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception:\n        pass\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA) # FIXED: cv2\n    b, g, r = cv2.split(image_resized) # FIXED: cv2\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)) # FIXED: cv2\n    g = clahe.apply(g)\n    return cv2.merge((b, g, r)) # FIXED: cv2\n\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = cv2.imread(img_path) # FIXED: cv2\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # FIXED: cv2\n        if self.transform:\n            augmented = self.transform(image=img)\n            img = augmented['image']\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\n# --- Utility ---\ndef ordinal_to_class(outputs):\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\n\n# =============================================================================\n# TESTING FUNCTION\n# =============================================================================\ndef test_model():\n    print(\"--- Starting Final Model Evaluation ---\")\n    \n    # 1. Load Data\n    test_df = pd.read_csv(CFG.TEST_CSV)\n    print(f\"Test data loaded: {len(test_df)} samples from {CFG.TEST_CSV.split('/')[-1]}\")\n    \n    # 2. Define Transforms (no augmentations for testing)\n    test_transform = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, CFG.IMG_SIZE)),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    # 3. Create Dataset and DataLoader\n    test_dataset = DiabeticRetinopathyDataset(test_df, CFG.TEST_DIR, transform=test_transform)\n    test_loader = DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # 4. Load Model\n    model = EfficientNetOrdinal(CFG.MODEL_NAME, pretrained=False).to(CFG.DEVICE)\n    model.load_state_dict(torch.load(CFG.MODEL_PATH, map_location=CFG.DEVICE))\n    model.eval()\n    print(f\"Model loaded from {CFG.MODEL_PATH}\")\n    \n    # 5. Get Predictions\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Predicting on test set\"):\n            images = images.to(CFG.DEVICE)\n            \n            outputs = model(images)\n            preds = ordinal_to_class(outputs)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    # 6. Calculate and Display Metrics\n    print(\"\\n--- Final Test Results ---\")\n    qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    print(f\"Quadratic Weighted Kappa: {qwk:.4f}\")\n    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    \n    # 7. Detailed Report\n    print(\"\\n--- Classification Report ---\")\n    print(classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(5)]))\n    \n    # 8. Confusion Matrix\n    print(\"\\n--- Confusion Matrix ---\")\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# --- RUN THE EVALUATION ---\ntest_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T19:24:36.293147Z","iopub.execute_input":"2025-09-10T19:24:36.293941Z","iopub.status.idle":"2025-09-10T19:25:19.624411Z","shell.execute_reply.started":"2025-09-10T19:24:36.293903Z","shell.execute_reply":"2025-09-10T19:25:19.623564Z"}},"outputs":[{"name":"stdout","text":"--- Starting Final Model Evaluation ---\nTest data loaded: 366 samples from test.csv\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1543076114.py:109: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, CFG.IMG_SIZE)),\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from best_model_final.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting on test set:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ed9ee672fe44898142df9b331b5b81"}},"metadata":{}},{"name":"stdout","text":"\n--- Final Test Results ---\nQuadratic Weighted Kappa: 0.9129\nAccuracy: 0.8142 (81.42%)\n\n--- Classification Report ---\n              precision    recall  f1-score   support\n\n     Class 0       0.99      0.97      0.98       199\n     Class 1       0.46      0.57      0.51        30\n     Class 2       0.72      0.69      0.71        87\n     Class 3       0.30      0.47      0.36        17\n     Class 4       0.79      0.58      0.67        33\n\n    accuracy                           0.81       366\n   macro avg       0.65      0.66      0.65       366\nweighted avg       0.84      0.81      0.82       366\n\n\n--- Confusion Matrix ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"----------------------","metadata":{}},{"cell_type":"markdown","source":"**SWIM V2**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\n# =============================================================================\n# CONFIGURATION FOR SWIN TRANSFORMER V2\n# =============================================================================\nclass CFG:\n    MODEL_NAME = 'swinv2_tiny_window8_256'\n    IMG_SIZE = 256\n    BATCH_SIZE = 16\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    TRAIN_CSV = os.path.join(BASE_PATH, \"train_1.csv\")\n    VAL_CSV   = os.path.join(BASE_PATH, \"valid.csv\")\n    TRAIN_DIR = os.path.join(BASE_PATH, \"train_images\", \"train_images\")\n    VAL_DIR   = os.path.join(BASE_PATH, \"val_images\", \"val_images\")\n    S1_EPOCHS = 15\n    S1_LR = 5e-5\n    S1_WEIGHT_DECAY = 0.05\n    S1_USE_MIXUP = True\n    S2_EPOCHS = 15\n    S2_LR = 1e-5 \n    S2_WEIGHT_DECAY = 0.05\n    S2_USE_MIXUP = False\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    NUM_WORKERS = 2\n    PATIENCE = 5\n    SEED = 42\n    LABEL_SMOOTHING = 0.05\n    SAVE_PATH_S1 = \"best_model_swin_stage1.pth\"\n    SAVE_PATH_FINAL = \"best_model_swin_final.pth\"\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG.SEED)\n\n# =============================================================================\n# PREPROCESSING & AUGMENTATIONS (Unchanged)\n# =============================================================================\ndef preprocess_ben_graham(image_np, output_size):\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n        if gray.mean() < 15:\n             return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception:\n        pass\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(image_resized)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\ndef get_transforms(img_size):\n    pre_transforms = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7),\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n    ])\n    post_transforms = A.Compose([\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    val_transforms = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n    return pre_transforms, post_transforms, val_transforms\n\n# =============================================================================\n# DATASET, LOSSES (Unchanged)\n# =============================================================================\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, pre_transform=None, post_transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.pre_transform = pre_transform\n        self.post_transform = post_transform\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.pre_transform:\n            augmented = self.pre_transform(image=img)\n            img = augmented['image']\n        if self.post_transform:\n            tensor_aug = self.post_transform(image=img)\n            img = tensor_aug['image']\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\nclass WeightedOrdinalFocalLoss(nn.Module):\n    def __init__(self, num_classes=5, gamma=2.0, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.gamma = gamma\n        self.class_weights = class_weights\n        self.label_smoothing = label_smoothing\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n    def forward(self, outputs, targets):\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, t in enumerate(targets):\n            if t > 0:\n                ordinal_targets[i, :t] = 1.0\n        if self.label_smoothing > 0.0:\n            ordinal_targets = ordinal_targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n        bce = self.bce(outputs, ordinal_targets)\n        if self.class_weights is not None:\n            weights = self.class_weights[targets].view(-1, 1).expand(-1, outputs.shape[1])\n            bce = bce * weights\n        pt = torch.exp(-bce)\n        focal = (1 - pt) ** self.gamma * bce\n        return focal.mean()\n\nclass SmoothKappaLoss(nn.Module):\n    def __init__(self, num_classes=5, eps=1e-7):\n        super().__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes):\n                W[i,j] = ((i - j)**2) / ((num_classes - 1)**2)\n        self.register_buffer(\"W\", W)\n    def forward(self, outputs, targets):\n        device = outputs.device\n        B = outputs.size(0)\n        probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes-1):\n            class_probs[:, k] = probs[:, k-1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        hist_true = one_hot.sum(dim=0)\n        hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        W = self.W.to(device)\n        obs = torch.sum(W * conf_mat)\n        exp = torch.sum(W * expected)\n        kappa = 1.0 - (B * obs) / (exp + self.eps)\n        return 1.0 - kappa\n\n# =============================================================================\n# CORRECTED SWIN TRANSFORMER MODEL CLASS\n# =============================================================================\nclass SwinOrdinal(nn.Module):\n    def __init__(self, model_name, num_classes=5, pretrained=True):\n        super().__init__()\n        # THE FIX IS HERE: added global_pool='avg'\n        self.backbone = timm.create_model(\n            model_name, \n            pretrained=pretrained, \n            num_classes=0, \n            global_pool='avg' \n        )\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes - 1)\n        )\n    def forward(self, x):\n        feat = self.backbone(x)\n        return self.classifier(feat)\n\n# =============================================================================\n# UTILITIES & TRAINING LOOPS (Unchanged)\n# =============================================================================\ndef ordinal_to_class(outputs):\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\ndef calculate_metrics(outputs, targets):\n    preds = ordinal_to_class(outputs).cpu().numpy()\n    targets_np = targets.cpu().numpy()\n    acc = accuracy_score(targets_np, preds)\n    qwk = cohen_kappa_score(targets_np, preds, weights='quadratic')\n    return acc, qwk\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\ndef mixup_data(x, y, alpha=0.4):\n    if alpha > 0: lam = np.random.beta(alpha, alpha)\n    else: lam = 1\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, device, use_mixup):\n    model.train()\n    running_loss = 0.0\n    all_out, all_t = [], []\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    for images, targets in pbar:\n        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if use_mixup: images, targets_a, targets_b, lam = mixup_data(images, targets)\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            if use_mixup: loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n            else: loss = criterion(outputs, targets)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        all_out.append(outputs.detach())\n        all_t.append(targets.detach())\n        pbar.set_postfix(loss=loss.item())\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_out, all_t = [], []\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n        for images, targets in pbar:\n            images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_out.append(outputs)\n            all_t.append(targets)\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\n# =============================================================================\n# MAIN TRAINING PIPELINE\n# =============================================================================\ndef main():\n    print(f\"Device: {CFG.DEVICE}, Model: {CFG.MODEL_NAME}, Image Size: {CFG.IMG_SIZE}\")\n    train_df = pd.read_csv(CFG.TRAIN_CSV)\n    val_df = pd.read_csv(CFG.VAL_CSV)\n\n    pre_tf, post_tf, val_tf = get_transforms(CFG.IMG_SIZE)\n\n    train_ds = DiabeticRetinopathyDataset(train_df, CFG.TRAIN_DIR, pre_transform=pre_tf, post_transform=post_tf)\n    val_ds   = DiabeticRetinopathyDataset(val_df, CFG.VAL_DIR, pre_transform=val_tf)\n\n    class_weights_sampler = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    sample_weights = np.array([class_weights_sampler[int(l)] for l in train_df['diagnosis']])\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, sampler=sampler, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=CFG.BATCH_SIZE*2, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n\n    model = SwinOrdinal(CFG.MODEL_NAME, num_classes=5).to(CFG.DEVICE)\n    \n    class_weights_loss = torch.tensor(class_weights_sampler, dtype=torch.float).to(CFG.DEVICE)\n    focal_loss = WeightedOrdinalFocalLoss(num_classes=5, gamma=2.0, class_weights=class_weights_loss, label_smoothing=CFG.LABEL_SMOOTHING)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    \n    def hybrid_loss(outputs, targets):\n        return 0.7 * kappa_loss(outputs, targets) + 0.3 * focal_loss(outputs, targets)\n    \n    scaler = torch.cuda.amp.GradScaler()\n\n    # --- STAGE 1: WEIGHTED FOCAL LOSS ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 1: WEIGHTED FOCAL LOSS\\n\" + \"=\"*50)\n    opt = optim.AdamW(model.parameters(), lr=CFG.S1_LR, weight_decay=CFG.S1_WEIGHT_DECAY)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S1_EPOCHS)\n    best_val_qwk, patience_counter = -1, 0\n\n    for epoch in range(CFG.S1_EPOCHS):\n        clear_memory()\n        print(f\"\\nEpoch {epoch+1}/{CFG.S1_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, focal_loss, scaler, CFG.DEVICE, CFG.S1_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, focal_loss, CFG.DEVICE)\n        sched.step()\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n        if val_qwk > best_val_qwk:\n            print(f\"Val QWK improved from {best_val_qwk:.4f} to {val_qwk:.4f}. Saving model...\")\n            best_val_qwk, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), CFG.SAVE_PATH_S1)\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 1.\"); break\n    \n    # --- STAGE 2: HYBRID LOSS FINE-TUNING ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 2: HYBRID LOSS FINE-TUNING\\n\" + \"=\"*50)\n    model.load_state_dict(torch.load(CFG.SAVE_PATH_S1))\n    opt = optim.AdamW(model.parameters(), lr=CFG.S2_LR, weight_decay=CFG.S2_WEIGHT_DECAY)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S2_EPOCHS)\n    best_val_qwk_stage2, patience_counter = best_val_qwk, 0\n\n    for epoch in range(CFG.S2_EPOCHS):\n        clear_memory()\n        print(f\"\\nEpoch {epoch+1}/{CFG.S2_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, hybrid_loss, scaler, CFG.DEVICE, CFG.S2_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, hybrid_loss, CFG.DEVICE)\n        sched.step()\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n        if val_qwk > best_val_qwk_stage2:\n            print(f\"Val QWK improved from {best_val_qwk_stage2:.4f} to {val_qwk:.4f}. Saving final model...\")\n            best_val_qwk_stage2, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), CFG.SAVE_PATH_FINAL)\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 2.\"); break\n\n    print(f\"\\nTraining Finished!\\nBest Stage 1 QWK: {best_val_qwk:.4f}\\nFinal Best QWK: {best_val_qwk_stage2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T23:12:55.641258Z","iopub.execute_input":"2025-09-10T23:12:55.642252Z","iopub.status.idle":"2025-09-11T00:53:36.433404Z","shell.execute_reply.started":"2025-09-10T23:12:55.642208Z","shell.execute_reply":"2025-09-11T00:53:36.432495Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, Model: swinv2_tiny_window8_256, Image Size: 256\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:85: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)),\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_36/218312670.py:97: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)),\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\n     STARTING STAGE 1: WEIGHTED FOCAL LOSS\n==================================================\n\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:303: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.4184, Acc: 0.2502, QWK: 0.3046\nValid -> Loss: 0.1634, Acc: 0.1831, QWK: 0.6888\nVal QWK improved from -1.0000 to 0.6888. Saving model...\n\nEpoch 2/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.3689, Acc: 0.3085, QWK: 0.3123\nValid -> Loss: 0.1558, Acc: 0.1803, QWK: 0.6726\n\nEpoch 3/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.3289, Acc: 0.3433, QWK: 0.4021\nValid -> Loss: 0.1575, Acc: 0.1831, QWK: 0.6685\n\nEpoch 4/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.3249, Acc: 0.3413, QWK: 0.4119\nValid -> Loss: 0.1355, Acc: 0.2077, QWK: 0.6624\n\nEpoch 5/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.3127, Acc: 0.3573, QWK: 0.3810\nValid -> Loss: 0.1329, Acc: 0.2240, QWK: 0.6691\n\nEpoch 6/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2713, Acc: 0.3840, QWK: 0.4373\nValid -> Loss: 0.1204, Acc: 0.2213, QWK: 0.6537\nEarly stopping in Stage 1.\n\n==================================================\n     STARTING STAGE 2: HYBRID LOSS FINE-TUNING\n==================================================\n\nEpoch 1/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.3094, Acc: 0.5420, QWK: 0.8218\nValid -> Loss: 0.2002, Acc: 0.6393, QWK: 0.8656\nVal QWK improved from 0.6888 to 0.8656. Saving final model...\n\nEpoch 2/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2531, Acc: 0.6201, QWK: 0.8677\nValid -> Loss: 0.2094, Acc: 0.6175, QWK: 0.8647\n\nEpoch 3/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2174, Acc: 0.6693, QWK: 0.8916\nValid -> Loss: 0.1783, Acc: 0.6776, QWK: 0.8775\nVal QWK improved from 0.8656 to 0.8775. Saving final model...\n\nEpoch 4/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.2202, Acc: 0.6850, QWK: 0.8868\nValid -> Loss: 0.1645, Acc: 0.6995, QWK: 0.8936\nVal QWK improved from 0.8775 to 0.8936. Saving final model...\n\nEpoch 5/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1998, Acc: 0.6850, QWK: 0.8957\nValid -> Loss: 0.1549, Acc: 0.7022, QWK: 0.8986\nVal QWK improved from 0.8936 to 0.8986. Saving final model...\n\nEpoch 6/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1978, Acc: 0.6857, QWK: 0.8988\nValid -> Loss: 0.1487, Acc: 0.7022, QWK: 0.8943\n\nEpoch 7/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1767, Acc: 0.7140, QWK: 0.9092\nValid -> Loss: 0.1637, Acc: 0.6967, QWK: 0.8870\n\nEpoch 8/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1657, Acc: 0.7331, QWK: 0.9148\nValid -> Loss: 0.1531, Acc: 0.7104, QWK: 0.9001\nVal QWK improved from 0.8986 to 0.9001. Saving final model...\n\nEpoch 9/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1604, Acc: 0.7430, QWK: 0.9203\nValid -> Loss: 0.1465, Acc: 0.7131, QWK: 0.8998\n\nEpoch 10/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1463, Acc: 0.7594, QWK: 0.9304\nValid -> Loss: 0.1424, Acc: 0.7158, QWK: 0.9015\nVal QWK improved from 0.9001 to 0.9015. Saving final model...\n\nEpoch 11/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1578, Acc: 0.7440, QWK: 0.9198\nValid -> Loss: 0.1425, Acc: 0.7158, QWK: 0.9038\nVal QWK improved from 0.9015 to 0.9038. Saving final model...\n\nEpoch 12/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1445, Acc: 0.7536, QWK: 0.9268\nValid -> Loss: 0.1388, Acc: 0.7268, QWK: 0.9039\nVal QWK improved from 0.9038 to 0.9039. Saving final model...\n\nEpoch 13/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1432, Acc: 0.7621, QWK: 0.9309\nValid -> Loss: 0.1392, Acc: 0.7131, QWK: 0.9042\nVal QWK improved from 0.9039 to 0.9042. Saving final model...\n\nEpoch 14/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1433, Acc: 0.7509, QWK: 0.9241\nValid -> Loss: 0.1399, Acc: 0.7131, QWK: 0.9001\n\nEpoch 15/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/218312670.py:265: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.1410, Acc: 0.7515, QWK: 0.9280\nValid -> Loss: 0.1413, Acc: 0.7158, QWK: 0.9012\n\nTraining Finished!\nBest Stage 1 QWK: 0.6888\nFinal Best QWK: 0.9042\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport timm\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# =============================================================================\n# REUSED CLASSES AND FUNCTIONS\n# =============================================================================\n\nclass CFG:\n    # --- CONFIGURATION FOR THE SWIN TRANSFORMER YOU JUST TRAINED ---\n    MODEL_NAME = 'swinv2_tiny_window8_256'\n    IMG_SIZE = 256\n    \n    # --- PATHS TO YOUR TEST DATA ---\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n    TEST_DIR = os.path.join(BASE_PATH, \"test_images\", \"test_images\")\n    \n    # --- PATH TO YOUR SAVED SWIN MODEL ---\n    MODEL_PATH = \"best_model_swin_final.pth\"\n    \n    # --- INFERENCE CONFIGURATION ---\n    BATCH_SIZE = 16 \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    NUM_WORKERS = 2\n\n# --- Model Class (Must match the trained model) ---\nclass SwinOrdinal(nn.Module):\n    def __init__(self, model_name, num_classes=5, pretrained=False):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name, \n            pretrained=pretrained, \n            num_classes=0, \n            global_pool='avg'\n        )\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes - 1)\n        )\n    def forward(self, x):\n        feat = self.backbone(x)\n        return self.classifier(feat)\n\n# --- Preprocessing and Dataset (Must match the trained model) ---\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\n\ndef preprocess_ben_graham(image_np, output_size):\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n        if gray.mean() < 15:\n             return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception:\n        pass\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(image_resized)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=img)\n            img = augmented['image']\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\n# --- Utility ---\ndef ordinal_to_class(outputs):\n    probs = torch.sigmoid(outputs)\n    return torch.sum(probs > 0.5, dim=1).long()\n\n\n# =============================================================================\n# TESTING FUNCTION\n# =============================================================================\ndef test_model():\n    print(\"--- Starting Final Model Evaluation for Swin Transformer ---\")\n    \n    # 1. Load Data\n    test_df = pd.read_csv(CFG.TEST_CSV)\n    print(f\"Test data loaded: {len(test_df)} samples from {CFG.TEST_CSV.split('/')[-1]}\")\n    \n    # 2. Define Transforms (no augmentations for testing)\n    test_transform = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, CFG.IMG_SIZE)),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    # 3. Create Dataset and DataLoader\n    test_dataset = DiabeticRetinopathyDataset(test_df, CFG.TEST_DIR, transform=test_transform)\n    test_loader = DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    \n    # 4. Load Model\n    model = SwinOrdinal(CFG.MODEL_NAME, pretrained=False).to(CFG.DEVICE)\n    model.load_state_dict(torch.load(CFG.MODEL_PATH, map_location=CFG.DEVICE))\n    model.eval()\n    print(f\"Model loaded from {CFG.MODEL_PATH}\")\n    \n    # 5. Get Predictions\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Predicting on test set\"):\n            images = images.to(CFG.DEVICE)\n            \n            outputs = model(images)\n            preds = ordinal_to_class(outputs)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    # 6. Calculate and Display Metrics\n    print(\"\\n--- Final Test Results (Swin Transformer) ---\")\n    qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    print(f\"Quadratic Weighted Kappa: {qwk:.4f}\")\n    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    \n    # 7. Detailed Report\n    print(\"\\n--- Classification Report ---\")\n    print(classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(5)]))\n    \n    # 8. Confusion Matrix\n    print(\"\\n--- Confusion Matrix ---\")\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix (Swin Transformer)\")\n    plt.show()\n\n# --- RUN THE EVALUATION ---\ntest_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T00:56:46.909050Z","iopub.execute_input":"2025-09-11T00:56:46.909801Z","iopub.status.idle":"2025-09-11T00:57:48.720296Z","shell.execute_reply.started":"2025-09-11T00:56:46.909761Z","shell.execute_reply":"2025-09-11T00:57:48.719676Z"}},"outputs":[{"name":"stdout","text":"--- Starting Final Model Evaluation for Swin Transformer ---\nTest data loaded: 366 samples from test.csv\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1099717778.py:115: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, CFG.IMG_SIZE)),\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from best_model_swin_final.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting on test set:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f850c6c4474451c92c6487ebf191faa"}},"metadata":{}},{"name":"stdout","text":"\n--- Final Test Results (Swin Transformer) ---\nQuadratic Weighted Kappa: 0.9045\nAccuracy: 0.7295 (72.95%)\n\n--- Classification Report ---\n              precision    recall  f1-score   support\n\n     Class 0       0.99      0.97      0.98       199\n     Class 1       0.38      0.77      0.51        30\n     Class 2       0.77      0.28      0.41        87\n     Class 3       0.20      0.71      0.31        17\n     Class 4       0.75      0.45      0.57        33\n\n    accuracy                           0.73       366\n   macro avg       0.62      0.63      0.55       366\nweighted avg       0.83      0.73      0.74       366\n\n\n--- Confusion Matrix ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAokAAAIjCAYAAABvUIGpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABiQ0lEQVR4nO3dd3gUVRfH8d8mJAuEFJIAIdJBegfpVTqKNEWaBESKAgoBxEhHJSigiBQLUkQQKyioKNKVIsUIIiAgiEovAZOQEJJ5/0D2dR1Kgllmk/1+eOZ52DuzM2d3dpOTc+/csRmGYQgAAAD4By+rAwAAAID7IUkEAACACUkiAAAATEgSAQAAYEKSCAAAABOSRAAAAJiQJAIAAMCEJBEAAAAmJIkAAAAwIUlElnbgwAE1b95cgYGBstlsWrZsWYbu/8iRI7LZbJo/f36G7jcza9SokRo1apSh+/z999+VPXt2fffddxm635sZN26cbDbbHTueOzp58qQefPBBhYSEyGazadq0aVaHlGF+/vlnZcuWTT/99JPVoQBuiyQRLnfo0CH169dPxYoVU/bs2RUQEKC6devq1Vdf1aVLl1x67IiICO3evVsvvPCCFi5cqOrVq7v0eHdSz549ZbPZFBAQcN338cCBA7LZbLLZbJoyZUq693/s2DGNGzdOMTExGRDtfzNhwgTVrFlTdevWdWpfvny5GjZsqLx58ypnzpwqVqyYOnXqpJUrV1oU6fU1atTIcS5utowbN87qUJ0MGTJEX331laKiorRw4UK1bNnS6pAyTNmyZXXfffdpzJgxVocCuK1sVgeArO3zzz/XQw89JLvdrh49eqh8+fK6fPmyvv32Ww0fPlx79uzRm2++6ZJjX7p0SZs3b9bIkSM1cOBAlxyjcOHCunTpknx8fFyy/1vJli2bEhIStHz5cnXq1Mlp3aJFi5Q9e3YlJibe1r6PHTum8ePHq0iRIqpcuXKan/f111/f1vFu5PTp01qwYIEWLFjg1D5lyhQNHz5cDRs2VFRUlHLmzKmDBw/qm2++0ZIlS/5zQjNq1Cg988wz/2kf14wcOVKPPfaY4/G2bds0ffp0PfvssypTpoyjvWLFihlyvIyyZs0atW3bVsOGDbM6FJfo37+/WrdurUOHDql48eJWhwO4HZJEuMzhw4fVuXNnFS5cWGvWrFH+/Pkd6wYMGKCDBw/q888/d9nxT58+LUkKCgpy2TFsNpuyZ8/usv3fit1uV926dfXee++ZksTFixfrvvvu08cff3xHYklISFDOnDnl6+uboft99913lS1bNrVp08bRduXKFT333HNq1qzZdZPSU6dO/efjZsuWTdmyZcyPyGbNmjk9zp49u6ZPn65mzZrdtGs+Pj5efn5+GRLD7Th16lSGfn8SExPl6+srLy/rOrGuXLmi1NRU+fr6qmnTpsqdO7cWLFigCRMmWBYT4K7obobLvPTSS4qLi9Pbb7/tlCBeU6JECT311FOOx9d+8RcvXlx2u11FihTRs88+q6SkJKfnFSlSRPfff7++/fZb1ahRQ9mzZ1exYsX0zjvvOLYZN26cChcuLEkaPny4bDabihQpIulqN+21///T9cagrVq1SvXq1VNQUJBy5cqlUqVK6dlnn3Wsv9GYxDVr1qh+/fry8/NTUFCQ2rZtq7179173eAcPHlTPnj0VFBSkwMBA9erVSwkJCTd+Y/+la9eu+vLLLxUbG+to27Ztmw4cOKCuXbuatj937pyGDRumChUqKFeuXAoICFCrVq30448/OrZZt26d7rnnHklSr169HN2h115no0aNVL58ee3YsUMNGjRQzpw5He/Lv8ckRkREKHv27KbX36JFC+XOnVvHjh276etbtmyZatasqVy5cjnazpw5o4sXL5q6n6/JmzevJMkwDIWGhioyMtKxLjU1VUFBQfL29nZ6z1588UVly5ZNcXFxkq7/ebDZbBo4cKCWLVum8uXLy263q1y5chnSvX3teD///LO6du2q3Llzq169epKkXbt2qWfPno4hG2FhYXr00Ud19uzZ6+4jLZ+pm32258+fL5vNJsMwNHPmTMf5v+bXX3/VQw89pODgYOXMmVO1atUy/cG3bt062Ww2LVmyRKNGjdJdd92lnDlz6uLFi+rZs6dy5cqlo0eP6v7771euXLl01113aebMmZKk3bt3695775Wfn58KFy6sxYsXm96v2NhYDR48WAULFpTdbleJEiX04osvKjU11bHNte/nlClTNG3aNMfPlp9//lmS5OPjo0aNGunTTz+93dMGZGlUEuEyy5cvV7FixVSnTp00bf/YY49pwYIFevDBBzV06FBt3bpV0dHR2rt3r5YuXeq07cGDB/Xggw+qd+/eioiI0Ny5c9WzZ09Vq1ZN5cqVU4cOHRQUFKQhQ4aoS5cuat26tVOSkRZ79uzR/fffr4oVK2rChAmy2+06ePDgLS+e+Oabb9SqVSsVK1ZM48aN06VLl/Taa6+pbt262rlzpylB7dSpk4oWLaro6Gjt3LlTc+bMUd68efXiiy+mKc4OHTqof//++uSTT/Too49KulpFLF26tKpWrWra/tdff9WyZcv00EMPqWjRojp58qTeeOMNNWzYUD///LPCw8NVpkwZTZgwQWPGjFHfvn1Vv359SXI6l2fPnlWrVq3UuXNnde/eXfny5btufK+++qrWrFmjiIgIbd68Wd7e3nrjjTf09ddfa+HChQoPD7/ha0tOTta2bdv0+OOPO7XnzZtXOXLk0PLlyzVo0CAFBwdf9/k2m01169bVhg0bHG27du3ShQsX5OXlpe+++0733XefJGnjxo2qUqXKLT8n3377rT755BM98cQT8vf31/Tp09WxY0cdPXpUISEhN31uWjz00EO6++67NXHiRBmGIelqQvfrr7+qV69eCgsLcwzT2LNnj7Zs2WJKZm/1mbrVZ7tBgwZauHChHnnkETVr1kw9evRw7PvkyZOqU6eOEhIS9OSTTyokJEQLFizQAw88oI8++kjt27d3iuW5556Tr6+vhg0bpqSkJEelOSUlRa1atVKDBg300ksvadGiRRo4cKD8/Pw0cuRIdevWTR06dNDrr7+uHj16qHbt2ipatKikq1Xrhg0b6s8//1S/fv1UqFAhbdq0SVFRUTp+/LjpApt58+YpMTFRffv2ld1ud/q8VKtWTZ9++qkuXryogICA/3z+gCzFAFzgwoULhiSjbdu2ado+JibGkGQ89thjTu3Dhg0zJBlr1qxxtBUuXNiQZGzYsMHRdurUKcNutxtDhw51tB0+fNiQZEyePNlpnxEREUbhwoVNMYwdO9b451filVdeMSQZp0+fvmHc144xb948R1vlypWNvHnzGmfPnnW0/fjjj4aXl5fRo0cP0/EeffRRp322b9/eCAkJueEx//k6/Pz8DMMwjAcffNBo0qSJYRiGkZKSYoSFhRnjx4+/7nuQmJhopKSkmF6H3W43JkyY4Gjbtm2b6bVd07BhQ0OS8frrr193XcOGDZ3avvrqK0OS8fzzzxu//vqrkStXLqNdu3a3fI0HDx40JBmvvfaaad2YMWMMSYafn5/RqlUr44UXXjB27Nhh2m7y5MmGt7e3cfHiRcMwDGP69OlG4cKFjRo1ahgjRowwDOPqexYUFGQMGTLE8bx/fx4MwzAkGb6+vsbBgwcdbT/++OMNY7yRDz/80JBkrF271nS8Ll26mLZPSEgwtb333num70FaP1Np+WwbxtXXO2DAAKe2wYMHG5KMjRs3Otr++usvo2jRokaRIkUcn621a9cakoxixYqZ4o+IiDAkGRMnTnS0nT9/3siRI4dhs9mMJUuWONr37dtnSDLGjh3raHvuuecMPz8/45dffnHa7zPPPGN4e3sbR48eNQzj/9/PgIAA49SpU9d9jYsXLzYkGVu3br3pewF4Irqb4RIXL16UJPn7+6dp+y+++EKSnLoFJWno0KGSZOrKKlu2rKO6JUl58uRRqVKl9Ouvv952zP92bSzWp59+6tSFdTPHjx9XTEyMevbs6VStqFixopo1a+Z4nf/Uv39/p8f169fX2bNnHe9hWnTt2lXr1q3TiRMntGbNGp04ceK6Xc3S1XGM18aEpaSk6OzZs47uxp07d6b5mHa7Xb169UrTts2bN1e/fv00YcIEdejQQdmzZ9cbb7xxy+dd607NnTu3ad348eO1ePFiValSRV999ZVGjhypatWqqWrVqk5d2/Xr11dKSoo2bdok6WrFsH79+qpfv742btwoSfrpp58UGxvr9Jm6kaZNmzpd5FCxYkUFBARk2Gfv358HScqRI4fj/4mJiTpz5oxq1aolSdc9Z7f6TN3OZ/uaL774QjVq1HB0hUtSrly51LdvXx05csTRlXtNRESEU/z/9M+LeYKCglSqVCn5+fk5ja8tVaqUgoKCnN7fDz/8UPXr11fu3Ll15swZx9K0aVOlpKQ4VY4lqWPHjsqTJ891Y7j22Tpz5kwa3wHAc5AkwiWuddv89ddfadr+t99+k5eXl0qUKOHUHhYWpqCgIP32229O7YUKFTLtI3fu3Dp//vxtRmz28MMPq27dunrssceUL18+de7cWR988MFNf6lei7NUqVKmdWXKlNGZM2cUHx/v1P7v13Ltl1Z6Xkvr1q3l7++v999/X4sWLdI999xjei+vSU1N1SuvvKK7775bdrtdoaGhypMnj6MbNq3uuuuudF2kMmXKFAUHBysmJkbTp093jBtMC+Pvbtd/69KlizZu3Kjz58/r66+/VteuXfXDDz+oTZs2jqu6q1atqpw5czoSwmtJYoMGDbR9+3YlJiY61v0z8bkRV3/2rnWp/tO5c+f01FNPKV++fMqRI4fy5Mnj2O565+xWn6nb+Wxf89tvv93w831t/a1ej3T14p1/J26BgYEqUKCAqfs8MDDQ6f09cOCAVq5cqTx58jgtTZs2lWS+cOlGMUj//2x5+pyYwPUwJhEuERAQoPDw8HRPVJvWH9Te3t7Xbb9RMpGWY6SkpDg9zpEjhzZs2KC1a9fq888/18qVK/X+++/r3nvv1ddff33DGNLrv7yWa+x2uzp06KAFCxbo119/vel8exMnTtTo0aP16KOP6rnnnlNwcLC8vLw0ePDgdFWVblQdupEffvjB8ct79+7d6tKlyy2fc22M360SsICAADVr1kzNmjWTj4+PFixYoK1bt6phw4by8fFRzZo1tWHDBh08eFAnTpxQ/fr1lS9fPiUnJ2vr1q3auHGjSpcufcNq0z9lxPm6meu9r506ddKmTZs0fPhwVa5cWbly5VJqaqpatmx53XN2qxjv1Gf7Rq/nZjGm5f1NTU1Vs2bN9PTTT19325IlS6YpBun/n63Q0NAbbgN4KpJEuMz999+vN998U5s3b1bt2rVvum3hwoWVmpqqAwcOOM0bd/LkScXGxjquVM4IuXPndrqq9Zp/V0AkycvLS02aNFGTJk308ssva+LEiRo5cqTWrl3rqFr8+3VI0v79+03r9u3bp9DQUJdNadK1a1fNnTtXXl5e6ty58w23++ijj9S4cWO9/fbbTu2xsbFOvygzsrISHx+vXr16qWzZsqpTp45eeukltW/f3nEF9Y0UKlRIOXLk0OHDh9N8rOrVq2vBggU6fvy4o61+/fp68cUX9c033yg0NFSlS5eWzWZTuXLltHHjRm3cuFH333//bb8+Vzp//rxWr16t8ePHO038fODAgf+03/R+tq8pXLjwDT/f19a7WvHixRUXF3fTONPq8OHD8vLyMiWWAOhuhgs9/fTT8vPz02OPPaaTJ0+a1h86dEivvvqqpKvdpZJMVyW+/PLLkuS4AjUjFC9eXBcuXNCuXbscbcePHzddQX3u3DnTc69NKv3vaXmuyZ8/vypXrqwFCxY4JaI//fSTvv76a8frdIXGjRvrueee04wZMxQWFnbD7by9vU1Vrw8//FB//vmnU9u1ZPZ6CXV6jRgxQkePHtWCBQv08ssvq0iRIoqIiLjh+3iNj4+Pqlevru3btzu1JyQkaPPmzdd9zpdffinJucu/fv36SkpK0rRp01SvXj1HAly/fn0tXLhQx44dS9N4RCtcq6z9+5z9l1vk3c5n+5rWrVvr+++/d3r/4+Pj9eabb6pIkSIqW7bsbceVVp06ddLmzZv11VdfmdbFxsbqypUrad7Xjh07VK5cOQUGBmZkiECWQCURLlO8eHEtXrxYDz/8sMqUKeN0x5VNmzbpww8/VM+ePSVJlSpVUkREhN58803FxsaqYcOG+v7777VgwQK1a9dOjRs3zrC4OnfurBEjRqh9+/Z68sknlZCQoNmzZ6tkyZJOFwFMmDBBGzZs0H333afChQvr1KlTmjVrlgoUKHDTsWuTJ09Wq1atVLt2bfXu3dsxBU5gYKBLb7vm5eWlUaNG3XK7+++/XxMmTFCvXr1Up04d7d69W4sWLVKxYsWctitevLiCgoL0+uuvy9/fX35+fqpZs+ZNx3ddz5o1azRr1iyNHTvWMSXPvHnz1KhRI40ePVovvfTSTZ/ftm1bjRw50mmKkoSEBNWpU0e1atVSy5YtVbBgQcXGxmrZsmXauHGj2rVrpypVqjj2Ubt2bWXLlk379+9X3759He0NGjTQ7NmzJcltk8SAgADHNDHJycm666679PXXX6eruvpvt/vZlqRnnnlG7733nlq1aqUnn3xSwcHBWrBggQ4fPqyPP/74jkyUPXz4cH322We6//77HVNfxcfHa/fu3froo4905MiRNHUfJycna/369XriiSdcHjOQGZEkwqUeeOAB7dq1S5MnT9ann36q2bNny263q2LFipo6dar69Onj2HbOnDkqVqyY5s+fr6VLlyosLExRUVEaO3ZshsYUEhKipUuXKjIyUk8//bRjPrkDBw44JYkPPPCAjhw5orlz5+rMmTMKDQ1Vw4YNNX78+JtWHZo2baqVK1dq7NixGjNmjHx8fNSwYUO9+OKL6U6wXOHZZ59VfHy8Fi9erPfff19Vq1bV559/broF3bWxfVFRUerfv7+uXLmiefPmpes1/PXXX3r00UdVpUoVjRw50tFev359PfXUU5o6dao6dOjguFL3eh555BE988wz+uyzz9S9e3dJV6+Efeutt/T5559r3rx5OnHihLy9vVWqVClNnjxZTz75pNM+/Pz8VKVKFW3bts0pCbqWGBYsWPCOdJPersWLF2vQoEGaOXOmDMNQ8+bN9eWXX950jsmbud3PtiTly5dPmzZt0ogRI/Taa68pMTFRFStW1PLlyzO04n8zOXPm1Pr16zVx4kR9+OGHeueddxQQEKCSJUum6TVcs3r1ap07d04REREujhjInGxGRo22BgAX6d27t3755RfHVchARmjXrp1sNptpqAmAq0gSAbi9o0ePqmTJklq9evUNb8UHpMfevXtVoUIFxcTEqHz58laHA7glkkQAAACYcHUzAAAATEgSAQAAYEKSCAAAABOSRAAAAJiQJAIAAMAkS06mnaPKQKtDwN9ObZ5udQj4B59s/F0IwH1ltzArcWXucOmHGS7btyvxGwMAAAAmWbKSCAAAkC426mb/RpIIAABgs1kdgdshbQYAAIAJlUQAAAC6m014RwAAAGBCJREAAIAxiSZUEgEAAGBCJREAAIAxiSa8IwAAADChkggAAMCYRBOSRAAAALqbTXhHAAAAYEIlEQAAgO5mEyqJAAAAMKGSCAAAwJhEE94RAAAAmFBJBAAAYEyiCZVEAAAAmFBJBAAAYEyiCUkiAAAA3c0mpM0AAAAwoZIIAABAd7MJ7wgAAABMqCQCAABQSTThHQEAAIAJlUQAAAAvrm7+NyqJAAAAMKGSCAAAwJhEE5JEAAAAJtM2IW0GAACACZVEAAAAuptNeEcAAABgQiURAACAMYkmVBIBAABgQiURAACAMYkmvCMAAAAwoZIIAADAmEQTKokAAAA2L9ct6bRhwwa1adNG4eHhstlsWrZsmXOoNtt1l8mTJzu2KVKkiGn9pEmT0hUHSSIAAIAbiY+PV6VKlTRz5szrrj9+/LjTMnfuXNlsNnXs2NFpuwkTJjhtN2jQoHTFQXfzHVS3anEN6dFUVcsWUv48geo05E0tX7fLsT5vsL+ef6qtmtYuo8BcOfTtzoOKfOlDHTp62rHNayM7696apZQ/T6DiLiVpy4+HNerVT/XLkZNWvKQs79TJk3pt2lRt+m6DEhMTVaBgIY2dMFFly5W3OjSPtGTxIi2Y97bOnDmtkqVK65lnR6tCxYpWh+WROBfug3ORQdyou7lVq1Zq1arVDdeHhYU5Pf7000/VuHFjFStWzKnd39/ftG16UEm8g/xy2LX7lz81OPr9667/4JW+KlogVA8NfkO1ukzS0ePn9MXrg5Qzu69jmx/2/q6+495V5Q7P64EnZspms2nFrAHy8nKfD3dWcfHiBfXu2VXZsmXTqzPf1AefrNCQoSMUEBBgdWgeaeWXX2jKS9Hq98QALflwqUqVKq3H+/XW2bNnrQ7N43Au3AfnInNISkrSxYsXnZakpKQM2ffJkyf1+eefq3fv3qZ1kyZNUkhIiKpUqaLJkyfrypUr6do3SeId9PV3P2v8rBX6bO0u07oShfKqZsWievKFJdrx81Ed+O2Unpz4vrLbfdSpVTXHdnM/+U7f7Tyko8fPKWbfHxo/c7kK5g9W4fCQO/lSPMKCuXOUL19+jX1uospXqKi7ChRQrTp1VaBgIatD80gLF8xThwc7qV37jipeooRGjR2v7Nmza9knH1sdmsfhXLgPzkUGcuGYxOjoaAUGBjot0dHRGRL2ggUL5O/vrw4dOji1P/nkk1qyZInWrl2rfv36aeLEiXr66afTtW9Lu5vPnDmjuXPnavPmzTpx4oSkqyXUOnXqqGfPnsqTJ4+V4d1Rdt+rpyLx8v+zfMMwdPnyFdWpXFzzl242PSdndl/1eKCWDv9xRn+cOH/HYvUUG9avVa06dTVi2GDt3L5NefLm00MPd1b7jp2sDs3jJF++rL0/71HvPv0cbV5eXqpVq452/fiDhZF5Hs6F++BcZB5RUVGKjIx0arPb7Rmy77lz56pbt27Knj27U/s/j1exYkX5+vqqX79+io6OTvOxLaskbtu2TSVLltT06dMVGBioBg0aqEGDBgoMDNT06dNVunRpbd++/Zb7uV4J10hNuQOvIGPtP3JCR4+f03ODHlCQfw75ZPPW0J5NVSAst8JCA5227ftQfZ3+bqrObn5ZzeuW1X2Pz1Dylcz3mt3dn3/8ro8/WKJChQrrtdlv6cFOnTXlxYla8dkyq0PzOOdjzyslJUUhIc4V85CQEJ05c8aiqDwT58J9cC4ymM3mssVutysgIMBpyYgkcePGjdq/f78ee+yxW25bs2ZNXblyRUeOHEnz/i2rJA4aNEgPPfSQXn/9ddn+NVjUMAz1799fgwYN0ubN5graP0VHR2v8+PFObd757pFP/hoZHrMrXbmSqs5D39Lssd10fMNkXbmSojVb92vlt3tMY2mXfLlNq7fuU1hogAb3aKp3X3xU9/Z6WUmX0zfWADeXmmqobLlyGvDkEElS6TJldejgAX384RLd/0A7a4MDAHi8t99+W9WqVVOlSpVuuW1MTIy8vLyUN2/eNO/fsiTxxx9/1Pz5800JonR1/p8hQ4aoSpUqt9zP9Uq4eeuPyLA476Qf9v6uWp0nKSBXdvn6ZNOZ83Ha8M4w7fj5qNN2F+MSdTEuUYeOntb3u47o+IaX1PbeSvpg5Q6LIs+aQvOEqmix4k5tRYsV05pvvrYoIs+VOyi3vL29TYPxz549q9DQUIui8kycC/fBuchgbnRbvri4OB08eNDx+PDhw4qJiVFwcLAKFbo6Lv7ixYv68MMPNXXqVNPzN2/erK1bt6px48by9/fX5s2bNWTIEHXv3l25c+dOcxyWvSNhYWH6/vvvb7j++++/V758+W65n+uVcG1e3hkZ6h13MS5RZ87HqXihPKpatpBWrDNf6HKNzWaTTTb5+jCbUUarVLmqfvtXWf63344of3i4NQF5MB9fX5UpW05bt/y/ZyE1NVVbt25WxUq3/mMSGYdz4T44FxnMjSbT3r59u6pUqeIolkVGRqpKlSoaM2aMY5slS5bIMAx16dLF9Hy73a4lS5aoYcOGKleunF544QUNGTJEb775ZrrisCyzGDZsmPr27asdO3aoSZMmjoTw5MmTWr16td566y1NmTLFqvBcwi+Hr4oX/P/FOEXuClHFknfp/MUE/X7ivDo0raLT5+P0+4lzKn93uKYMf1DL1+3S6i37HNs/2KKaVm/eqzPn43RXviAN7dVcl5KS9dW3e6x6WVlW1+4RejSiq+bOeUPNmrfUnp92a+lHH2rkmPG3fjIy3CMRvTT62REqV668yleoqHcXLtClS5fUrn2HWz8ZGYpz4T44F1lTo0aNZBjGTbfp27ev+vbte911VatW1ZYtW/5zHJYliQMGDFBoaKheeeUVzZo1SykpVy+88Pb2VrVq1TR//nx16pS1riKtWrawvp7zlOPxS8Ouzoy+8LMt6jv2XYXlCdCLQzsob4i/Tpy5qEUrtir6zZWO7ZMuX1HdKsU1sGsj5Q7IqVNn/9K3Ow+qcc+pOn0+7o6/nqyuXPkKmvLydM2Y/ormvDFL4XcV0NCnn1Gr+9pYHZpHatmqtc6fO6dZM6brzJnTKlW6jGa9MUchdKvdcZwL98G5yEBuNJm2u7AZt0pV74Dk5GTHlVihoaHy8fH5T/vLUWVgRoSFDHBq83SrQ8A/+GRznzE3APBv2S0cOZXjgdku2/elzx532b5dyS0Gsvn4+Ch//vxWhwEAADyVG1244i54RwAAAGDiFpVEAAAASzEm0YRKIgAAAEyoJAIAADAm0YQkEQAAgO5mE9JmAAAAmFBJBAAAHs9GJdGESiIAAABMqCQCAACPRyXRjEoiAAAATKgkAgAAUEg0oZIIAAAAEyqJAADA4zEm0YwkEQAAeDySRDO6mwEAAGBCJREAAHg8KolmVBIBAABgQiURAAB4PCqJZlQSAQAAYEIlEQAAgEKiCZVEAAAAmFBJBAAAHo8xiWZUEgEAAGBCJREAAHg8KolmJIkAAMDjkSSa0d0MAAAAEyqJAADA41FJNKOSCAAAABMqiQAAABQSTagkAgAAwIRKIgAA8HiMSTSjkggAAAATKokAAMDjUUk0I0kEAAAejyTRjO5mAAAAmFBJBAAAoJBoQiURAAAAJlQSAQCAx2NMohmVRAAAAJhkyUriue9nWB0C/nbyYqLVIeAfQnPZrQ4Bf/P2omoBuBMqiWZUEgEAAGCSJSuJAAAA6UEl0YwkEQAAeDySRDO6mwEAAGBCJREAAIBCogmVRAAAAJiQJAIAAI9ns9lctqTXhg0b1KZNG4WHh8tms2nZsmVO63v27Gk6RsuWLZ22OXfunLp166aAgAAFBQWpd+/eiouLS1ccJIkAAABuJD4+XpUqVdLMmTNvuE3Lli11/Phxx/Lee+85re/WrZv27NmjVatWacWKFdqwYYP69u2brjgYkwgAADyeO13d3KpVK7Vq1eqm29jtdoWFhV133d69e7Vy5Upt27ZN1atXlyS99tprat26taZMmaLw8PA0xUElEQAAwIWSkpJ08eJFpyUpKek/7XPdunXKmzevSpUqpccff1xnz551rNu8ebOCgoIcCaIkNW3aVF5eXtq6dWuaj0GSCAAAPJ4rxyRGR0crMDDQaYmOjr7tWFu2bKl33nlHq1ev1osvvqj169erVatWSklJkSSdOHFCefPmdXpOtmzZFBwcrBMnTqT5OHQ3AwAAuLC3OSoqSpGRkU5tdrv9tvfXuXNnx/8rVKigihUrqnjx4lq3bp2aNGly2/v9NyqJAAAALmS32xUQEOC0/Jck8d+KFSum0NBQHTx4UJIUFhamU6dOOW1z5coVnTt37objGK+HJBEAAHg8d5oCJ73++OMPnT17Vvnz55ck1a5dW7GxsdqxY4djmzVr1ig1NVU1a9ZM837pbgYAAHAjcXFxjqqgJB0+fFgxMTEKDg5WcHCwxo8fr44dOyosLEyHDh3S008/rRIlSqhFixaSpDJlyqhly5bq06ePXn/9dSUnJ2vgwIHq3Llzmq9slkgSAQAA3GoKnO3bt6tx48aOx9fGM0ZERGj27NnatWuXFixYoNjYWIWHh6t58+Z67rnnnLqwFy1apIEDB6pJkyby8vJSx44dNX369HTFYTMMw8iYl+Q+LiVbHQGuOXkx0eoQ8A+huTJuDAz+G28v9/mFBLiLHD7WHbvwk8tdtu/fprdx2b5diUoiAADweO5USXQXXLgCAAAAEyqJAADA41FJNCNJBAAAIEc0obsZAAAAJlQSAQCAx6O72YxKIgAAAEyoJAIAAI9HJdGMSiIAAABMqCQCAACPRyHRjEoiAAAATKgkAgAAj8eYRDOSRAAA4PHIEc3obgYAAIAJlUQAAODx6G42o5IIAAAAEyqJAADA41FINKOSCAAAABMqiQAAwON5eVFK/DcqiQAAADChkggAADweYxLNSBIBAIDHYwocM7qbAQAAYEKS6GZ2bN+mJwf0V7PG9VS5fCmtWf2N1SF5hCXvvK1Bj3ZVu6a11al1I40bMVi//3bEaZtXX5ygng/epzaNaqhT60Ya+/RTOnrksDUBe7h5b7+pahVLa8qLE60OxSPxc8p9cC4yjs3muiWzIkl0M5cuJahkqVKKGjnW6lA8yq4ftqtNx4c17c2Fin71DaVcuaJnB/dX4qUExzZ3lyqroSMn6K33luqFV2ZLMvTskP5KSUmxLnAPtOen3frkw/d1d8lSVofisfg55T44F3AlxiS6mXr1G6pe/YZWh+FxJr4y2+nx0FET9PB9jXVg315VqFJNktS63YOO9WH571JE34F6vMdDOnn8mMILFLyj8XqqhIR4jYoaplHjntPbb86+9RPgEvycch+ci4zDmEQzKonAdcTHx0mS/AMCrrs+8VKCvv78U4WF36U8+cLuZGgebdILE1SvfiPVrFXH6lAAIMtz60ri77//rrFjx2ru3Lk33CYpKUlJSUlObaledtntdleHhywqNTVVr097SeUqVlaR4nc7rVv+8fuaM+sVJV66pAKFiih62hvy8fGxKFLP8tWXn2vf3p+18L2PrA4FQBZEJdHMrSuJ586d04IFC266TXR0tAIDA52WyS9G36EIkRXNmDpRv/16SFETXjKtu7dFa82a/76mzJyrAoUK64XRw3X5X3+kIOOdOHFcU16cqBcmTeEPQAC4QyytJH722Wc3Xf/rr7/ech9RUVGKjIx0akv14pcIbs+MqRO19bsNmjprrvLkzWda75fLX365/HVXwcIqXb6iOraop+/Wr1Hj5q0siNZz7P15j86dO6tuD3dwtKWkpGjnju36YMkibd6+S97e3hZGCCCzo5BoZmmS2K5dO9lsNhmGccNtblX+tdvNXcuXkjMkPHgQwzA08+VobVq/RpNnvq2w8AJpeo4MKTn58h2I0LPVqFlL73/s/Efl+DHPqkjRYoro9RgJIoD/jO5mM0uTxPz582vWrFlq27btddfHxMSoWrVqdzgqayUkxOvo0aOOx3/++Yf27durwMBA5c8fbmFkWduMKRO1dtWXGvfiNOXI6adzZ89Ikvxy5ZLdnl3H//xD61d/pWo1aiswKLdOnz6pDxbOla/drhq161kcfdbn55dLJe4u6dSWI0cOBQYGmdrhevycch+cC7iSpUlitWrVtGPHjhsmibeqMmZFe376SX0e7eF4PPWlq+Mr27Rtr+demGRVWFneiqUfSJKGD+jt1D505AQ1v6+tfH199dOPO7X0/XcV99dFBQWHqELlanrljXcUFBxiRciAZfg55T44FxmHQqKZzbAwC9u4caPi4+PVsmXL666Pj4/X9u3b1bBh+uaAorvZfZy8mGh1CPiH0FyM13UX3l78RgL+LYeFk0VUnbDGZfveOeZel+3blSytJNavX/+m6/38/NKdIAIAAKQXYxLN3HoKHAAAAFjDrSfTBgAAuBMoJJpRSQQAAIAJlUQAAODxGJNoRiURAAAAJlQSAQCAx6OQaEaSCAAAPB7dzWZ0NwMAAMCESiIAAPB4FBLNqCQCAADAhEoiAADweIxJNKOSCAAAABMqiQAAwONRSDSjkggAAAATkkQAAODxbDaby5b02rBhg9q0aaPw8HDZbDYtW7bMsS45OVkjRoxQhQoV5Ofnp/DwcPXo0UPHjh1z2keRIkVMcUyaNCldcZAkAgAAj2ezuW5Jr/j4eFWqVEkzZ840rUtISNDOnTs1evRo7dy5U5988on279+vBx54wLTthAkTdPz4cccyaNCgdMXBmEQAAAAXSkpKUlJSklOb3W6X3W6/7vatWrVSq1atrrsuMDBQq1atcmqbMWOGatSooaNHj6pQoUKOdn9/f4WFhd123FQSAQCAx3Nld3N0dLQCAwOdlujo6AyL/cKFC7LZbAoKCnJqnzRpkkJCQlSlShVNnjxZV65cSdd+qSQCAAC4UFRUlCIjI53ablRFTK/ExESNGDFCXbp0UUBAgKP9ySefVNWqVRUcHKxNmzYpKipKx48f18svv5zmfZMkAgAAj+fKybRv1rX8XyQnJ6tTp04yDEOzZ892WvfPpLRixYry9fVVv379FB0dneZY6G4GAADIZK4liL/99ptWrVrlVEW8npo1a+rKlSs6cuRImo9BJREAAHi8zDSZ9rUE8cCBA1q7dq1CQkJu+ZyYmBh5eXkpb968aT4OSSIAAIAbiYuL08GDBx2PDx8+rJiYGAUHByt//vx68MEHtXPnTq1YsUIpKSk6ceKEJCk4OFi+vr7avHmztm7dqsaNG8vf31+bN2/WkCFD1L17d+XOnTvNcdgMwzAy/NVZ7FKy1RHgmpMXE60OAf8Qmivjx8Tg9nh7ZaKyBXCH5PCx7tiNpm1y2b7XDa6Tvu3XrVPjxo1N7RERERo3bpyKFi163eetXbtWjRo10s6dO/XEE09o3759SkpKUtGiRfXII48oMjIyXWMjqSQCAACP507dzY0aNdLNani3qu9VrVpVW7Zs+c9xcOEKAAAATKgkAgAAj+fKKXAyKyqJAAAAMKGSCAAAPB6FRDMqiQAAADChkggAADyeF6VEEyqJAAAAMKGSCAAAPB6FRDOSRAAA4PGYAseM7mYAAACYUEkEAAAej9upm1FJBAAAgAmVRAAA4PEYk2hGJREAAAAmVBIBAIDHo5BoliWTRE60+7icnGp1CPiHM38lWR0C/pbId8NtFArNYXUIcOAXuDvJkkkiAABAethIUE1IEgEAgMdjChwzLlwBAACACZVEAADg8ZgCx4xKIgAAAEyoJAIAAI9HIdGMSiIAAABMqCQCAACP50Up0YRKIgAAAEyoJAIAAI9HIdGMJBEAAHg8psAxS1OSuGvXrjTvsGLFircdDAAAANxDmpLEypUry2azyTCM666/ts5msyklJSVDAwQAAHA1ColmaUoSDx8+7Oo4AAAA4EbSlCQWLlzY1XEAAABYhilwzG5rCpyFCxeqbt26Cg8P12+//SZJmjZtmj799NMMDQ4AAADWSHeSOHv2bEVGRqp169aKjY11jEEMCgrStGnTMjo+AAAAl7O5cMms0p0kvvbaa3rrrbc0cuRIeXt7O9qrV6+u3bt3Z2hwAAAAsEa650k8fPiwqlSpYmq32+2Kj4/PkKAAAADuJOZJNEt3JbFo0aKKiYkxta9cuVJlypTJiJgAAADuKC+b65bMKt2VxMjISA0YMECJiYkyDEPff/+93nvvPUVHR2vOnDmuiBEAAAB3WLqTxMcee0w5cuTQqFGjlJCQoK5duyo8PFyvvvqqOnfu7IoYAQAAXIruZrPbundzt27d1K1bNyUkJCguLk558+bN6LgAAABgodtKEiXp1KlT2r9/v6Sr2XeePHkyLCgAAIA7iUKiWbovXPnrr7/0yCOPKDw8XA0bNlTDhg0VHh6u7t2768KFC66IEQAAAHdYupPExx57TFu3btXnn3+u2NhYxcbGasWKFdq+fbv69evnihgBAABcymazuWzJrNLd3bxixQp99dVXqlevnqOtRYsWeuutt9SyZcsMDQ4AAADWSHeSGBISosDAQFN7YGCgcufOnSFBAQAA3EmZeT5DV0l3d/OoUaMUGRmpEydOONpOnDih4cOHa/To0RkaHAAAwJ1Ad7NZmiqJVapUcXqRBw4cUKFChVSoUCFJ0tGjR2W323X69GnGJQIAAGQBaUoS27Vr5+IwAAAArJN5632uk6YkcezYsa6OAwAAAG4k3WMSAQAAshovm81lS3pt2LBBbdq0UXh4uGw2m5YtW+a03jAMjRkzRvnz51eOHDnUtGlTHThwwGmbc+fOqVu3bgoICFBQUJB69+6tuLi49L0n6Q08JSVFU6ZMUY0aNRQWFqbg4GCnBQAAALcvPj5elSpV0syZM6+7/qWXXtL06dP1+uuva+vWrfLz81OLFi2UmJjo2KZbt27as2ePVq1apRUrVmjDhg3q27dvuuJId5I4fvx4vfzyy3r44Yd14cIFRUZGqkOHDvLy8tK4cePSuzsAAADL2WyuW9KrVatWev7559W+fXvTOsMwNG3aNI0aNUpt27ZVxYoV9c477+jYsWOOiuPevXu1cuVKzZkzRzVr1lS9evX02muvacmSJTp27Fia40h3krho0SK99dZbGjp0qLJly6YuXbpozpw5GjNmjLZs2ZLe3QEAAGRpSUlJunjxotOSlJR0W/s6fPiwTpw4oaZNmzraAgMDVbNmTW3evFmStHnzZgUFBal69eqObZo2bSovLy9t3bo1zcdKd5J44sQJVahQQZKUK1cux/2a77//fn3++efp3R0AAIDlXDlPYnR0tAIDA52W6Ojo24rz2jzV+fLlc2rPly+fY92JEyeUN29ep/XZsmVTcHCw0zzXt5LuJLFAgQI6fvy4JKl48eL6+uuvJUnbtm2T3W5P7+4AAACytKioKF24cMFpiYqKsjqsW0p3kti+fXutXr1akjRo0CCNHj1ad999t3r06KFHH300wwMEAABwNVeOSbTb7QoICHBabrewFhYWJkk6efKkU/vJkycd68LCwnTq1Cmn9VeuXNG5c+cc26RFuu/dPGnSJMf/H374YRUuXFibNm3S3XffrTZt2qR3d7iOJYsXacG8t3XmzGmVLFVazzw7WhUqVrQ6rCzti2Uf6MtPP9LJE1cH9BYqUkydI/qqeq16kqTLSUl6e9bL2rjmKyUnX1aVe2rr8SHPKndwiJVhZ1lL3nlb361frd9/Oyxfu11lK1RW78cHq2DhIqZtDcPQqGEDtH3Ldxob/YrqNLj3zgechX356dXvxql/fDcejuirajWvfje+Wv6xNnzzpQ4d2KdLCfFatHyDcvn7Wxmyx3j7rTe05ptVOnL4V9mzZ1elylX01JChKlK0mNWhZUq3M1WNFYoWLaqwsDCtXr1alStXliRdvHhRW7du1eOPPy5Jql27tmJjY7Vjxw5Vq1ZNkrRmzRqlpqaqZs2aaT7Wf54nsVatWoqMjFTNmjU1ceLE/7o7j7fyyy805aVo9XtigJZ8uFSlSpXW4/166+zZs1aHlqWF5smniH6DNO2tRXrlzUWqWLWGXhg5RL8dPiRJmjNjir7ftEEjxr+k6Ffn6NyZ04oePdTiqLOuXTHb1abDw5r25kJFT3tDKVeu6Nkh/ZV4KcG07dL335WNeyW4TEiefOrRd5BefnORpr6xSBWq1tDEkUN09O/vRlJioqrUqKMHu9GTdKft3L5ND3fpqncWv6/Zb87VleQrerzvY7qUYP6eIHOJi4tTTEyMYmJiJF29WCUmJkZHjx6VzWbT4MGD9fzzz+uzzz7T7t271aNHD4WHhzvukFemTBm1bNlSffr00ffff6/vvvtOAwcOVOfOnRUeHp7mOGyGYRgZ8YJ+/PFHVa1aVSkpKRmxu/8k8YrVEdy+bp0fUrnyFfTsqDGSpNTUVDVv0lBduj6i3n3SN7+ROzh6JvP+sOpyf0P1enyw6jZsqu5t79Ww0RNVt1EzSdLvvx3WEz06aPKsBSpdLvNUeX2zZc7582PPn9PD9zfWlJlzVaFyNUf7oV/2aczTg/Ta2++pywNNMlUlMTE51eoQblu3Ng3Vs/9gNbvv/9Nz7P5hu0YN6ZMpK4mFQnNYHUKGOHfunJo0qKM58xeqWvV7rA7ntuT0se4Pvic++dll+57VoWy6tl+3bp0aN25sao+IiND8+fNlGIbGjh2rN998U7GxsapXr55mzZqlkiVLOrY9d+6cBg4cqOXLl8vLy0sdO3bU9OnTlStXrjTHke7uZrhO8uXL2vvzHvXu08/R5uXlpVq16mjXjz9YGJlnSUlJ0XfrVikx8ZJKl6uog7/s1ZUrV1SpWi3HNgULF1WefGHat2dXpkoSM6v4+Kt3CfAPCHC0JSZe0qTxURow9FkFh4RaFZpH+ed3oxSfe7cTF/eXpKvToSBza9SokW5Ww7PZbJowYYImTJhww22Cg4O1ePHi/xQHSaIbOR97XikpKQoJcR7nFhISosOHf7UoKs9x5NABDR8QocuXLytHjhwa+fxUFSpSXL8e+EXZfHxM1ZGg3CGKPccwAFdLTU3V66++pHIVK6tIsbsd7W9Mn6yy5SupTn3zX9vIWEd+PaART/z/uxH13NXvBtxHamqqpkyaqMpVqqrE3SVv/QSY2DLJmMQ7yfIk8dKlS9qxY4eCg4NVtqxzOTYxMVEffPCBevToccPnJyUlmSakNLztTMeDdLurUBG9OmeJEuLj9N36b/TKxDGKnj7H6rA83oypE/Xbr4c0dfZ8R9vmjesUs2ObZs1737K4PMldBYto2pwlio+P06b13+jV6DF64dU5JIpuJPr5CTp48IDmvfPfKkfAP6U5SYyMjLzp+tOnT6f74L/88ouaN2/uGIhZr149LVmyRPnz55ckXbhwQb169bppkhgdHa3x48c7tY0cPVajxoxLdzxWyx2UW97e3qaLVM6ePavQULrTXM3Hx0fhBQpJkkqUKqsD+/bos4/eU/17m+tKcrLi/vrLqZoYe/6sgri62aVmTJ2orZs2aOrMucqT9/8Tx8bs+F7H//xdHVrWc9r+uZFDVb5SVU2e8fadDjVL8/HxUf5/fTdWfPyenhg6yuLIIEmTXpigjevX6e0F7ypfOqY3gbPMOWLbtdKcJP7ww63HxDVo0CBdBx8xYoTKly+v7du3KzY2VoMHD1bdunW1bt06FSpUKE37iIqKMiWwhnfmrCL6+PqqTNly2rpls+5tcvV2O6mpqdq6dbM6d+lucXSex0g1lJx8WSVKllG2bNn0486tqtvw6nn54+gRnT55gvGILmIYhma+HK1NG9Zo8oy3FRZewGn9w488qlYPON/TtN8jD6rfk8NUq27DOxmqRzIMQ8mXL1sdhsczDEMvTnxOa1Z/o7fmvaO7ChS49ZOAdEhzkrh27doMP/imTZv0zTffKDQ0VKGhoVq+fLmeeOIJ1a9fX2vXrpWfn98t92G3m7uWM/PVzY9E9NLoZ0eoXLnyKl+hot5duECXLl1Su/YdrA4tS1vw5nRVq1lXefLm16WEeK1f/aV2x2zX+Mmz5JfLX81at9PbM6fK3z9QOf389MarL6p0uYokiS4yY+pErV31pcZNmqYcOf107uwZSZJfrlyy27MrOCT0uher5M2X35RQ4r955+/vRmje/Lp0KV4bvvlSP8Vs17jJsyRJ58+e0flzZ3X8z6OSpN8OH1COHH7Kky9M/gFcQOFK0c9P0JdfrNAr02fKz89PZ85c7dHLlctf2bNntzi6zIcxiWaWjkm8dOmSsmX7fwg2m02zZ8/WwIED1bBhw/98VU5m1LJVa50/d06zZkzXmTOnVap0Gc16Y45C6G52qQvnz+mViaN17uwZ+fnlUpHid2v85Fmqcs/VK5ofGzhMNi8vRY8ZpuTky6p6Tx09PsT9b6mUWa1Y+oEkafjA3k7tQ5+doOb3tbUiJI91Ifacpk0crXPnrn43Che7W+Mmz1Ll6le/Gys/+0hLFrzh2P7ZJ6+esydHjFeTVg9YErOn+PD99yRJfXo5D8ka//xEPdCOwkJ6eZEjmmTYPIm3o0aNGho0aJAeeeQR07qBAwdq0aJFunjxYrrnXszMlcSsJjPPk5gVZdZ5ErOizDxPYlaTVeZJzAqsnCdx8Kf7XLbvaW1Lu2zfrmTpb4z27dvrvffeu+66GTNmqEuXLjedJwgAACAjeNlct2RWllYSXYVKovugkuheqCS6DyqJ7oNKovuwspIY+ZnrKokvP5A5K4mWz5MIAABgNS5cMbutssLGjRvVvXt31a5dW3/++ackaeHChfr2228zNDgAAABYI91J4scff6wWLVooR44c+uGHHxx3O7lw4YImTpyY4QECAAC4GmMSzdKdJD7//PN6/fXX9dZbb8nHx8fRXrduXe3cuTNDgwMAAIA10j0mcf/+/de9s0pgYKBiY2MzIiYAAIA7iiGJZumuJIaFhengwYOm9m+//VbFihXLkKAAAADuJC+bzWVLZpXuJLFPnz566qmntHXrVtlsNh07dkyLFi3SsGHD9Pjjj7siRgAAANxh6e5ufuaZZ5SamqomTZooISFBDRo0kN1u17BhwzRo0CBXxAgAAOBSzCJrdtuTaV++fFkHDx5UXFycypYtq1y5cmV0bLeNybTdB5Npuxcm03YfTKbtPphM231YOZn2s1/84rJ9T2xd0mX7dqXbnkzb19dXZcuWzchYAAAALJGJhw66TLqTxMaNG990VvI1a9b8p4AAAABgvXQniZUrV3Z6nJycrJiYGP3000+KiIjIqLgAAADumMx8FbKrpDtJfOWVV67bPm7cOMXFxf3ngAAAAGC9DBvF3r17d82dOzejdgcAAHDH2GyuWzKr275w5d82b96s7NmzZ9TuAAAA7pjMfI9lV0l3ktihQwenx4Zh6Pjx49q+fbtGjx6dYYEBAADAOulOEgMDA50ee3l5qVSpUpowYYKaN2+eYYEBAADcKVy4YpauJDElJUW9evVShQoVlDt3blfFBAAAAIul68IVb29vNW/eXLGxsS4KBwAA4M7jwhWzdF/dXL58ef3666+uiAUAAABuIt1J4vPPP69hw4ZpxYoVOn78uC5evOi0AAAAZDZeNtctmVWaxyROmDBBQ4cOVevWrSVJDzzwgNPt+QzDkM1mU0pKSsZHCQAAgDsqzUni+PHj1b9/f61du9aV8QAAANxxNmXikp+LpDlJNAxDktSwYUOXBQMAAGCFzNwt7CrpGpNoy8yX6AAAACDN0jVPYsmSJW+ZKJ47d+4/BQQAAHCnUUk0S1eSOH78eNMdVwAAAJD1pCtJ7Ny5s/LmzeuqWAAAACzBkDqzNI9J5M0DAADwHOm+uhkAACCrYUyiWZqTxNTUVFfGAQAAADeSrjGJAAAAWRGj6sxIEgEAgMfzIks0Sddk2gAAAPAMVBIBAIDH48IVMyqJAAAAMKGSCAAAPB5DEs2oJAIAAMCESiIAAPB4XqKU+G8kiXCpQqE5rQ4B/5Cayp2T3EXC5RSrQwCAm6K7GQAAeDybzXVLehQpUkQ2m820DBgwQJLUqFEj07r+/fu74B2hkggAAOA2U+Bs27ZNKSn/72n46aef1KxZMz300EOOtj59+mjChAmOxzlzuqbXjiQRAADATeTJk8fp8aRJk1S8eHE1bNjQ0ZYzZ06FhYW5PBa6mwEAgMfzstlctiQlJenixYtOS1JS0i1junz5st599109+uijsv2j33rRokUKDQ1V+fLlFRUVpYSEBNe8Jy7ZKwAAACRJ0dHRCgwMdFqio6Nv+bxly5YpNjZWPXv2dLR17dpV7777rtauXauoqCgtXLhQ3bt3d0ncNsMwstzljolXrI4AcE9c3ew+uLrZfeS0e1sdAv6W08e6gYFvbf3NZfvuUTnMVDm02+2y2+03fV6LFi3k6+ur5cuX33CbNWvWqEmTJjp48KCKFy+eIfFew5hEAAAAF0pLQvhvv/32m7755ht98sknN92uZs2akkSSCAAA4ApebnZfvnnz5ilv3ry67777brpdTEyMJCl//vwZHgNJIgAAgBtJTU3VvHnzFBERoWzZ/p+qHTp0SIsXL1br1q0VEhKiXbt2aciQIWrQoIEqVqyY4XGQJAIAAI/nToXEb775RkePHtWjjz7q1O7r66tvvvlG06ZNU3x8vAoWLKiOHTtq1KhRLomDC1cAD8KFK+6DC1fcBxeuuA8rL1yZv+2oy/bd855CLtu3KzEFDgAAAEzobgYAAB7P5k79zW6CSiIAAABMqCQCAACPRx3RjEoiAAAATKgkAgAAj+duk2m7AyqJAAAAMKGSCAAAPB51RDOSRAAA4PHobTajuxkAAAAmVBIBAIDHYzJtMyqJAAAAMKGSCAAAPB5VMzPeEwAAAJhQSQQAAB6PMYlmVBIBAABgQiURAAB4POqIZlQSAQAAYEIlEQAAeDzGJJqRJAIAAI9H16oZ7wkAAABMqCQCAACPR3ezGZVEAAAAmFBJBAAAHo86ohmVRAAAAJhQSQQAAB6PIYlmVBIBAABgQiURAAB4PC9GJZqQJAIAAI9Hd7MZ3c1uaMniRWrV7F7dU6WCunV+SLt37bI6JI/G+bDeB++/p04dHlC9WtVUr1Y19ej2sL7duMHqsDxCzM7tenrwE3qgRSPVrVZOG9audqy7kpysWdOn6pFO7dSkbnU90KKRnhsTpdOnT1kYsed4+6031O3hB1W3RlXd26COhjw5QEcO/2p1WMhCSBLdzMovv9CUl6LV74kBWvLhUpUqVVqP9+uts2fPWh2aR+J8uId8+fJp0OChWvT+x1q05CPVqFlLQ54coEMHD1gdWpZ36dIllShZSkNHjDKtS0xM1P59e9Xzsf6au+hDTZzyqo4eOawRQwZaEKnn2bl9mx7u0lXvLH5fs9+cqyvJV/R438d0KSHB6tAyJZsL/2VWNsMwDKuDyGiJV6yO4PZ16/yQypWvoGdHjZEkpaamqnmThurS9RH17tPX4ug8T1Y7H6mpWefr3rBuTQ0eOlztOzxodSi3JeFyitUhpFvdauUUPWW6GjRucsNt9u7Zrcd6dNbHK1YpLH/4HYzu9uW0e1sdQoY4d+6cmjSooznzF6pa9XusDue25PSxLqH6/CfXVcDvK5/XZft2JSqJbiT58mXt/XmPatWu42jz8vJSrVp1tOvHHyyMzDNxPtxTSkqKVn75uS5dSlDFSpWtDgf/EhcXJ5vNJn//AKtD8ThxcX9JkgIDAy2OJHOy2Vy3ZFaWX7iyd+9ebdmyRbVr11bp0qW1b98+vfrqq0pKSlL37t1177333vT5SUlJSkpKcmozvO2y2+2uDNslzseeV0pKikJCQpzaQ0JCdJhxJncc58O9HPhlvyK6d9Hly0nKkTOnpk6boeLFS1gdFv4hKSlJs6e/rKYtWssvVy6rw/EoqampmjJpoipXqaoSd5e0OhxkEZZWEleuXKnKlStr2LBhqlKlilauXKkGDRro4MGD+u2339S8eXOtWbPmpvuIjo5WYGCg0zL5xeg79AoA3ClFihbVko+W6p1F7+uhTp01ZtQzOnTooNVh4W9XkpM1+plIGYah4VFjrA7H40Q/P0EHDx7QpMkvWx1KpuUlm8uWzMrSJHHChAkaPny4zp49q3nz5qlr167q06ePVq1apdWrV2v48OGaNGnSTfcRFRWlCxcuOC3DR0TdoVeQsXIH5Za3t7fpooizZ88qNDTUoqg8F+fDvfj4+KpQocIqW668nhw8VCVLltZ7775jdVjQtQRxqE4eP6Zps+ZQRbzDJr0wQRvXr9Nbc99RvrAwq8NBFmJpkrhnzx717NlTktSpUyf99ddfevDB/w9C79atm3bdYroRu92ugIAApyUzdjVLko+vr8qULaetWzY72lJTU7V162ZVrFTFwsg8E+fDvRlGqi5fvmx1GB7vWoL4+++/adrstxUYFGR1SB7DMAxNemGC1qz+Rm/Mna+7ChSwOqRMjTGJZpaPSbT9/e55eXkpe/bsTgNu/f39deHCBatCs8QjEb00+tkRKleuvMpXqKh3Fy7QpUuX1K59B6tD80icD/cwfdpU1a3XQPnz51d8fLy+/GKFtm/7XrNen2N1aFleQkK8/vj9qOPxsWN/6Jf9exUQEKjQ0DwaOWKIftm3Vy9Nm6nUlBSdPXNakhQQGCgfH1+rwvYI0c9P0JdfrNAr02fKz89PZ/5+73Pl8lf27Nktji7zyczJnKtYmiQWKVJEBw4cUPHixSVJmzdvVqFChRzrjx49qvz581sVniVatmqt8+fOadaM6Tpz5rRKlS6jWW/MUQjdm5bgfLiHc+fOafTIETpz+rRy+fvr7rtLadbrc1SrTl2rQ8vy9v28R4P69XI8fu3llyRJre5vq979Bujb9WslST27dHR63mtvzFPV6jXuXKAe6MP335Mk9enVw6l9/PMT9UA7/pDFf2fpPImvv/66ChYsqPvuu++665999lmdOnVKc+akr1qQmedJBFwpK82TmNllxnkSs6qsMk9iVmDlPImr9p5x2b6blcmchQUm0wY8CEmi+yBJdB8kie6DJNG9WD4mEQAAwGpejEk04Y4rAAAAMKGSCAAAPJ4tE0967SpUEgEAAGBCJREAAHg85kk0I0kEAAAej+5mM7qbAQAAYEIlEQAAeDymwDGjkggAAOAmxo0bJ5vN5rSULl3asT4xMVEDBgxQSEiIcuXKpY4dO+rkyZMuiYUkEQAAeDybC/+lV7ly5XT8+HHH8u233zrWDRkyRMuXL9eHH36o9evX69ixY+rQwTX36qa7GQAAwI1ky5ZNYWFhpvYLFy7o7bff1uLFi3XvvfdKkubNm6cyZcpoy5YtqlWrVobGQSURAAB4PJvNdUtSUpIuXrzotCQlJd0wlgMHDig8PFzFihVTt27ddPToUUnSjh07lJycrKZNmzq2LV26tAoVKqTNmzdn+HtCkggAAOBC0dHRCgwMdFqio6Ovu23NmjU1f/58rVy5UrNnz9bhw4dVv359/fXXXzpx4oR8fX0VFBTk9Jx8+fLpxIkTGR433c0AAMDjufLi5qioKEVGRjq12e32627bqlUrx/8rVqyomjVrqnDhwvrggw+UI0cOF0ZpRpIIAAA8npcLb7lit9tvmBTeSlBQkEqWLKmDBw+qWbNmunz5smJjY52qiSdPnrzuGMb/iu5mAAAANxUXF6dDhw4pf/78qlatmnx8fLR69WrH+v379+vo0aOqXbt2hh+bSiIAAPB47jKX9rBhw9SmTRsVLlxYx44d09ixY+Xt7a0uXbooMDBQvXv3VmRkpIKDgxUQEKBBgwapdu3aGX5ls0SSCAAA4Db++OMPdenSRWfPnlWePHlUr149bdmyRXny5JEkvfLKK/Ly8lLHjh2VlJSkFi1aaNasWS6JxWYYhuGSPVso8YrVEQDuKTU1y33dM62EyylWh4C/5bR7Wx0C/pbTx7p63pZDsS7bd63iQS7btysxJhEAAAAmdDcDAACPdzu3z8vqqCQCAADAhEoiAADweC6cJjHTIkkEAAAejxzRjO5mAAAAmFBJBAAAoJRoQiURAAAAJlQSAQCAx2MKHDMqiQAAADChkggAADweU+CYUUkEAACACZVEAADg8SgkmpEkAgAAkCWa0N0MAAAAEyqJAADA4zEFjhmVRAAAAJhQSQQAAB6PKXDMqCQCAADAhEoiAADweBQSzUgS4VKGYXUE+CcvL34MuovklFSrQ8Df/rrEDyp3kdPHx+oQ8A8kiQAAAPwNbUKSCAAAPB5T4Jhx4QoAAABMqCQCAACPxxQ4ZlQSAQAAYEIlEQAAeDwKiWZUEgEAAGBCJREAAIBSogmVRAAAAJhQSQQAAB6PeRLNqCQCAADAhEoiAADweMyTaEaSCAAAPB45ohndzQAAADChkggAAEAp0YRKIgAAAEyoJAIAAI/HFDhmVBIBAABgQiURAAB4PKbAMaOSCAAAABMqiQAAwONRSDQjSQQAACBLNKG7GQAAACZUEgEAgMdjChwzKokAAAAwoZIIAAA8HlPgmFFJBAAAgAmVRAAA4PEoJJpRSQQAAHAT0dHRuueee+Tv76+8efOqXbt22r9/v9M2jRo1ks1mc1r69++f4bGQJAIAANhcuKTD+vXrNWDAAG3ZskWrVq1ScnKymjdvrvj4eKft+vTpo+PHjzuWl1566bZe9s3Q3QwAADyeu0yBs3LlSqfH8+fPV968ebVjxw41aNDA0Z4zZ06FhYW5NBYqiQAAAC6UlJSkixcvOi1JSUlpeu6FCxckScHBwU7tixYtUmhoqMqXL6+oqCglJCRkeNwkiQAAwOPZbK5boqOjFRgY6LRER0ffMqbU1FQNHjxYdevWVfny5R3tXbt21bvvvqu1a9cqKipKCxcuVPfu3TP+PTEMw8jwvVos8YrVEeCarPfpytyYB8x9nI+/bHUI+JsXXwy3kS/Ax7JjHz6T6LJ9h/vbTJVDu90uu91+0+c9/vjj+vLLL/Xtt9+qQIECN9xuzZo1atKkiQ4ePKjixYtnSMwSYxIBAABcOiIxLQnhvw0cOFArVqzQhg0bbpogSlLNmjUliSQRAAAgqzIMQ4MGDdLSpUu1bt06FS1a9JbPiYmJkSTlz58/Q2MhSQQAAHCTUQcDBgzQ4sWL9emnn8rf318nTpyQJAUGBipHjhw6dOiQFi9erNatWyskJES7du3SkCFD1KBBA1WsWDFDY2FMIlwq6326MjeGXrkPxiS6D8Ykug8rxyQeOeu6MYlFQrKneVvbDT6P8+bNU8+ePfX777+re/fu+umnnxQfH6+CBQuqffv2GjVqlAICAjIq5KuxkCTClbLepytz43eh+yBJdB8kie7DyiTxt7Npm5LmdhQOSd94RHdBdzMAAPB4/K1gxjyJbmjJ4kVq1exe3VOlgrp1fki7d+2yOiSPtGP7Nj05oL+aNa6nyuVLac3qb6wOyePx3bjzfty5XVGRA9Wx9b1qVKOCNq5b7bS+UY0K112WLJxnUcRZV8zO7XpmyAC1b9VYDe4pbzoXE8eNVIN7yjstwwb1syhaZAUkiW5m5ZdfaMpL0er3xAAt+XCpSpUqrcf79dbZs2etDs3jXLqUoJKlSilq5FirQ4H4blglMfGSit9dUoOHj7zu+o+/WOu0jBg9QTabTQ3ubXqHI836Ei9dUvGSpTTk6eufC0mqWbueln65zrGMfSHj7+ebVbnJrZvdCt3Nbmbhgnnq8GAntWvfUZI0aux4bdiwTss++Vi9+/S1ODrPUq9+Q9Wr39DqMPA3vhvWqFmnvmrWqX/D9SGhoU6Pv12/VlWq1VD4XQVdHZrHqVW3vmrVvfG5kCQfX1/TOQFul9tVErPgdTRplnz5svb+vEe1atdxtHl5ealWrTra9eMPFkYGWIvvRuZw7uwZbfluo1o/0N7qUDxWzI5teqB5A3XreL+mTpqgC7GxVoeUabjytnyZldsliXa7XXv37rU6DEucjz2vlJQUhYSEOLWHhITozJkzFkUFWI/vRubw1eefKadfTtVvTFezFWrWqatnx03UK7PmqP+gIYrZuV3Dn+qvlJQUq0NDJmVZd3NkZOR121NSUjRp0iTHL4OXX375pvtJSkoy3Q/R8E7/7W8AAP/NF8uXqmmL+/j5a5EmzVs7/l+8REkVL1FSndu3UsyObapWo5aFkWUWmbjk5yKWVRKnTZumtWvX6ocffnBaDMPQ3r179cMPPzhuM3Mz0dHRCgwMdFomvxjt+hfgArmDcsvb29s0EP/s2bMKZYwJPBjfDfe364cd+v23I7qvbUerQ8HfwgsUVGBQbv3xx1GrQ0EmZVmSOHHiRF24cEGjR4/W2rVrHYu3t7fmz5+vtWvXas2aNbfcT1RUlC5cuOC0DB8RdQdeQcbz8fVVmbLltHXLZkdbamqqtm7drIqVqlgYGWAtvhvu7/PPPlHJ0mVVomQpq0PB306dPKGLF2IVEpLH6lAyBcYkmlnW3fzMM8+oSZMm6t69u9q0aaPo6Gj5+KR/pnW73dy1nJnvuPJIRC+NfnaEypUrr/IVKurdhQt06dIltWvfwerQPE5CQryOHv3/X+B//vmH9u3bq8DAQOXPH25hZJ6J74Y1EhIS9Oc/KlEnjv2pA7/sU0BAoPKF5ZckxcfFaf3qVXr8qWFWhekREhIS9Ofv/z8Xx4/9qQP79ykgMFD+AYGa/9YsNby3mYJDQnXsj981+7WXdVfBQqpRu66FUWcemTiXcxlLp8C55557tGPHDg0YMEDVq1fXokWLbnjPQk/RslVrnT93TrNmTNeZM6dVqnQZzXpjDlMaWGDPTz+pz6M9HI+nvnR1GEObtu313AuTrArLY/HdsMb+vXs05PFHHY9nTpssSWpx3wOKGvuCJGnNqi9lGIaatGhlSYyeYv/en/RU//+fixmvXJ0DseV9bTX0mdE6dPAXrfz8M8X9dVGhefLqnpp11Lv/QPn6+loVMjI5t7l385IlSzR48GCdPn1au3fvVtmyZW97X5m5kpjVuMenC9d4+N9gboV7N7sP7t3sPqy8d/PxC677TuYPzJyJutskiZL0xx9/aMeOHWratKn8/Pxuez8kie7DfT5dkEgS3QlJovsgSXQfJInuxa3uuFKgQAEVKFDA6jAAAICHsTEq0cTtJtMGAACA9dyqkggAAGAJCokmVBIBAABgQiURAAB4PAqJZiSJAADA43GRuxndzQAAADChkggAADweU+CYUUkEAACACZVEAAAACokmVBIBAABgQiURAAB4PAqJZlQSAQAAYEIlEQAAeDzmSTQjSQQAAB6PKXDM6G4GAACACZVEAADg8ehuNqOSCAAAABOSRAAAAJiQJAIAAMCEMYkAAMDjMSbRjEoiAAAATKgkAgAAj8c8iWYkiQAAwOPR3WxGdzMAAABMqCQCAACPRyHRjEoiAAAATKgkAgAAUEo0oZIIAAAAEyqJAADA4zEFjhmVRAAAAJhQSQQAAB6PeRLNqCQCAADAhEoiAADweBQSzUgSAQAAyBJN6G4GAACACUkiAADweDYX/rsdM2fOVJEiRZQ9e3bVrFlT33//fQa/4lsjSQQAAHAj77//viIjIzV27Fjt3LlTlSpVUosWLXTq1Kk7GofNMAzjjh7xDki8YnUEuCbrfboyN6Z4cB/n4y9bHQL+5sUXw23kC/Cx7NiuzB2yp/MKkJo1a+qee+7RjBkzJEmpqakqWLCgBg0apGeeecYFEV4flUQAAAAXSkpK0sWLF52WpKSk6257+fJl7dixQ02bNnW0eXl5qWnTptq8efOdCllSFr26Ob0ZuztKSkpSdHS0oqKiZLfbrQ7Ho3Eu3EdWOhf5A32tDuE/yUrnIivgfPx3rswdxj0frfHjxzu1jR07VuPGjTNte+bMGaWkpChfvnxO7fny5dO+fftcF+R1ZMnu5qzg4sWLCgwM1IULFxQQEGB1OB6Nc+E+OBfug3PhXjgf7i0pKclUObTb7ddN6I8dO6a77rpLmzZtUu3atR3tTz/9tNavX6+tW7e6PN5rskDNDQAAwH3dKCG8ntDQUHl7e+vkyZNO7SdPnlRYWJgrwrshxiQCAAC4CV9fX1WrVk2rV692tKWmpmr16tVOlcU7gUoiAACAG4mMjFRERISqV6+uGjVqaNq0aYqPj1evXr3uaBwkiW7Kbrdr7NixDEB2A5wL98G5cB+cC/fC+chaHn74YZ0+fVpjxozRiRMnVLlyZa1cudJ0MYurceEKAAAATBiTCAAAABOSRAAAAJiQJAIAAMCEJBEAAAAmJIluaObMmSpSpIiyZ8+umjVr6vvvv7c6JI+0YcMGtWnTRuHh4bLZbFq2bJnVIXms6Oho3XPPPfL391fevHnVrl077d+/3+qwPNLs2bNVsWJFBQQEKCAgQLVr19aXX35pdViQNGnSJNlsNg0ePNjqUJBFkCS6mffff1+RkZEaO3asdu7cqUqVKqlFixY6deqU1aF5nPj4eFWqVEkzZ860OhSPt379eg0YMEBbtmzRqlWrlJycrObNmys+Pt7q0DxOgQIFNGnSJO3YsUPbt2/Xvffeq7Zt22rPnj1Wh+bRtm3bpjfeeEMVK1a0OhRkIUyB42Zq1qype+65RzNmzJB0dZb1ggULatCgQXrmmWcsjs5z2Ww2LV26VO3atbM6FEg6ffq08ubNq/Xr16tBgwZWh+PxgoODNXnyZPXu3dvqUDxSXFycqlatqlmzZun5559X5cqVNW3aNKvDQhZAJdGNXL58WTt27FDTpk0dbV5eXmratKk2b95sYWSAe7lw4YKkq8kJrJOSkqIlS5YoPj7+jt8uDP83YMAA3XfffU6/O4CMwB1X3MiZM2eUkpJimlE9X7582rdvn0VRAe4lNTVVgwcPVt26dVW+fHmrw/FIu3fvVu3atZWYmKhcuXJp6dKlKlu2rNVheaQlS5Zo586d2rZtm9WhIAsiSQSQqQwYMEA//fSTvv32W6tD8VilSpVSTEyMLly4oI8++kgRERFav349ieId9vvvv+upp57SqlWrlD17dqvDQRZEkuhGQkND5e3trZMnTzq1nzx5UmFhYRZFBbiPgQMHasWKFdqwYYMKFChgdTgey9fXVyVKlJAkVatWTdu2bdOrr76qN954w+LIPMuOHTt06tQpVa1a1dGWkpKiDRs2aMaMGUpKSpK3t7eFESKzY0yiG/H19VW1atW0evVqR1tqaqpWr17NeB94NMMwNHDgQC1dulRr1qxR0aJFrQ4J/5CamqqkpCSrw/A4TZo00e7duxUTE+NYqlevrm7duikmJoYEEf8ZlUQ3ExkZqYiICFWvXl01atTQtGnTFB8fr169elkdmseJi4vTwYMHHY8PHz6smJgYBQcHq1ChQhZG5nkGDBigxYsX69NPP5W/v79OnDghSQoMDFSOHDksjs6zREVFqVWrVipUqJD++usvLV68WOvWrdNXX31ldWgex9/f3zQu18/PTyEhIYzXRYYgSXQzDz/8sE6fPq0xY8boxIkTqly5slauXGm6mAWut337djVu3NjxODIyUpIUERGh+fPnWxSVZ5o9e7YkqVGjRk7t8+bNU8+ePe98QB7s1KlT6tGjh44fP67AwEBVrFhRX331lZo1a2Z1aAAyGPMkAgAAwIQxiQAAADAhSQQAAIAJSSIAAABMSBIBAABgQpIIAAAAE5JEAAAAmJAkAgAAwIQkEQAAACYkiQAyTM+ePdWuXTvH40aNGmnw4MF3PI5169bJZrMpNjbWZcf492u9HXciTgC4XSSJQBbXs2dP2Ww22Ww2+fr6qkSJEpowYYKuXLni8mN/8skneu6559K07Z1OmIoUKaJp06bdkWMBQGbEvZsBD9CyZUvNmzdPSUlJ+uKLLzRgwAD5+PgoKirKtO3ly5fl6+ubIccNDg7OkP0AAO48KomAB7Db7QoLC1PhwoX1+OOPq2nTpvrss88k/b/b9IUXXlB4eLhKlSolSfr999/VqVMnBQUFKTg4WG3bttWRI0cc+0xJSVFkZKSCgoIUEhKip59+Wv++Ffy/u5uTkpI0YsQIFSxYUHa7XSVKlNDbb7+tI0eOqHHjxpKk3Llzy2azqWfPnpKk1NRURUdHq2jRosqRI4cqVaqkjz76yOk4X3zxhUqWLKkcOXKocePGTnHejpSUFPXu3dtxzFKlSunVV1+97rbjx49Xnjx5FBAQoP79++vy5cuOdWmJHQDcFZVEwAPlyJFDZ8+edTxevXq1AgICtGrVKklScnKyWrRoodq1a2vjxo3Kli2bnn/+ebVs2VK7du2Sr6+vpk6dqvnz52vu3LkqU6aMpk6dqqVLl+ree++94XF79OihzZs3a/r06apUqZIOHz6sM2fOqGDBgvr444/VsWNH7d+/XwEBAcqRI4ckKTo6Wu+++65ef/113X333dqwYYO6d++uPHnyqGHDhvr999/VoUMHDRgwQH379tX27ds1dOjQ//T+pKamqkCBAvrwww8VEhKiTZs2qW/fvsqfP786derk9L5lz55d69at05EjR9SrVy+FhITohRdeSFPsAODWDABZWkREhNG2bVvDMAwjNTXVWLVqlWG3241hw4Y51ufLl89ISkpyPGfhwoVGqVKljNTUVEdbUlKSkSNHDuOrr74yDMMw8ufPb7z00kuO9cnJyUaBAgUcxzIMw2jYsKHx1FNPGYZhGPv37zckGatWrbpunGvXrjUkGefPn3e0JSYmGjlz5jQ2bdrktG3v3r2NLl26GIZhGFFRUUbZsmWd1o8YMcK0r38rXLiw8corr9xw/b8NGDDA6Nixo+NxRESEERwcbMTHxzvaZs+ebeTKlctISUlJU+zXe80A4C6oJAIeYMWKFcqVK5eSk5OVmpqqrl27aty4cY71FSpUcBqH+OOPP+rgwYPy9/d32k9iYqIOHTqkCxcu6Pjx46pZs6ZjXbZs2VS9enVTl/M1MTEx8vb2TlcF7eDBg0pISFCzZs2c2i9fvqwqVapIkvbu3esUhyTVrl07zce4kZkzZ2ru3Lk6evSoLl26pMuXL6ty5cpO21SqVEk5c+Z0Om5cXJx+//13xcXF3TJ2AHBnJImAB2jcuLFmz54tX19fhYeHK1s256++n5+f0+O4uDhVq1ZNixYtMu0rT548txXDte7j9IiLi5Mkff7557rrrruc1tnt9tuKIy2WLFmiYcOGaerUqapdu7b8/f01efJkbd26Nc37sCp2AMgoJImAB/Dz81OJEiXSvH3VqlX1/vvvK2/evAoICLjuNvnz59fWrVvVoEEDSdKVK1e0Y8cOVa1a9brbV6hQQampqVq/fr2aNm1qWn+tkpmSkuJoK1u2rOx2u44ePXrDCmSZMmUcF+Fcs2XLllu/yJv47rvvVKdOHT3xxBOOtkOHDpm2+/HHH3Xp0iVHArxlyxblypVLBQsWVHBw8C1jBwB3xtXNAEy6deum0NBQtW3bVhs3btThw4e1bt06Pfnkk/rjjz8kSU899ZQmTZqkZcuWad++fXriiSduOsdhkSJFFBERoUcffVTLli1z7PODDz6QJBUuXFg2m00rVqzQ6dOnFRcXJ39/fw0bNkxDhgzRggULdOjQIe3cuVOvvfaaFixYIEnq37+/Dhw4oOHDh2v//v1avHix5s+fn6bX+eeffyomJsZpOX/+vO6++25t375dX331lX755ReNHj1a27ZtMz3/8uXL6t27t37++Wd98cUXGjt2rAYOHCgvL680xQ4Abs3qQZEAXOufF66kZ/3x48eNHj16GKGhoYbdbjeKFStm9OnTx7hw4YJhGFcvVHnqqaeMgIAAIygoyIiMjDR69OhxwwtXDMMwLl26ZAwZMsTInz+/4evra5QoUcKYO3euY/2ECROMsLAww2azGREREYZhXL3YZtq0aUapUqUMHx8fI0+ePEaLFi2M9evXO563fPlyo0SJEobdbjfq169vzJ07N00XrkgyLQsXLjQSExONnj17GoGBgUZQUJDx+OOPG88884xRqVIl0/s2ZswYIyQkxMiVK5fRp08fIzEx0bHNrWLnwhUA7sxmGDcYZQ4AAACPRXczAAAATEgSAQAAYEKSCAAAABOSRAAAAJiQJAIAAMCEJBEAAAAmJIkAAAAwIUkEAACACUkiAAAATEgSAQAAYEKSCAAAAJP/ASvJ5b1uY2y0AAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport timm\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.optimize import minimize\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\nclass CFG:\n    MODEL_NAME = 'swinv2_tiny_window8_256'\n    IMG_SIZE = 256\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    \n    # We need paths for both validation (to find thresholds) and test (for final score)\n    VAL_CSV = os.path.join(BASE_PATH, \"valid.csv\")\n    VAL_DIR = os.path.join(BASE_PATH, \"val_images\", \"val_images\")\n    TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n    TEST_DIR = os.path.join(BASE_PATH, \"test_images\", \"test_images\")\n    \n    MODEL_PATH = \"best_model_swin_final.pth\"\n    BATCH_SIZE = 16 \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    NUM_WORKERS = 2\n\n# =============================================================================\n# REUSED CLASSES & PREPROCESSING (Unchanged)\n# =============================================================================\nclass SwinOrdinal(nn.Module):\n    def __init__(self, model_name, num_classes=5, pretrained=False):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(nn.Linear(feature_dim, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes - 1))\n    def forward(self, x): return self.classifier(self.backbone(x))\n\ndef preprocess_ben_graham(image_np, output_size):\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n        if gray.mean() < 15: return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception: pass\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(image_resized); clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)); g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df, self.img_dir, self.transform = df.reset_index(drop=True), img_dir, transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = cv2.imread(img_path); img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform: img = self.transform(image=img)['image']\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\n# =============================================================================\n# OPTIMIZATION FUNCTIONS\n# =============================================================================\ndef ordinal_to_class_with_thresholds(outputs, thresholds):\n    probs = torch.sigmoid(outputs).cpu().numpy()\n    preds = np.sum(probs > thresholds, axis=1)\n    return preds\n\ndef kappa_objective(thresholds, outputs, targets):\n    preds = ordinal_to_class_with_thresholds(outputs, thresholds)\n    return -cohen_kappa_score(targets, preds, weights=\"quadratic\")\n\ndef find_best_thresholds(outputs, targets):\n    print(\"Finding optimal thresholds...\")\n    outputs = outputs.detach()\n    targets = targets.cpu().numpy()\n    init_thresh = np.array([0.5, 0.5, 0.5, 0.5])\n    bounds = [(0.1, 0.9)] * len(init_thresh)\n    res = minimize(kappa_objective, init_thresh, args=(outputs, targets), method=\"Powell\", bounds=bounds)\n    best_thresholds = res.x\n    print(f\"Optimal thresholds found: {np.round(best_thresholds, 4)}\")\n    return best_thresholds\n\n# =============================================================================\n# MAIN SCRIPT\n# =============================================================================\ndef run_optimization_and_test():\n    # --- Step 0: Load Model ---\n    model = SwinOrdinal(CFG.MODEL_NAME, pretrained=False).to(CFG.DEVICE)\n    model.load_state_dict(torch.load(CFG.MODEL_PATH, map_location=CFG.DEVICE))\n    model.eval()\n    print(f\"Model loaded successfully from {CFG.MODEL_PATH}\")\n\n    transform = A.Compose([\n        A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, CFG.IMG_SIZE)),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\n    # --- Step 1: Get Raw Answers on the Validation Set ---\n    print(\"\\n--- Step 1: Evaluating on Validation Set to find thresholds ---\")\n    val_df = pd.read_csv(CFG.VAL_CSV)\n    val_dataset = DiabeticRetinopathyDataset(val_df, CFG.VAL_DIR, transform=transform)\n    val_loader = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS)\n    \n    val_outputs_list, val_labels_list = [], []\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=\"Getting Validation Outputs\"):\n            images = images.to(CFG.DEVICE)\n            outputs = model(images)\n            val_outputs_list.append(outputs.cpu())\n            val_labels_list.append(labels)\n    val_outputs = torch.cat(val_outputs_list)\n    val_labels = torch.cat(val_labels_list)\n    \n    # --- Step 2: Find the Perfect \"Grading Scale\" ---\n    print(\"\\n--- Step 2: Optimizing Thresholds ---\")\n    best_thresholds = find_best_thresholds(val_outputs, val_labels)\n\n    # --- Step 3: Use the Grading Scale on the Test Set ---\n    print(\"\\n--- Step 3: Evaluating on Test Set with new thresholds ---\")\n    test_df = pd.read_csv(CFG.TEST_CSV)\n    test_dataset = DiabeticRetinopathyDataset(test_df, CFG.TEST_DIR, transform=transform)\n    test_loader = DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS)\n\n    test_outputs_list, test_labels_list = [], []\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Getting Test Outputs\"):\n            images = images.to(CFG.DEVICE)\n            outputs = model(images)\n            test_outputs_list.append(outputs.cpu())\n            test_labels_list.append(labels)\n    test_outputs = torch.cat(test_outputs_list)\n    test_labels = torch.cat(test_labels_list).numpy()\n    \n    # --- FINAL RESULTS ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"                 FINAL RESULTS COMPARISON\")\n    print(\"=\"*50)\n\n    # Score with OLD simple 0.5 threshold\n    preds_old = torch.sum(torch.sigmoid(test_outputs) > 0.5, dim=1).numpy()\n    qwk_old = cohen_kappa_score(test_labels, preds_old, weights='quadratic')\n    acc_old = accuracy_score(test_labels, preds_old)\n    print(f\"\\nOriginal Score (Threshold = 0.5):\")\n    print(f\"  QWK: {qwk_old:.4f}\")\n    print(f\"  Accuracy: {acc_old*100:.2f}%\")\n\n    # Score with NEW optimized thresholds\n    preds_new = ordinal_to_class_with_thresholds(test_outputs, best_thresholds)\n    qwk_new = cohen_kappa_score(test_labels, preds_new, weights='quadratic')\n    acc_new = accuracy_score(test_labels, preds_new)\n    print(f\"\\nPolished Score (Optimized Thresholds):\")\n    print(f\"  QWK: {qwk_new:.4f}\")\n    print(f\"  Accuracy: {acc_new*100:.2f}%\")\n    print(\"=\"*50)\n    \n    # --- NEW: DETAILED REPORT FOR POLISHED SCORE ---\n    print(\"\\n--- Polished Classification Report ---\")\n    print(classification_report(test_labels, preds_new, target_names=[f\"Class {i}\" for i in range(5)]))\n    \n    print(\"\\n--- Polished Confusion Matrix ---\")\n    cm = confusion_matrix(test_labels, preds_new)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n    plt.xlabel(\"Predicted Label (Optimized)\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix (Optimized Thresholds)\")\n    plt.show()\n    \nrun_optimization_and_test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T01:21:30.196983Z","iopub.execute_input":"2025-09-11T01:21:30.197418Z","iopub.status.idle":"2025-09-11T01:23:10.564765Z","shell.execute_reply.started":"2025-09-11T01:21:30.197359Z","shell.execute_reply":"2025-09-11T01:23:10.564089Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully from best_model_swin_final.pth\n\n--- Step 1: Evaluating on Validation Set to find thresholds ---\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1045962344.py:107: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, CFG.IMG_SIZE)),\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Getting Validation Outputs:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d8ec836928e4850adaf904aa3cb2bc0"}},"metadata":{}},{"name":"stdout","text":"\n--- Step 2: Optimizing Thresholds ---\nFinding optimal thresholds...\nOptimal thresholds found: [0.5947 0.1133 0.7584 0.5467]\n\n--- Step 3: Evaluating on Test Set with new thresholds ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Getting Test Outputs:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be21c3e0369c4300ab4df058974f8d1c"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\n                 FINAL RESULTS COMPARISON\n==================================================\n\nOriginal Score (Threshold = 0.5):\n  QWK: 0.9045\n  Accuracy: 72.95%\n\nPolished Score (Optimized Thresholds):\n  QWK: 0.9143\n  Accuracy: 77.32%\n==================================================\n\n--- Polished Classification Report ---\n              precision    recall  f1-score   support\n\n     Class 0       0.99      0.97      0.98       199\n     Class 1       0.45      0.63      0.53        30\n     Class 2       0.75      0.52      0.61        87\n     Class 3       0.23      0.71      0.35        17\n     Class 4       0.78      0.42      0.55        33\n\n    accuracy                           0.77       366\n   macro avg       0.64      0.65      0.60       366\nweighted avg       0.84      0.77      0.79       366\n\n\n--- Polished Confusion Matrix ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"------------------ efficent net B4--------------------","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\n# =============================================================================\n# CONFIGURATION FOR A ROBUST EFFICIENTNET-B4 RUN\n# =============================================================================\nclass CFG:\n    # --- MODEL & IMAGE SIZE ---\n    MODEL_NAME = 'efficientnet_b4' # The single, powerful upgrade\n    IMG_SIZE = 384                 # The optimal size for this model family\n    BATCH_SIZE = 8                 # Safe batch size for this large model\n\n    # --- DATA PATHS (Unchanged) ---\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    TRAIN_CSV = os.path.join(BASE_PATH, \"train_1.csv\")\n    VAL_CSV   = os.path.join(BASE_PATH, \"valid.csv\")\n    TRAIN_DIR = os.path.join(BASE_PATH, \"train_images\", \"train_images\")\n    VAL_DIR   = os.path.join(BASE_PATH, \"val_images\", \"val_images\")\n    \n    # --- PROVEN, STABLE TRAINING STRATEGY ---\n    # Stage 1: Weighted Focal Loss Training\n    S1_EPOCHS = 15\n    S1_LR = 1e-4\n    S1_USE_MIXUP = True\n    \n    # Stage 2: Hybrid Loss Fine-tuning\n    S2_EPOCHS = 15\n    S2_LR = 3e-5 \n    S2_USE_MIXUP = False\n    \n    # --- GENERAL & SAVING ---\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    NUM_WORKERS = 2\n    PATIENCE = 5\n    SEED = 42\n    LABEL_SMOOTHING = 0.05\n    # Save paths for this new model\n    SAVE_PATH_S1 = \"best_model_effnet_b4_stage1.pth\"\n    SAVE_PATH_FINAL = \"best_model_effnet_b4_final.pth\"\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG.SEED)\n\ndef preprocess_ben_graham(image_np, output_size):\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n        if gray.mean() < 15: return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception: pass\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(image_resized); clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)); g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\ndef get_transforms(img_size):\n    pre_transforms = A.Compose([A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7), A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7), A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5)])\n    post_transforms = A.Compose([A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()])\n    val_transforms = A.Compose([A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()])\n    return pre_transforms, post_transforms, val_transforms\n\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, pre_transform=None, post_transform=None):\n        self.df, self.img_dir, self.pre_transform, self.post_transform = df.reset_index(drop=True), img_dir, pre_transform, post_transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = cv2.imread(img_path); img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.pre_transform: img = self.pre_transform(image=img)['image']\n        if self.post_transform: img = self.post_transform(image=img)['image']\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\nclass WeightedOrdinalFocalLoss(nn.Module):\n    def __init__(self, num_classes=5, gamma=2.0, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.num_classes, self.gamma, self.class_weights, self.label_smoothing = num_classes, gamma, class_weights, label_smoothing\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n    def forward(self, outputs, targets):\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, t in enumerate(targets):\n            if t > 0: ordinal_targets[i, :t] = 1.0\n        if self.label_smoothing > 0.0: ordinal_targets = ordinal_targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n        bce = self.bce(outputs, ordinal_targets)\n        if self.class_weights is not None:\n            weights = self.class_weights[targets].view(-1, 1).expand(-1, outputs.shape[1])\n            bce = bce * weights\n        pt = torch.exp(-bce); focal = (1 - pt) ** self.gamma * bce\n        return focal.mean()\n\nclass SmoothKappaLoss(nn.Module):\n    def __init__(self, num_classes=5, eps=1e-7):\n        super().__init__(); self.num_classes, self.eps = num_classes, eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes): W[i,j] = ((i - j)**2) / ((num_classes - 1)**2)\n        self.register_buffer(\"W\", W)\n    def forward(self, outputs, targets):\n        device = outputs.device; B = outputs.size(0); probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes-1): class_probs[:, k] = probs[:, k-1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        hist_true = one_hot.sum(dim=0); hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        W = self.W.to(device); obs = torch.sum(W * conf_mat); exp = torch.sum(W * expected)\n        kappa = 1.0 - (B * obs) / (exp + self.eps)\n        return 1.0 - kappa\n\nclass EfficientNetOrdinal(nn.Module):\n    def __init__(self, model_name, num_classes=5, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(nn.Dropout(0.5), nn.Linear(feature_dim, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes - 1))\n    def forward(self, x): return self.classifier(self.backbone(x))\n\ndef mixup_data(x, y, alpha=0.4):\n    if alpha > 0: lam = np.random.beta(alpha, alpha)\n    else: lam = 1\n    batch_size = x.size()[0]; index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]; y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef ordinal_to_class(outputs): return torch.sum(torch.sigmoid(outputs) > 0.5, dim=1).long()\ndef calculate_metrics(outputs, targets):\n    preds = ordinal_to_class(outputs).cpu().numpy(); targets_np = targets.cpu().numpy()\n    return accuracy_score(targets_np, preds), cohen_kappa_score(targets_np, preds, weights='quadratic')\ndef clear_memory(): gc.collect(); torch.cuda.empty_cache()\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, device, use_mixup):\n    model.train(); running_loss = 0.0; all_out, all_t = [], []\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    for images, targets in pbar:\n        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if use_mixup: images, targets_a, targets_b, lam = mixup_data(images, targets)\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            if use_mixup: loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n            else: loss = criterion(outputs, targets)\n        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n        running_loss += loss.item(); all_out.append(outputs.detach()); all_t.append(targets.detach())\n        pbar.set_postfix(loss=loss.item())\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval(); running_loss = 0.0; all_out, all_t = [], []\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n        for images, targets in pbar:\n            images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_out.append(outputs)\n            all_t.append(targets)\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef main():\n    print(f\"Device: {CFG.DEVICE}, Model: {CFG.MODEL_NAME}, Image Size: {CFG.IMG_SIZE}\")\n    train_df = pd.read_csv(CFG.TRAIN_CSV); val_df = pd.read_csv(CFG.VAL_CSV)\n    pre_tf, post_tf, val_tf = get_transforms(CFG.IMG_SIZE)\n    train_ds = DiabeticRetinopathyDataset(train_df, CFG.TRAIN_DIR, pre_transform=pre_tf, post_transform=post_tf)\n    val_ds = DiabeticRetinopathyDataset(val_df, CFG.VAL_DIR, pre_transform=val_tf)\n    class_weights_sampler = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    sample_weights = np.array([class_weights_sampler[int(l)] for l in train_df['diagnosis']])\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n    train_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, sampler=sampler, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=CFG.BATCH_SIZE*2, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    \n    model = EfficientNetOrdinal(CFG.MODEL_NAME).to(CFG.DEVICE)\n    class_weights_loss = torch.tensor(class_weights_sampler, dtype=torch.float).to(CFG.DEVICE)\n    focal_loss = WeightedOrdinalFocalLoss(num_classes=5, gamma=2.0, class_weights=class_weights_loss, label_smoothing=CFG.LABEL_SMOOTHING)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    \n    def hybrid_loss(outputs, targets):\n        return 0.7 * kappa_loss(outputs, targets) + 0.3 * focal_loss(outputs, targets)\n    \n    scaler = torch.cuda.amp.GradScaler()\n\n    # --- STAGE 1: WEIGHTED FOCAL LOSS ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 1: WEIGHTED FOCAL LOSS\\n\" + \"=\"*50)\n    opt = optim.AdamW(model.parameters(), lr=CFG.S1_LR, weight_decay=1e-4)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S1_EPOCHS)\n    best_val_qwk, patience_counter = -1, 0\n\n    for epoch in range(CFG.S1_EPOCHS):\n        clear_memory(); print(f\"\\nEpoch {epoch+1}/{CFG.S1_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, focal_loss, scaler, CFG.DEVICE, CFG.S1_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, focal_loss, CFG.DEVICE)\n        sched.step()\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n        if val_qwk > best_val_qwk:\n            print(f\"Val QWK improved from {best_val_qwk:.4f} to {val_qwk:.4f}. Saving model...\")\n            best_val_qwk, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), CFG.SAVE_PATH_S1)\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 1.\"); break\n    \n    # --- STAGE 2: HYBRID LOSS FINE-TUNING ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 2: HYBRID LOSS FINE-TUNING\\n\" + \"=\"*50)\n    model.load_state_dict(torch.load(CFG.SAVE_PATH_S1))\n    opt = optim.AdamW(model.parameters(), lr=CFG.S2_LR, weight_decay=1e-5)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S2_EPOCHS)\n    best_val_qwk_stage2, patience_counter = best_val_qwk, 0\n\n    for epoch in range(CFG.S2_EPOCHS):\n        clear_memory(); print(f\"\\nEpoch {epoch+1}/{CFG.S2_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, hybrid_loss, scaler, CFG.DEVICE, CFG.S2_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, hybrid_loss, CFG.DEVICE)\n        sched.step()\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n        if val_qwk > best_val_qwk_stage2:\n            print(f\"Val QWK improved from {best_val_qwk_stage2:.4f} to {val_qwk:.4f}. Saving final model...\")\n            best_val_qwk_stage2, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), CFG.SAVE_PATH_FINAL)\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 2.\"); break\n\n    print(f\"\\nTraining Finished!\\nBest Stage 1 QWK: {best_val_qwk:.4f}\\nFinal Best QWK: {best_val_qwk_stage2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T01:34:35.217618Z","iopub.execute_input":"2025-09-11T01:34:35.217889Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, Model: efficientnet_b4, Image Size: 384\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:81: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  pre_transforms = A.Compose([A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7), A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7), A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5)])\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_1675/3562726915.py:83: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n  val_transforms = A.Compose([A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()])\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\n     STARTING UPGRADED TRAINING PIPELINE\n==================================================\n\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:182: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/tmp/ipykernel_1675/3562726915.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(): outputs = model(images); loss = criterion(outputs, targets)\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.6936, Acc: 0.2157, QWK: 0.0157\nValid -> Loss: 0.2700, Acc: 0.0601, QWK: 0.0000\nVal QWK improved from -1.0000 to 0.0000. Saving final model...\n\nEpoch 2/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/tmp/ipykernel_1675/3562726915.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(): outputs = model(images); loss = criterion(outputs, targets)\n","output_type":"stream"},{"name":"stdout","text":"Train -> Loss: 0.4408, Acc: 0.2618, QWK: 0.3002\nValid -> Loss: 0.1694, Acc: 0.1858, QWK: 0.6413\nVal QWK improved from 0.0000 to 0.6413. Saving final model...\n\nEpoch 3/15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/367 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c90486bcb3433897e26bdc0f82e625"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_1675/3562726915.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/tmp/ipykernel_1675/3562726915.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"--------------- test with diff optimizer if fails","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\n# =============================================================================\n# CONFIGURATION FOR A ROBUST EFFICIENTNET-B4 RUN\n# =============================================================================\nclass CFG:\n    # --- MODEL & IMAGE SIZE ---\n    MODEL_NAME = 'efficientnet_b4' # The single, powerful upgrade\n    IMG_SIZE = 384                 # The optimal size for this model family\n    BATCH_SIZE = 8                 # Safe batch size for this large model\n\n    # --- DATA PATHS (Unchanged) ---\n    BASE_PATH = \"/kaggle/input/aptos2019\"\n    TRAIN_CSV = os.path.join(BASE_PATH, \"train_1.csv\")\n    VAL_CSV   = os.path.join(BASE_PATH, \"valid.csv\")\n    TRAIN_DIR = os.path.join(BASE_PATH, \"train_images\", \"train_images\")\n    VAL_DIR   = os.path.join(BASE_PATH, \"val_images\", \"val_images\")\n    \n    # --- PROVEN, STABLE TRAINING STRATEGY ---\n    # Stage 1: Weighted Focal Loss Training\n    S1_EPOCHS = 15\n    S1_LR = 1e-4\n    S1_USE_MIXUP = True\n    \n    # Stage 2: Hybrid Loss Fine-tuning\n    S2_EPOCHS = 15\n    S2_LR = 3e-5 \n    S2_USE_MIXUP = False\n    \n    # --- GENERAL & SAVING ---\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    NUM_WORKERS = 2\n    PATIENCE = 5\n    SEED = 42\n    LABEL_SMOOTHING = 0.05\n    # Save paths for this new model\n    SAVE_PATH_S1 = \"best_model_effnet_b4_stage1.pth\"\n    SAVE_PATH_FINAL = \"best_model_effnet_b4_final.pth\"\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG.SEED)\n\ndef preprocess_ben_graham(image_np, output_size):\n    try:\n        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n        if gray.mean() < 15: return cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n        _, thresh = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv2.contourArea)\n            x, y, w, h = cv2.boundingRect(largest_contour)\n            image_np = image_np[y:y+h, x:x+w]\n    except Exception: pass\n    image_resized = cv2.resize(image_np, (output_size, output_size), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(image_resized); clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)); g = clahe.apply(g)\n    return cv2.merge((b, g, r))\n\ndef get_transforms(img_size):\n    pre_transforms = A.Compose([A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7), A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7), A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5)])\n    post_transforms = A.Compose([A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()])\n    val_transforms = A.Compose([A.Lambda(image=lambda x, **kwargs: preprocess_ben_graham(x, img_size)), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()])\n    return pre_transforms, post_transforms, val_transforms\n\nclass DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, df, img_dir, pre_transform=None, post_transform=None):\n        self.df, self.img_dir, self.pre_transform, self.post_transform = df.reset_index(drop=True), img_dir, pre_transform, post_transform\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['id_code'] + '.png')\n        img = cv2.imread(img_path); img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.pre_transform: img = self.pre_transform(image=img)['image']\n        if self.post_transform: img = self.post_transform(image=img)['image']\n        label = torch.tensor(row['diagnosis'], dtype=torch.long)\n        return img, label\n\nclass WeightedOrdinalFocalLoss(nn.Module):\n    def __init__(self, num_classes=5, gamma=2.0, class_weights=None, label_smoothing=0.0):\n        super().__init__()\n        self.num_classes, self.gamma, self.class_weights, self.label_smoothing = num_classes, gamma, class_weights, label_smoothing\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n    def forward(self, outputs, targets):\n        ordinal_targets = torch.zeros_like(outputs)\n        for i, t in enumerate(targets):\n            if t > 0: ordinal_targets[i, :t] = 1.0\n        if self.label_smoothing > 0.0: ordinal_targets = ordinal_targets * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n        bce = self.bce(outputs, ordinal_targets)\n        if self.class_weights is not None:\n            weights = self.class_weights[targets].view(-1, 1).expand(-1, outputs.shape[1])\n            bce = bce * weights\n        pt = torch.exp(-bce); focal = (1 - pt) ** self.gamma * bce\n        return focal.mean()\n\nclass SmoothKappaLoss(nn.Module):\n    def __init__(self, num_classes=5, eps=1e-7):\n        super().__init__(); self.num_classes, self.eps = num_classes, eps\n        W = torch.zeros(num_classes, num_classes)\n        for i in range(num_classes):\n            for j in range(num_classes): W[i,j] = ((i - j)**2) / ((num_classes - 1)**2)\n        self.register_buffer(\"W\", W)\n    def forward(self, outputs, targets):\n        device = outputs.device; B = outputs.size(0); probs = torch.sigmoid(outputs)\n        class_probs = torch.zeros(B, self.num_classes, device=device)\n        class_probs[:, 0] = 1 - probs[:, 0]\n        for k in range(1, self.num_classes-1): class_probs[:, k] = probs[:, k-1] - probs[:, k]\n        class_probs[:, -1] = probs[:, -1]\n        class_probs = torch.clamp(class_probs, min=self.eps, max=1.0)\n        one_hot = F.one_hot(targets, num_classes=self.num_classes).float().to(device)\n        conf_mat = torch.matmul(one_hot.T, class_probs)\n        hist_true = one_hot.sum(dim=0); hist_pred = class_probs.sum(dim=0)\n        expected = torch.outer(hist_true, hist_pred)\n        W = self.W.to(device); obs = torch.sum(W * conf_mat); exp = torch.sum(W * expected)\n        kappa = 1.0 - (B * obs) / (exp + self.eps)\n        return 1.0 - kappa\n\nclass EfficientNetOrdinal(nn.Module):\n    def __init__(self, model_name, num_classes=5, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(nn.Dropout(0.5), nn.Linear(feature_dim, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes - 1))\n    def forward(self, x): return self.classifier(self.backbone(x))\n\ndef mixup_data(x, y, alpha=0.4):\n    if alpha > 0: lam = np.random.beta(alpha, alpha)\n    else: lam = 1\n    batch_size = x.size()[0]; index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]; y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef ordinal_to_class(outputs): return torch.sum(torch.sigmoid(outputs) > 0.5, dim=1).long()\ndef calculate_metrics(outputs, targets):\n    preds = ordinal_to_class(outputs).cpu().numpy(); targets_np = targets.cpu().numpy()\n    return accuracy_score(targets_np, preds), cohen_kappa_score(targets_np, preds, weights='quadratic')\ndef clear_memory(): gc.collect(); torch.cuda.empty_cache()\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, device, use_mixup):\n    model.train(); running_loss = 0.0; all_out, all_t = [], []\n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    for images, targets in pbar:\n        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if use_mixup: images, targets_a, targets_b, lam = mixup_data(images, targets)\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            if use_mixup: loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n            else: loss = criterion(outputs, targets)\n        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n        running_loss += loss.item(); all_out.append(outputs.detach()); all_t.append(targets.detach())\n        pbar.set_postfix(loss=loss.item())\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef validate_epoch(model, loader, criterion, device):\n    model.eval(); running_loss = 0.0; all_out, all_t = [], []\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n        for images, targets in pbar:\n            images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            all_out.append(outputs)\n            all_t.append(targets)\n    all_out, all_t = torch.cat(all_out), torch.cat(all_t)\n    return running_loss / len(loader), *calculate_metrics(all_out, all_t)\n\ndef main():\n    print(f\"Device: {CFG.DEVICE}, Model: {CFG.MODEL_NAME}, Image Size: {CFG.IMG_SIZE}\")\n    train_df = pd.read_csv(CFG.TRAIN_CSV); val_df = pd.read_csv(CFG.VAL_CSV)\n    pre_tf, post_tf, val_tf = get_transforms(CFG.IMG_SIZE)\n    train_ds = DiabeticRetinopathyDataset(train_df, CFG.TRAIN_DIR, pre_transform=pre_tf, post_transform=post_tf)\n    val_ds = DiabeticRetinopathyDataset(val_df, CFG.VAL_DIR, pre_transform=val_tf)\n    class_weights_sampler = compute_class_weight('balanced', classes=np.unique(train_df['diagnosis']), y=train_df['diagnosis'])\n    sample_weights = np.array([class_weights_sampler[int(l)] for l in train_df['diagnosis']])\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n    train_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, sampler=sampler, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=CFG.BATCH_SIZE*2, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n    \n    model = EfficientNetOrdinal(CFG.MODEL_NAME).to(CFG.DEVICE)\n    class_weights_loss = torch.tensor(class_weights_sampler, dtype=torch.float).to(CFG.DEVICE)\n    focal_loss = WeightedOrdinalFocalLoss(num_classes=5, gamma=2.0, class_weights=class_weights_loss, label_smoothing=CFG.LABEL_SMOOTHING)\n    kappa_loss = SmoothKappaLoss(num_classes=5)\n    \n    def hybrid_loss(outputs, targets):\n        return 0.7 * kappa_loss(outputs, targets) + 0.3 * focal_loss(outputs, targets)\n    \n    scaler = torch.cuda.amp.GradScaler()\n\n    # --- STAGE 1: WEIGHTED FOCAL LOSS ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 1: WEIGHTED FOCAL LOSS\\n\" + \"=\"*50)\n    opt = optim.AdamW(model.parameters(), lr=CFG.S1_LR, weight_decay=1e-4)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S1_EPOCHS)\n    best_val_qwk, patience_counter = -1, 0\n\n    for epoch in range(CFG.S1_EPOCHS):\n        clear_memory(); print(f\"\\nEpoch {epoch+1}/{CFG.S1_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, focal_loss, scaler, CFG.DEVICE, CFG.S1_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, focal_loss, CFG.DEVICE)\n        sched.step()\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n        if val_qwk > best_val_qwk:\n            print(f\"Val QWK improved from {best_val_qwk:.4f} to {val_qwk:.4f}. Saving model...\")\n            best_val_qwk, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), CFG.SAVE_PATH_S1)\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 1.\"); break\n    \n    # --- STAGE 2: HYBRID LOSS FINE-TUNING ---\n    print(\"\\n\" + \"=\"*50 + \"\\n     STARTING STAGE 2: HYBRID LOSS FINE-TUNING\\n\" + \"=\"*50)\n    model.load_state_dict(torch.load(CFG.SAVE_PATH_S1))\n    opt = optim.AdamW(model.parameters(), lr=CFG.S2_LR, weight_decay=1e-5)\n    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG.S2_EPOCHS)\n    best_val_qwk_stage2, patience_counter = best_val_qwk, 0\n\n    for epoch in range(CFG.S2_EPOCHS):\n        clear_memory(); print(f\"\\nEpoch {epoch+1}/{CFG.S2_EPOCHS}\")\n        train_loss, train_acc, train_qwk = train_epoch(model, train_loader, opt, hybrid_loss, scaler, CFG.DEVICE, CFG.S2_USE_MIXUP)\n        val_loss, val_acc, val_qwk = validate_epoch(model, val_loader, hybrid_loss, CFG.DEVICE)\n        sched.step()\n        print(f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, QWK: {train_qwk:.4f}\")\n        print(f\"Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, QWK: {val_qwk:.4f}\")\n        if val_qwk > best_val_qwk_stage2:\n            print(f\"Val QWK improved from {best_val_qwk_stage2:.4f} to {val_qwk:.4f}. Saving final model...\")\n            best_val_qwk_stage2, patience_counter = val_qwk, 0\n            torch.save(model.state_dict(), CFG.SAVE_PATH_FINAL)\n        else:\n            patience_counter += 1\n            if patience_counter >= CFG.PATIENCE: print(\"Early stopping in Stage 2.\"); break\n\n    print(f\"\\nTraining Finished!\\nBest Stage 1 QWK: {best_val_qwk:.4f}\\nFinal Best QWK: {best_val_qwk_stage2:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}